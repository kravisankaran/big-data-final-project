{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abc2c2f-4a55-4bfc-b5d8-b9b93740a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").config(\"spark.executor.memory\", \"500mb\").config(\"spark.driver.memory\", \"1g\").getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f154c3-09fd-4be2-a3b8-013790fa7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bddb45a-c047-4263-b866-670bae0f2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).json(r\"C:\\Users\\Emma\\Downloads\\school\\Big_Data\\project\\amazon_review_data\\AMAZON_FASHION_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bb8558-a926-4b5e-978d-029089c13817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin|               style|      reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "|    5.0|    true| 09 4, 2015| ALJ66O1Y6SLHA|B000K2PJ4K|      { Blue/Orange}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true| 09 4, 2015| ALJ66O1Y6SLHA|B000K2PJ4K|{ Black (37467610...|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true| 09 4, 2015| ALJ66O1Y6SLHA|B000K2PJ4K|   { Blue/Gray Logo}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true| 09 4, 2015| ALJ66O1Y6SLHA|B000K2PJ4K|{ Blue (37867638-...|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true| 09 4, 2015| ALJ66O1Y6SLHA|B000K2PJ4K|        { Blue/Pink}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    3.0|    true| 05 6, 2015|A3W11493KS6Z2L|B000K2PJ4K|      { White/Black}|            NaeNae|Waaay too small. ...|    1430870400|\n",
      "|    5.0|    true| 05 6, 2015|A3W11493KS6Z2L|B000K2PJ4K|      { Blue/Orange}|            NaeNae|Stays vibrant aft...|    1430870400|\n",
      "|    5.0|    true| 05 6, 2015|A3W11493KS6Z2L|B000K2PJ4K|{ Blue (37867638-...|            NaeNae|Stays vibrant aft...|    1430870400|\n",
      "|    5.0|    true| 05 6, 2015|A3W11493KS6Z2L|B000K2PJ4K|        { Blue/Pink}|            NaeNae|My son really lik...|    1430870400|\n",
      "|    3.0|    true| 05 6, 2015|A3W11493KS6Z2L|B000K2PJ4K|   { Light Blue/Red}|            NaeNae|Waaay too small. ...|    1430870400|\n",
      "|    2.0|    true|01 25, 2018|A3HX4X3TIABWOV|B000KPIHQ4|              {NULL}|   Denise A. Conte|Relieved my Plant...|    1516838400|\n",
      "|    2.0|    true| 01 5, 2017| AW8UBYMNJ894V|B000KPIHQ4|              {NULL}|Cognizant Consumer|This is my 6th pa...|    1483574400|\n",
      "|    5.0|    true|10 17, 2016|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|We have used thes...|    1476662400|\n",
      "|    5.0|    true|08 22, 2016| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|Pinnacle seems to...|    1471824000|\n",
      "|    5.0|    true|03 23, 2016|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|Excellent insole ...|    1458691200|\n",
      "|    5.0|    true|06 24, 2015| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|A little more cus...|    1435104000|\n",
      "|    5.0|    true|11 17, 2014|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|These insoles hel...|    1416182400|\n",
      "|    2.0|    true|01 25, 2018|A3HX4X3TIABWOV|B000V0IBDM|                NULL|   Denise A. Conte|Relieved my Plant...|    1516838400|\n",
      "|    2.0|    true| 01 5, 2017| AW8UBYMNJ894V|B000V0IBDM|                NULL|Cognizant Consumer|This is my 6th pa...|    1483574400|\n",
      "|    5.0|    true|10 17, 2016|A265UZVOZWTTXQ|B000V0IBDM|                NULL|    William_Jasper|We have used thes...|    1476662400|\n",
      "+-------+--------+-----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cba66da-f8a4-4850-b5b5-3a4eeba998e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "df_modified = df.withColumn(\"style\", struct(col(\"style.Color:\").alias(\"Color\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a432e56-d412-4a35-b395-13e91a7e4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_with_date = df.withColumn(\"reviewTime\", to_date(df.reviewTime, \"MM d, yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86819dc-aa07-46f2-81cc-fdf5ed72dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|               style|      reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|      { Blue/Orange}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Black (37467610...|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|   { Blue/Gray Logo}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Blue (37867638-...|          Tonya B.|Great product and...|    1441324800|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|        { Blue/Pink}|          Tonya B.|Great product and...|    1441324800|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { White/Black}|            NaeNae|Waaay too small. ...|    1430870400|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { Blue/Orange}|            NaeNae|Stays vibrant aft...|    1430870400|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|{ Blue (37867638-...|            NaeNae|Stays vibrant aft...|    1430870400|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|        { Blue/Pink}|            NaeNae|My son really lik...|    1430870400|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|   { Light Blue/Red}|            NaeNae|Waaay too small. ...|    1430870400|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000KPIHQ4|              {NULL}|   Denise A. Conte|Relieved my Plant...|    1516838400|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000KPIHQ4|              {NULL}|Cognizant Consumer|This is my 6th pa...|    1483574400|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|We have used thes...|    1476662400|\n",
      "|    5.0|    true|2016-08-22| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|Pinnacle seems to...|    1471824000|\n",
      "|    5.0|    true|2016-03-23|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|Excellent insole ...|    1458691200|\n",
      "|    5.0|    true|2015-06-24| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|A little more cus...|    1435104000|\n",
      "|    5.0|    true|2014-11-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|These insoles hel...|    1416182400|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000V0IBDM|                NULL|   Denise A. Conte|Relieved my Plant...|    1516838400|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000V0IBDM|                NULL|Cognizant Consumer|This is my 6th pa...|    1483574400|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000V0IBDM|                NULL|    William_Jasper|We have used thes...|    1476662400|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8ee360-015e-430b-a593-17843883e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99a1669-2aaa-4b08-88c7-9a8885f760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db0b018-f69b-4700-8705-9dad8e559771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = df_with_date.withColumn(\"sentiment\", targetUDF(df_with_date[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f325a032-9a6f-4379-9ce1-d12d6dac766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|               style|      reviewerName|          reviewText|unixReviewTime|sentiment|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|      { Blue/Orange}|          Tonya B.|Great product and...|    1441324800|        1|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Black (37467610...|          Tonya B.|Great product and...|    1441324800|        1|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|   { Blue/Gray Logo}|          Tonya B.|Great product and...|    1441324800|        1|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Blue (37867638-...|          Tonya B.|Great product and...|    1441324800|        1|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|        { Blue/Pink}|          Tonya B.|Great product and...|    1441324800|        1|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { White/Black}|            NaeNae|Waaay too small. ...|    1430870400|        0|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { Blue/Orange}|            NaeNae|Stays vibrant aft...|    1430870400|        1|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|{ Blue (37867638-...|            NaeNae|Stays vibrant aft...|    1430870400|        1|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|        { Blue/Pink}|            NaeNae|My son really lik...|    1430870400|        1|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|   { Light Blue/Red}|            NaeNae|Waaay too small. ...|    1430870400|        0|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000KPIHQ4|              {NULL}|   Denise A. Conte|Relieved my Plant...|    1516838400|        0|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000KPIHQ4|              {NULL}|Cognizant Consumer|This is my 6th pa...|    1483574400|        0|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|We have used thes...|    1476662400|        1|\n",
      "|    5.0|    true|2016-08-22| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|Pinnacle seems to...|    1471824000|        1|\n",
      "|    5.0|    true|2016-03-23|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|Excellent insole ...|    1458691200|        1|\n",
      "|    5.0|    true|2015-06-24| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|A little more cus...|    1435104000|        1|\n",
      "|    5.0|    true|2014-11-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|These insoles hel...|    1416182400|        1|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000V0IBDM|                NULL|   Denise A. Conte|Relieved my Plant...|    1516838400|        0|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000V0IBDM|                NULL|Cognizant Consumer|This is my 6th pa...|    1483574400|        0|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000V0IBDM|                NULL|    William_Jasper|We have used thes...|    1476662400|        1|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2941e4ba-fb43-4ace-b68f-c557b110c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "df4 = tokenizer.transform(df_sentiment.filter(df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ec8635-eb42-40e2-81a0-405024c94bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+--------------------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|               style|      reviewerName|          reviewText|unixReviewTime|sentiment|               token|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+--------------------+\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|      { Blue/Orange}|          Tonya B.|Great product and...|    1441324800|        1|[great, product, ...|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Black (37467610...|          Tonya B.|Great product and...|    1441324800|        1|[great, product, ...|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|   { Blue/Gray Logo}|          Tonya B.|Great product and...|    1441324800|        1|[great, product, ...|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|{ Blue (37867638-...|          Tonya B.|Great product and...|    1441324800|        1|[great, product, ...|\n",
      "|    5.0|    true|2015-09-04| ALJ66O1Y6SLHA|B000K2PJ4K|        { Blue/Pink}|          Tonya B.|Great product and...|    1441324800|        1|[great, product, ...|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { White/Black}|            NaeNae|Waaay too small. ...|    1430870400|        0|[waaay, too, smal...|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|      { Blue/Orange}|            NaeNae|Stays vibrant aft...|    1430870400|        1|[stays, vibrant, ...|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|{ Blue (37867638-...|            NaeNae|Stays vibrant aft...|    1430870400|        1|[stays, vibrant, ...|\n",
      "|    5.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|        { Blue/Pink}|            NaeNae|My son really lik...|    1430870400|        1|[my, son, really,...|\n",
      "|    3.0|    true|2015-05-06|A3W11493KS6Z2L|B000K2PJ4K|   { Light Blue/Red}|            NaeNae|Waaay too small. ...|    1430870400|        0|[waaay, too, smal...|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000KPIHQ4|              {NULL}|   Denise A. Conte|Relieved my Plant...|    1516838400|        0|[relieved, my, pl...|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000KPIHQ4|              {NULL}|Cognizant Consumer|This is my 6th pa...|    1483574400|        0|[this, is, my, 6t...|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|We have used thes...|    1476662400|        1|[we, have, used, ...|\n",
      "|    5.0|    true|2016-08-22| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|Pinnacle seems to...|    1471824000|        1|[pinnacle, seems,...|\n",
      "|    5.0|    true|2016-03-23|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|Excellent insole ...|    1458691200|        1|[excellent, insol...|\n",
      "|    5.0|    true|2015-06-24| AW8UBYMNJ894V|B000KPIHQ4|                NULL|Cognizant Consumer|A little more cus...|    1435104000|        1|[a, little, more,...|\n",
      "|    5.0|    true|2014-11-17|A265UZVOZWTTXQ|B000KPIHQ4|                NULL|    William_Jasper|These insoles hel...|    1416182400|        1|[these, insoles, ...|\n",
      "|    2.0|    true|2018-01-25|A3HX4X3TIABWOV|B000V0IBDM|                NULL|   Denise A. Conte|Relieved my Plant...|    1516838400|        0|[relieved, my, pl...|\n",
      "|    2.0|    true|2017-01-05| AW8UBYMNJ894V|B000V0IBDM|                NULL|Cognizant Consumer|This is my 6th pa...|    1483574400|        0|[this, is, my, 6t...|\n",
      "|    5.0|    true|2016-10-17|A265UZVOZWTTXQ|B000V0IBDM|                NULL|    William_Jasper|We have used thes...|    1476662400|        1|[we, have, used, ...|\n",
      "+-------+--------+----------+--------------+----------+--------------------+------------------+--------------------+--------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d41c690-a5cf-4a3a-adb5-f0978ed1a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    expr    = '(@[A-Za-z0-a9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "              '(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "        \n",
    "    regex   = re.compile(expr)\n",
    "\n",
    "    cleaned = [t for t in tokens if not(regex.search(t)) if len(t) > 0]\n",
    "\n",
    "    return list(filter(None, cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d08bea8d-230f-4170-8b18-5e248173a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf340c2-989a-415f-bcd4-2609878074ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40d5b51-ca87-4bcd-b531-5af45d5b9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4 = df4.withColumn(\"tokens_re\", removeWEBUDF(df4[\"token\"]))\n",
    "\n",
    "# remove non english characters\n",
    "df4 = df4.withColumn(\"tokens_clean\", normalizeUDF(df4[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5 = df4.drop(\"token\",\"tokens_re\")\n",
    "df5 = df5.withColumnRenamed(\"tokens_clean\", \"tokens\")\\\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6 = df5.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a3d686-fda4-40dc-9007-aa438ff629a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-09-04</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>( Blue/Orange,)</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, product, and, price]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-09-04</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>( Black (37467610) / Red/White,)</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, product, and, price]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified  reviewTime     reviewerID        asin  \\\n",
       "0      5.0      True  2015-09-04  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "1      5.0      True  2015-09-04  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "\n",
       "                              style reviewerName                reviewText  \\\n",
       "0                   ( Blue/Orange,)     Tonya B.  Great product and price!   \n",
       "1  ( Black (37467610) / Red/White,)     Tonya B.  Great product and price!   \n",
       "\n",
       "   unixReviewTime  sentiment                        tokens  \n",
       "0      1441324800          1  [great, product, and, price]  \n",
       "1      1441324800          1  [great, product, and, price]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df6.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63401c31-418d-4517-aa49-733c8e87a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_name = \"poc\"\n",
    "# collection_name = \"reviews\"\n",
    "# df_sentiment.write.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/poc.reviews\").mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pymongo\n",
    "\n",
    "\n",
    "\n",
    "# conn = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "# db = conn.poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dde8f44-377e-44f2-bd39-1958e9e0ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = db.reviews\n",
    "# query = {\"asin\": \"0209688726\"}\n",
    "# # for res in results:\n",
    "# #     print(\"Document = {}\\n\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51b17ac-8201-4099-b202-02ab27ee11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_sentiment = {\"$group\": \n",
    "#                      {\"_id\" : {\"sentiment\":\"$sentiment\"},  # note use a $ on the field\n",
    "#                       \"ct\"  : {\"$sum\":1}\n",
    "#                      }\n",
    "#                   }\n",
    "# results = reviews.aggregate([count_sentiment], allowDiskUse=True)\n",
    "\n",
    "# for res in results:\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74097e4c-d28a-4812-9f04-c6273da70c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ced9341-57b7-44b6-8643-7cecc6d54c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|Great product and...|    1|\n",
      "|Great product and...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71a88421-0a3a-4ec4-b0c9-fd25df90a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df7.randomSplit([0.80, 0.20], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bdc0451-5bdb-4549-bbd1-2262e80ae654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf0fc799-6d4b-4354-9faf-e1d9792cdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2118|\n",
      "|    0|  449|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby(\"label\")\\\n",
    "     .count()\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b36091cb-32c8-48db-b235-f9a4b9be8358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32e1646b-6a70-4c07-a0f1-eedaf3c010e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  495|\n",
      "|    0|   98|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupby(\"label\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7261098-eea5-436d-b37b-f1926d37792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c9c8f41-f513-4f56-8f99-c76f9e102dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eac5783-7af1-495e-98ba-b661497226b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "\n",
    "# create entire pipeline\n",
    "basic_pipeline = Pipeline(stages=[tk, tf1, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "529bfc38-d052-4964-9791-ef3d9e233130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|A little more cus...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A nice lightweigh...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|A-MA-ZING!  I nee...|    1|\n",
      "|Absolutely love t...|    1|\n",
      "|Absolutely love t...|    1|\n",
      "|Absolutely love t...|    1|\n",
      "|Absolutely love t...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b393d84f-b46f-4884-96df-e830d5953558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1         = basic_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1a34354-1062-4284-875a-eae73d4a0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.9999793856936714\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "predictions1   = model1.transform(test)\n",
    "\n",
    "# get the performance on the test set\n",
    "score1         = evaluator.evaluate(predictions1)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c9b7f03-47f5-44c5-98b7-315010952c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9966273187183811\n"
     ]
    }
   ],
   "source": [
    "predictedAndLabels = predictions1.select([\"prediction\",\"label\"])\\\n",
    "                                 .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ef59e70-7988-4ae7-af59-2f1e84e99a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0d4741f-173e-4276-994e-3379b88a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214d357c-838f-46bb-aef1-2821673993c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.9999587713873429\n"
     ]
    }
   ],
   "source": [
    "sw_pipleline  = Pipeline(stages=[tk, sw, tf2, idf, lr])\n",
    "\n",
    "model2        = sw_pipleline.fit(train)\n",
    "predictions2  = model2.transform(test)\n",
    "score2        = evaluator.evaluate(predictions2)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb341530-320c-4ba0-9110-4283af14cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f77da5d4-2264-4c48-aaf4-c995281dadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw tokens: ['my', 'feelings', 'having', 'studied', 'all', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens  = \"my feelings having studied all day\".split(\" \")\n",
    "print(\"raw tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "123f38e9-ffbb-4428-8290-1e68e81741d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean tokens: ['my', 'feel', 'have', 'studi', 'all', 'day']\n"
     ]
    }
   ],
   "source": [
    "tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(\"clean tokens: {}\".format(tokens_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a0d2e5d-4111-4382-b4e3-7a6141789e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74b453bc-30b9-466f-b338-ea7ec40f929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3300aaf-3fce-4fa1-bd04-ccab1d906dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "526e1aa8-ebd9-4273-9b32-961148c4bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int, tokens: array<string>, stemmed: array<string>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stem = stem_pipeline.transform(train)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "test_stem  = stem_pipeline.transform(test)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# cache them to avoid running stemming \n",
    "# each iteration in the grid search\n",
    "train_stem.cache()\n",
    "test_stem.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13d1015b-a383-4be1-a2ce-f0286ffe36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|          reviewText|label|              tokens|             stemmed|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|A little more cus...|    1|[a, little, more,...|[a, littl, more, ...|\n",
      "|A nice lightweigh...|    1|[a, nice, lightwe...|[a, nice, lightwe...|\n",
      "|Absolutely love t...|    1|[absolutely, love...|[absolut, love, t...|\n",
      "|Absolutely love t...|    1|[absolutely, love...|[absolut, love, t...|\n",
      "|Absolutely love t...|    1|[absolutely, love...|[absolut, love, t...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stem.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47f5c5d6-c9dd-4880-b61b-4f1bc61f6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(inputCol=\"tokens\", outputCol=\"bigrams\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7b91cc9-2f2e-4426-a864-f131fb903285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf5   = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "bigram_pipeline  = Pipeline(stages= [tk, bigram, tf5, idf, lr])\n",
    "\n",
    "model5           = bigram_pipeline.fit(train)\n",
    "predictions5     = model5.transform(test)\n",
    "\n",
    "score5           = evaluator.evaluate(predictions5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "825f2a88-2806-494c-a39d-dd4335a6bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.9989692846835705\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC SCORE: {}\".format(score5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a635b32-bb8e-4ff4-9d11-c36672a12837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "# Create a UDF\n",
    "stem_text_udf = udf(stem_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a9b599c-5447-4159-b071-59bb2f5e8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.9989692846835704\n"
     ]
    }
   ],
   "source": [
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "lr      = LogisticRegression(maxIter=20)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "model6                = stem_bigram_pipeline.fit(train_stem)\n",
    "predictions6          = model6.transform(test_stem)\n",
    "\n",
    "score6                = evaluator.evaluate(predictions6)\n",
    "print(\"AUC SCORE: {}\".format(score6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c792163d-1cd4-4317-91e2-5f3e8c7b2fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 21\u001b[0m\n\u001b[0;32m     12\u001b[0m paramGrid \u001b[38;5;241m=\u001b[39m ParamGridBuilder() \\\n\u001b[0;32m     13\u001b[0m                         \u001b[38;5;241m.\u001b[39maddGrid(idf\u001b[38;5;241m.\u001b[39mminDocFreq, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m]) \\\n\u001b[0;32m     14\u001b[0m                         \u001b[38;5;241m.\u001b[39maddGrid(lr\u001b[38;5;241m.\u001b[39mregParam, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]) \\\n\u001b[0;32m     15\u001b[0m                         \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m     16\u001b[0m crossval \u001b[38;5;241m=\u001b[39m CrossValidator(estimator          \u001b[38;5;241m=\u001b[39m stem_bigram_pipeline,\n\u001b[0;32m     17\u001b[0m                           estimatorParamMaps \u001b[38;5;241m=\u001b[39m paramGrid,\n\u001b[0;32m     18\u001b[0m                           evaluator          \u001b[38;5;241m=\u001b[39m BinaryClassificationEvaluator(),\n\u001b[0;32m     19\u001b[0m                           numFolds           \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m model    \u001b[38;5;241m=\u001b[39m crossval\u001b[38;5;241m.\u001b[39mfit(train_stem)\n\u001b[0;32m     22\u001b[0m predictions   \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_stem)\n\u001b[0;32m     23\u001b[0m score         \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr      = LogisticRegression(maxIter=20)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "                        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "                        .build()\n",
    "crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator          = BinaryClassificationEvaluator(),\n",
    "                          numFolds           = 3)\n",
    "\n",
    "model    = crossval.fit(train_stem)\n",
    "predictions   = model.transform(test_stem)\n",
    "score         = evaluator.evaluate(predictions)\n",
    "print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing bigram model without tuning\n",
    "print(\"AUC SCORE: {}\".format(score6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = model.bestModel\n",
    "predictedAndLabels = predictions.select([\"prediction\",\"label\"])\\\n",
    "                                .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba290cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.stages[2].explainParam('minDocFreq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b92880",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.stages[-1].explainParam('regParam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f574a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = bestModel.stages[-1].summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(summary.roc.select('FPR').collect(),\n",
    "         summary.roc.select('TPR').collect())\n",
    "plt.xlabel('False Positive Rare')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
