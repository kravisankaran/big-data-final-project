{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abc2c2f-4a55-4bfc-b5d8-b9b93740a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/01 20:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f154c3-09fd-4be2-a3b8-013790fa7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bddb45a-c047-4263-b866-670bae0f2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).json(\"/Users/kravisankaran/Downloads/Automotive_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bb8558-a926-4b5e-978d-029089c13817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|    4.0|   false| 05 1, 2015| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|\n",
      "|    1.0|    true|04 19, 2018| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|\n",
      "|    1.0|    true|04 16, 2018|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|\n",
      "|    3.0|    true|04 13, 2018|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|\n",
      "|    5.0|    true| 04 8, 2018|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|\n",
      "|    5.0|    true|03 24, 2018| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|\n",
      "|    3.0|    true| 03 4, 2018|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|\n",
      "|    2.0|    true| 03 1, 2018|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|\n",
      "|    4.0|    true|02 22, 2018|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|\n",
      "|    5.0|    true|01 29, 2018|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|\n",
      "|    2.0|    true|01 21, 2018|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|\n",
      "|    3.0|    true|01 14, 2018|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|\n",
      "|    5.0|    true| 01 3, 2018|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|\n",
      "|    1.0|    true|12 16, 2017|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|\n",
      "|    5.0|    true|12 15, 2017|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|\n",
      "|    1.0|    true|12 10, 2017|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|\n",
      "|    5.0|    true| 11 9, 2017|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|\n",
      "|    5.0|    true|10 29, 2017|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|\n",
      "|    3.0|    true|10 20, 2017|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|\n",
      "|    5.0|   false|09 23, 2017|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|\n",
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cba66da-f8a4-4850-b5b5-3a4eeba998e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "df_modified = df.withColumn(\"style\", struct(col(\"style.Color:\").alias(\"Color\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a432e56-d412-4a35-b395-13e91a7e4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_with_date = df.withColumn(\"reviewTime\", to_date(df.reviewTime, \"MM d, yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86819dc-aa07-46f2-81cc-fdf5ed72dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|    4.0|   false|2015-05-01| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|\n",
      "|    1.0|    true|2018-04-19| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|\n",
      "|    1.0|    true|2018-04-16|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|\n",
      "|    3.0|    true|2018-04-13|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|\n",
      "|    5.0|    true|2018-04-08|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|\n",
      "|    5.0|    true|2018-03-24| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|\n",
      "|    3.0|    true|2018-03-04|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|\n",
      "|    2.0|    true|2018-03-01|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|\n",
      "|    4.0|    true|2018-02-22|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|\n",
      "|    5.0|    true|2018-01-29|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|\n",
      "|    2.0|    true|2018-01-21|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|\n",
      "|    3.0|    true|2018-01-14|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|\n",
      "|    5.0|    true|2018-01-03|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|\n",
      "|    1.0|    true|2017-12-16|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|\n",
      "|    5.0|    true|2017-12-15|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|\n",
      "|    1.0|    true|2017-12-10|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|\n",
      "|    5.0|    true|2017-11-09|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|\n",
      "|    5.0|    true|2017-10-29|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|\n",
      "|    3.0|    true|2017-10-20|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|\n",
      "|    5.0|   false|2017-09-23|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8ee360-015e-430b-a593-17843883e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99a1669-2aaa-4b08-88c7-9a8885f760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db0b018-f69b-4700-8705-9dad8e559771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = df_with_date.withColumn(\"sentiment\", targetUDF(df_with_date[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f325a032-9a6f-4379-9ce1-d12d6dac766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 20:37:45 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/01 20:37:45 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (albertastein-2.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/04/01 20:37:45 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_sentiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "df_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941e4ba-fb43-4ace-b68f-c557b110c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "df4 = tokenizer.transform(df_sentiment.filter(df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec8635-eb42-40e2-81a0-405024c94bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41c690-a5cf-4a3a-adb5-f0978ed1a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    expr    = '(@[A-Za-z0-a9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "              '(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "        \n",
    "    regex   = re.compile(expr)\n",
    "\n",
    "    cleaned = [t for t in tokens if not(regex.search(t)) if len(t) > 0]\n",
    "\n",
    "    return list(filter(None, cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bea8d-230f-4170-8b18-5e248173a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf340c2-989a-415f-bcd4-2609878074ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d5b51-ca87-4bcd-b531-5af45d5b9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4 = df4.withColumn(\"tokens_re\", removeWEBUDF(df4[\"token\"]))\n",
    "\n",
    "# remove non english characters\n",
    "df4 = df4.withColumn(\"tokens_clean\", normalizeUDF(df4[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5 = df4.drop(\"token\",\"tokens_re\")\n",
    "df5 = df5.withColumnRenamed(\"tokens_clean\", \"tokens\")\\\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6 = df5.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3d686-fda4-40dc-9007-aa438ff629a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63401c31-418d-4517-aa49-733c8e87a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_name = \"poc\"\n",
    "# collection_name = \"reviews\"\n",
    "# df_sentiment.write.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/poc.reviews\").mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pymongo\n",
    "\n",
    "\n",
    "\n",
    "# conn = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "# db = conn.poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde8f44-377e-44f2-bd39-1958e9e0ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = db.reviews\n",
    "# query = {\"asin\": \"0209688726\"}\n",
    "# # for res in results:\n",
    "# #     print(\"Document = {}\\n\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b17ac-8201-4099-b202-02ab27ee11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_sentiment = {\"$group\": \n",
    "#                      {\"_id\" : {\"sentiment\":\"$sentiment\"},  # note use a $ on the field\n",
    "#                       \"ct\"  : {\"$sum\":1}\n",
    "#                      }\n",
    "#                   }\n",
    "# results = reviews.aggregate([count_sentiment], allowDiskUse=True)\n",
    "\n",
    "# for res in results:\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74097e4c-d28a-4812-9f04-c6273da70c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a88421-0a3a-4ec4-b0c9-fd25df90a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df7.randomSplit([0.80, 0.20], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc0451-5bdb-4549-bbd1-2262e80ae654",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fc799-6d4b-4354-9faf-e1d9792cdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"label\")\\\n",
    "     .count()\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36091cb-32c8-48db-b235-f9a4b9be8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1646b-6a70-4c07-a0f1-eedaf3c010e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby(\"label\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7261098-eea5-436d-b37b-f1926d37792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c8f41-f513-4f56-8f99-c76f9e102dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac5783-7af1-495e-98ba-b661497226b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from tweets\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "\n",
    "# create entire pipeline\n",
    "basic_pipeline = Pipeline(stages=[tk, tf1, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393d84f-b46f-4884-96df-e830d5953558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1         = basic_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a34354-1062-4284-875a-eae73d4a0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "predictions1   = model1.transform(test)\n",
    "\n",
    "# get the performance on the test set\n",
    "score1         = evaluator.evaluate(predictions1)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b7f03-47f5-44c5-98b7-315010952c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedAndLabels = predictions1.select([\"prediction\",\"label\"])\\\n",
    "                                 .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef59e70-7988-4ae7-af59-2f1e84e99a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4741f-173e-4276-994e-3379b88a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d357c-838f-46bb-aef1-2821673993c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_pipleline  = Pipeline(stages=[tk, sw, tf2, idf, lr])\n",
    "\n",
    "model2        = sw_pipleline.fit(train)\n",
    "predictions2  = model2.transform(test)\n",
    "score2        = evaluator.evaluate(predictions2)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb341530-320c-4ba0-9110-4283af14cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77da5d4-2264-4c48-aaf4-c995281dadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens  = \"my feelings having studied all day\".split(\" \")\n",
    "print(\"raw tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f38e9-ffbb-4428-8290-1e68e81741d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(\"clean tokens: {}\".format(tokens_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d2e5d-4111-4382-b4e3-7a6141789e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b453bc-30b9-466f-b338-ea7ec40f929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3300aaf-3fce-4fa1-bd04-ccab1d906dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e1aa8-ebd9-4273-9b32-961148c4bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem = stem_pipeline.transform(train)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "train_stem.show()\n",
    "\n",
    "# test_stem  = stem_pipeline.transform(test)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# # cache them to avoid running stemming \n",
    "# # each iteration in the grid search\n",
    "# train_stem.cache()\n",
    "# test_stem.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1015b-a383-4be1-a2ce-f0286ffe36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stem.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5c5d6-c9dd-4880-b61b-4f1bc61f6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(inputCol=\"tokens\", outputCol=\"bigrams\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b91cc9-2f2e-4426-a864-f131fb903285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf5   = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "bigram_pipeline  = Pipeline(stages= [tk, bigram, tf5, idf, lr])\n",
    "\n",
    "model5           = bigram_pipeline.fit(train)\n",
    "predictions5     = model5.transform(test)\n",
    "\n",
    "score5           = evaluator.evaluate(predictions5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f2a88-2806-494c-a39d-dd4335a6bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC SCORE: {}\".format(score5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a635b32-bb8e-4ff4-9d11-c36672a12837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "# Create a UDF\n",
    "stem_text_udf = udf(stem_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b599c-5447-4159-b071-59bb2f5e8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "lr      = LogisticRegression(maxIter=20)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "model6                = stem_bigram_pipeline.fit(train_stem)\n",
    "predictions6          = model6.transform(test_stem)\n",
    "\n",
    "score6                = evaluator.evaluate(predictions6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792163d-1cd4-4317-91e2-5f3e8c7b2fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
