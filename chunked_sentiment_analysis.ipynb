{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kravisankaran/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kravisankaran/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad2f9551-bb6a-4992-9e46-78b157aee0c8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 2446ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   83  |   0   |   0   |   5   ||   78  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad2f9551-bb6a-4992-9e46-78b157aee0c8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 78 already retrieved (0kB/46ms)\n",
      "24/04/25 14:55:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "allocated_memory = 18 * 0.75 \n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\")\\\n",
    ".config(\"spark.executor.memory\", \"6g\") \\\n",
    ".master(\"local[*]\")  \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.network.timeout\", \"800s\")\\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"200s\")\\\n",
    ".config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.memory.fraction\", \"0.8\") \\\n",
    ".config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = []\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "def pre_process(chunk):\n",
    "    print(\"Processing chunk\")\n",
    "    df = spark.createDataFrame(json_objects, schema=schema)\n",
    "    reduced_df = df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "    unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "    unique_df = unique_df.filter(unique_df.reviewText.isNotNull())\n",
    "    df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "    return df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import json\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# from pyspark.sql.functions import rand\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# # Define the maximum file size in bytes (10MB)\n",
    "# max_file_size = 10 * 1024 * 1024\n",
    "\n",
    "# json_objects = []\n",
    "\n",
    "# # Read the file line by line until the maximum file size is reached\n",
    "# with open(json_training_file_path, 'r') as file:\n",
    "#     total_size = 0\n",
    "#     for line in file:\n",
    "#         # Calculate the size of the current line\n",
    "#         line_size = sys.getsizeof(line)\n",
    "\n",
    "#         # If adding the current line exceeds the maximum file size, stop reading\n",
    "#         if total_size + line_size > max_file_size:\n",
    "#             print(line_size, total_size)\n",
    "#             # Create a DataFrame from the list of JSON objects\n",
    "#             df = spark.createDataFrame(json_objects)\n",
    "#             reduced_df = df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "#             unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "#             df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "#             tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "#                       outputCol = \"token\")\n",
    "#             # Remove the rows with missing values and tokenize\n",
    "#             df_train_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))\n",
    "#             # remove hashtags, call outs and web addresses\n",
    "#             df4_train = df_train_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized[\"token\"]))\n",
    "#             # remove non english characters\n",
    "#             df4_train = df4_train.withColumn(\"tokens_clean\", normalizeUDF(df4_train[\"tokens_re\"]))\n",
    "#             # rename columns\n",
    "#             df5_train = df4_train.drop(\"token\",\"tokens_re\")\n",
    "#             df5_train = df5_train.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "#             # remove reviews where the tokens array is empty, i.e. where it was just\n",
    "#             # a hashtag, callout, numbers, web adress etc.\n",
    "#             df6_train = df5_train.where(F.size(F.col(\"tokens\")) > 0)\n",
    "#             df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")\n",
    "#             shuffled_train_df = df_train_for_model.orderBy(rand())\n",
    "#             if os.path.exists('bigram_pipeline_model'):\n",
    "#                 loaded_model = PipelineModel.load('bigram_pipeline_model')\n",
    "#                 stages_steps = loaded_model.stages\n",
    "#                 updated_model = Pipeline(stages = stages_steps).fit(shuffled_train_df)\n",
    "#                 shutil.rmtree('bigram_pipeline_model')\n",
    "#             else:\n",
    "#                 updated_model = bigram_pipeline.fit(shuffled_train_df)\n",
    "\n",
    "#             PipelineModel.save(updated_model, 'bigram_pipeline_model')\n",
    "#             print('Model saved')\n",
    "#             json_objects = []\n",
    "#             total_size = 0\n",
    "#         # Otherwise, add the line to the list of JSON objects\n",
    "#         json_objects.append(json.loads(line))\n",
    "#         total_size += line_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "def identify_model_type(model_path):\n",
    "    # Check if metadata exists to identify the model type\n",
    "    metadata_path = os.path.join(model_path, 'metadata')\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(os.path.join(metadata_path, 'part-00000'), 'r') as file:\n",
    "            metadata = json.load(file)\n",
    "            return metadata['class']\n",
    "    else: \n",
    "        try:\n",
    "            model = CrossValidatorModel.load(model_path)\n",
    "            del model\n",
    "            return \"CrossValidatorModel\"\n",
    "        except Exception as e1:\n",
    "            # Try loading the model as PipelineModel\n",
    "            try:\n",
    "                model = PipelineModel.load(model_path)\n",
    "                del model\n",
    "                return \"PipelineModel\"\n",
    "            except Exception as e2:\n",
    "                return \"Unknown model type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/1d/rj0w684n0z533z06yt6wyj2h0000gn/T/ipykernel_54249/41447811.py:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n"
     ]
    }
   ],
   "source": [
    "# Building the pipeline for nlp transformers\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import Tokenizer, DocumentAssembler, StopWordsCleaner, Normalizer, Stemmer\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, NGram\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "paramGrid = ParamGridBuilder() \n",
    "\n",
    "def get_nlp_pipeline():\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"reviewText\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    stopwords_cleaner = StopWordsCleaner()\\\n",
    "        .setInputCols(\"token\")\\\n",
    "        .setOutputCol(\"cleanTokens\")\\\n",
    "        .setCaseSensitive(False)\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"document\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"normalized\")\\\n",
    "        .setLowercase(True)\\\n",
    "        .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
    "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
    " \n",
    "\n",
    "    nlpPipeline = Pipeline(stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        stopwords_cleaner\n",
    "    ])\n",
    "    \n",
    "    return nlpPipeline\n",
    "\n",
    "def get_cross_val_pipeline():\n",
    "    bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "    tf6 = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "    lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "        .build()\n",
    "    stemmer = PorterStemming(inputCol=\"token\", outputCol=\"stemmed\")\n",
    " \n",
    "    stemmed_bigram_pipeline = Pipeline(stages=[bigram2, tf6, idf, lr])\n",
    "    return (stemmed_bigram_pipeline, paramGrid, stemmer)\n",
    "\n",
    "def get_crossval_evaluator():\n",
    "    stemmed_bigram_pipeline, paramGrid, stemmer = get_cross_val_pipeline()\n",
    "    crossval = CrossValidator(estimator= stemmed_bigram_pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=BinaryClassificationEvaluator(),\n",
    "                              numFolds=3,\n",
    "                              parallelism=2\n",
    "                              )\n",
    "    return (crossval, stemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current line size: 509, Total size: 10485463\n",
      "Processing chunk\n",
      "Show the first row of the preprocessed chunk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 14:56:10 WARN TaskSetManager: Stage 3 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|    5.0|A0020356UF96ZV361ST|B00MAN6K54|Karma is a bitch ...|        1|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Show the first row of the transformed chunk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 14:56:13 WARN TaskSetManager: Stage 6 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          reviewText|label|               token|\n",
      "+--------------------+-----+--------------------+\n",
      "|Karma is a bitch ...|    1|[Karma, bitch, ch...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Run 0\n",
      "Printing data frame for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 14:56:15 WARN TaskSetManager: Stage 9 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|          reviewText|label|               token|             stemmed|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|Karma is a bitch ...|    1|[Karma, bitch, ch...|[karma, bitch, ch...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 14:56:20 WARN TaskSetManager: Stage 12 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 14:56:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:56:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:02 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/25 14:57:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:06 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:27 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:57:35 WARN TaskSetManager: Stage 72 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 14:58:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 14:58:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 14:58:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:27 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:36 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:38 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:38 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:40 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:40 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:42 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:42 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:42 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:58:51 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 14:58:51 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 14:58:56 WARN TaskSetManager: Stage 178 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 14:59:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:42 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:54 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:54 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:59 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 14:59:59 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:02 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:02 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:04 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:00:05 WARN TaskSetManager: Stage 242 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:00:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 15:00:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 15:01:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:08 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:13 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:13 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:13 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:16 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:16 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:01:26 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 15:01:26 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 15:01:30 WARN BlockManager: Block rdd_1054_72 could not be removed as it was not found on disk or in memory\n",
      "24/04/25 15:01:30 WARN BlockManager: Block rdd_1054_72 was not removed normally.\n",
      "24/04/25 15:01:30 ERROR BlockManagerStorageEndpoint: Error in removing RDD 1054\n",
      "org.apache.spark.SparkException: Block rdd_1054_72 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2091)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2057)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1993)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/25 15:01:30 WARN BlockManagerMaster: Failed to remove RDD 1054 - Block rdd_1054_72 does not exist\n",
      "org.apache.spark.SparkException: Block rdd_1054_72 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2091)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2057)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1993)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/25 15:01:30 WARN TaskSetManager: Stage 346 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:02:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:16 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:23 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:27 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:27 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:27 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:02:36 WARN TaskSetManager: Stage 406 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:03:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 15:03:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/25 15:03:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:29 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:35 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:36 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:38 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:38 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:40 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:42 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:03:53 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 15:03:53 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/04/25 15:03:59 WARN TaskSetManager: Stage 511 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:04:09 WARN TaskSetManager: Stage 515 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:04:11 WARN TaskSetManager: Stage 516 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:04:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/25 15:04:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:22 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 15:04:33 WARN TaskSetManager: Stage 553 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:04:34 WARN TaskSetManager: Stage 554 contains a task of very large size (1246 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/25 15:04:38 WARN TaskSetManager: Stage 566 contains a task of very large size (3212 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved for current chunk\n",
      "Current line size: 489, Total size: 10485356\n",
      "Processing chunk\n",
      "Show the first row of the preprocessed chunk:\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|    5.0|A0689240FYOX63XMAO3Y|B006YKFYVK|Got it for my 6 M...|        1|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Show the first row of the transformed chunk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          reviewText|label|               token|\n",
      "+--------------------+-----+--------------------+\n",
      "|Got it for my 6 M...|    1|[Got, 6, Month, t...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Run 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m df_train\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m crossval, stemmer \u001b[38;5;241m=\u001b[39m get_crossval_evaluator()\n\u001b[0;32m---> 60\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrossval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df_train\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mrun_cross_validation\u001b[0;34m(df_train, pipeline, stem_pipeline, run)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossval_pipeline_model\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m identify_model_type(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossval_pipeline_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     loaded_model \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossval_pipeline_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m model_type)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossValidatorModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "# Define the maximum file size in bytes (10MB)\n",
    "max_file_size = 10 * 1024 * 1024\n",
    "json_training_file_path = \"combined_train_data_chunked_10mb_latest.json\"\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    " \n",
    "\n",
    "\n",
    "def transform_chunk(df):\n",
    "    nlpPipeline = get_nlp_pipeline()\n",
    "    tokenized_df = nlpPipeline.fit(df).transform(df)\n",
    "    #tokenized_df.select(\"cleanTokens.result\").show(1)\n",
    "    df6_train = tokenized_df.where(F.size(F.col(\"cleanTokens\")) > 0)\n",
    "    df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\", \"cleanTokens.result\").withColumnRenamed(\"sentiment\", \"label\").withColumnRenamed(\"result\", \"token\")\n",
    "    return df_train_for_model\n",
    "\n",
    "def run_cross_validation(df_train, pipeline, stem_pipeline, run):\n",
    "  \n",
    "    train_stem = stem_pipeline.transform(df_train)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "    print(\"Run %d\" % run)\n",
    "    if os.path.exists('crossval_pipeline_model'):\n",
    "        model_type = identify_model_type('crossval_pipeline_model')\n",
    "        \n",
    "        print(\"Model type: %s\" % model_type)\n",
    "        if model_type == 'CrossValidatorModel':\n",
    "            loaded_model = CrossValidatorModel.load('crossval_pipeline_model')\n",
    "            stages_steps = loaded_model.bestModel.stages \n",
    "        elif model_type == 'PipelineModel':\n",
    "            loaded_model = PipelineModel.load('crossval_pipeline_model')\n",
    "            stages_steps = loaded_model.stages\n",
    "        updated_model = Pipeline(stages = stages_steps).fit(train_stem)\n",
    "        updated_model.write().overwrite().save('crossval_pipeline_model')\n",
    "    else:\n",
    "        print(\"Printing data frame for training\")\n",
    "        train_stem.show(1)\n",
    "        updated_model = pipeline.fit(train_stem)\n",
    "        updated_model.write().overwrite().save('crossval_pipeline_model')\n",
    "    run +=1\n",
    "    print('Model saved for current chunk')\n",
    "    return run\n",
    "\n",
    "run = 0\n",
    "with open(json_training_file_path, 'r') as file:\n",
    "    total_size = 0\n",
    "    for line in file:\n",
    "        line_size = sys.getsizeof(line)\n",
    "        # print(\"Current line size: %d, Total size: %d\" % (line_size, total_size))\n",
    "        if total_size + line_size >= max_file_size:\n",
    "            print(\"Current line size: %d, Total size: %d\" % (line_size, total_size))\n",
    "            df  = pre_process(line)\n",
    "            print(\"Show the first row of the preprocessed chunk:\")\n",
    "            df.show(1)\n",
    "            df_train = transform_chunk(df)\n",
    "            print(\"Show the first row of the transformed chunk:\")\n",
    "            df_train.show(1)\n",
    "            crossval, stemmer = get_crossval_evaluator()\n",
    "            run = run_cross_validation(df_train, crossval, stemmer, run)\n",
    "            del df\n",
    "            del df_train\n",
    "            json_objects = []\n",
    "            total_size = 0\n",
    "        json_objects.append(json.loads(line))\n",
    "        total_size += line_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
