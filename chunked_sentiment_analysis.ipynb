{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kravisankaran/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kravisankaran/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7ec213b1-92d8-4237-96a7-8bef2be33ce5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 2382ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   83  |   0   |   0   |   5   ||   78  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7ec213b1-92d8-4237-96a7-8bef2be33ce5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 78 already retrieved (0kB/43ms)\n",
      "24/04/25 11:06:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "allocated_memory = 18 * 0.75 \n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\")\\\n",
    ".config(\"spark.executor.memory\", \"6g\") \\\n",
    ".master(\"local[*]\")  \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.network.timeout\", \"800s\")\\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"200s\")\\\n",
    ".config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.memory.fraction\", \"0.8\") \\\n",
    ".config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = []\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "def pre_process(chunk):\n",
    "    print(\"Processing chunk\")\n",
    "    df = spark.createDataFrame(json_objects, schema=schema)\n",
    "    reduced_df = df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "    unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "    unique_df = unique_df.filter(unique_df.reviewText.isNotNull())\n",
    "    df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "    return df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import json\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# from pyspark.sql.functions import rand\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# # Define the maximum file size in bytes (10MB)\n",
    "# max_file_size = 10 * 1024 * 1024\n",
    "\n",
    "# json_objects = []\n",
    "\n",
    "# # Read the file line by line until the maximum file size is reached\n",
    "# with open(json_training_file_path, 'r') as file:\n",
    "#     total_size = 0\n",
    "#     for line in file:\n",
    "#         # Calculate the size of the current line\n",
    "#         line_size = sys.getsizeof(line)\n",
    "\n",
    "#         # If adding the current line exceeds the maximum file size, stop reading\n",
    "#         if total_size + line_size > max_file_size:\n",
    "#             print(line_size, total_size)\n",
    "#             # Create a DataFrame from the list of JSON objects\n",
    "#             df = spark.createDataFrame(json_objects)\n",
    "#             reduced_df = df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "#             unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "#             df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "#             tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "#                       outputCol = \"token\")\n",
    "#             # Remove the rows with missing values and tokenize\n",
    "#             df_train_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))\n",
    "#             # remove hashtags, call outs and web addresses\n",
    "#             df4_train = df_train_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized[\"token\"]))\n",
    "#             # remove non english characters\n",
    "#             df4_train = df4_train.withColumn(\"tokens_clean\", normalizeUDF(df4_train[\"tokens_re\"]))\n",
    "#             # rename columns\n",
    "#             df5_train = df4_train.drop(\"token\",\"tokens_re\")\n",
    "#             df5_train = df5_train.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "#             # remove reviews where the tokens array is empty, i.e. where it was just\n",
    "#             # a hashtag, callout, numbers, web adress etc.\n",
    "#             df6_train = df5_train.where(F.size(F.col(\"tokens\")) > 0)\n",
    "#             df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")\n",
    "#             shuffled_train_df = df_train_for_model.orderBy(rand())\n",
    "#             if os.path.exists('bigram_pipeline_model'):\n",
    "#                 loaded_model = PipelineModel.load('bigram_pipeline_model')\n",
    "#                 stages_steps = loaded_model.stages\n",
    "#                 updated_model = Pipeline(stages = stages_steps).fit(shuffled_train_df)\n",
    "#                 shutil.rmtree('bigram_pipeline_model')\n",
    "#             else:\n",
    "#                 updated_model = bigram_pipeline.fit(shuffled_train_df)\n",
    "\n",
    "#             PipelineModel.save(updated_model, 'bigram_pipeline_model')\n",
    "#             print('Model saved')\n",
    "#             json_objects = []\n",
    "#             total_size = 0\n",
    "#         # Otherwise, add the line to the list of JSON objects\n",
    "#         json_objects.append(json.loads(line))\n",
    "#         total_size += line_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/1d/rj0w684n0z533z06yt6wyj2h0000gn/T/ipykernel_49607/2762731941.py:31: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n"
     ]
    }
   ],
   "source": [
    "# Building the pipeline for nlp transformers\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import Tokenizer, DocumentAssembler, StopWordsCleaner, Normalizer, Stemmer\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, NGram\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "paramGrid = ParamGridBuilder() \n",
    "\n",
    "def get_nlp_pipeline():\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"reviewText\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    stopwords_cleaner = StopWordsCleaner()\\\n",
    "        .setInputCols(\"token\")\\\n",
    "        .setOutputCol(\"cleanTokens\")\\\n",
    "        .setCaseSensitive(False)\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"document\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"normalized\")\\\n",
    "        .setLowercase(True)\\\n",
    "        .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
    "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
    " \n",
    "\n",
    "    nlpPipeline = Pipeline(stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        stopwords_cleaner\n",
    "    ])\n",
    "    \n",
    "    return nlpPipeline\n",
    "\n",
    "def get_cross_val_pipeline(stemmer):\n",
    "    bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "    tf6 = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "    lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "        .build()\n",
    "    stem2 = PorterStemming(inputCol=\"token\", outputCol=\"stemmed\")\n",
    " \n",
    "    stemmed_bigram_pipeline = Pipeline(stages=[bigram2, tf6, idf, lr])\n",
    "    return (stemmed_bigram_pipeline, paramGrid, stem2)\n",
    "\n",
    "def get_crossval_evaluator():\n",
    "    stemmer = Stemmer() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"stemmed\")\n",
    "    stemmed_bigram_pipeline, paramGrid, stem2 = get_cross_val_pipeline(stemmer)\n",
    "    \n",
    "    crossval = CrossValidator(estimator= stemmed_bigram_pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=BinaryClassificationEvaluator(),\n",
    "                              numFolds=2,\n",
    "                              parallelism=2\n",
    "                              )\n",
    "    return (crossval, stem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current line size: 509, Total size: 10485463\n",
      "Processing chunk\n",
      "Show the first row of the preprocessed chunk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|    5.0|A0020356UF96ZV361ST|B00MAN6K54|Karma is a bitch ...|        1|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Show the first row of the transformed chunk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          reviewText|label|               token|\n",
      "+--------------------+-----+--------------------+\n",
      "|Karma is a bitch ...|    1|[Karma, bitch, ch...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Printing data frame for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|          reviewText|label|               token|             stemmed|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|Best pads I have ...|    1|[Best, pads, foun...|[best, pad, found...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 11:07:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:47 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/25 11:07:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:49 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:54 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:07:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:04 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:04 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:06 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:08 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:08 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:10 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:14 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:16 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:20 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/04/25 11:08:20 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/04/25 11:08:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/25 11:08:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "[Stage 106:=======>    (122 + 12) / 200][Stage 108:>              (0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Define the maximum file size in bytes (10MB)\n",
    "max_file_size = 10 * 1024 * 1024\n",
    "json_training_file_path = \"combined_train_data_chunked_10mb_latest.json\"\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "def transform_chunk(df):\n",
    "    nlpPipeline = get_nlp_pipeline()\n",
    "    tokenized_df = nlpPipeline.fit(df).transform(df)\n",
    "    #tokenized_df.select(\"cleanTokens.result\").show(1)\n",
    "    df6_train = tokenized_df.where(F.size(F.col(\"cleanTokens\")) > 0)\n",
    "    df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\", \"cleanTokens.result\").withColumnRenamed(\"sentiment\", \"label\").withColumnRenamed(\"result\", \"token\")\n",
    "    return df_train_for_model\n",
    "\n",
    "def run_cross_validation(df_train, pipeline, stem_pipeline):\n",
    "    train_stem = stem_pipeline.transform(df_train)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "    train_stem.cache()\n",
    "    if os.path.exists('crossval_pipeline_model'):\n",
    "        loaded_model = CrossValidatorModel.load('crossval_pipeline_model')\n",
    "        stages_steps = loaded_model.bestModel.stages\n",
    "        updated_model = Pipeline(stages = stages_steps).fit(df_train)\n",
    "        shutil.rmtree('crossval_pipeline_model')\n",
    "    else:\n",
    "        print(\"Printing data frame for training\")\n",
    "        train_stem.show(1)\n",
    "        updated_model = pipeline.fit(train_stem)\n",
    "\n",
    "    CrossValidatorModel.save(updated_model, 'crossval_pipeline_model')\n",
    "    print('Model saved for cuurent chunk')\n",
    "\n",
    "with open(json_training_file_path, 'r') as file:\n",
    "    total_size = 0\n",
    "    for line in file:\n",
    "        line_size = sys.getsizeof(line)\n",
    "        # print(\"Current line size: %d, Total size: %d\" % (line_size, total_size))\n",
    "        if total_size + line_size >= max_file_size:\n",
    "            print(\"Current line size: %d, Total size: %d\" % (line_size, total_size))\n",
    "            df  = pre_process(line)\n",
    "            print(\"Show the first row of the preprocessed chunk:\")\n",
    "            df.show(1)\n",
    "            df_train = transform_chunk(df)\n",
    "            print(\"Show the first row of the transformed chunk:\")\n",
    "            df_train.show(1)\n",
    "            crossval, stem2 = get_crossval_evaluator()\n",
    "            run_cross_validation(df_train, crossval, stem2)\n",
    "            del df\n",
    "            del df_train\n",
    "            json_objects = []\n",
    "            total_size = 0\n",
    "        json_objects.append(json.loads(line))\n",
    "        total_size += line_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
