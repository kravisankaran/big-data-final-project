{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/13 18:11:42 WARN Utils: Your hostname, albertastein-2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.76 instead (on interface en0)\n",
      "24/04/13 18:11:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/13 18:11:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").config(\"spark.executor.memory\", \"4g\").config(\"spark.driver.memory\", \"4g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|  reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|    5.0|    true| 04 5, 2016|A1274GG1EB2JLJ|0486427706|{NULL}|   barbara ann|The pictures are ...|    1459814400|\n",
      "|    5.0|    true|02 13, 2016|A30X5EGBYAZQQK|0486427706|{NULL}|      Samantha|I absolutely love...|    1455321600|\n",
      "|    5.0|    true|12 10, 2015|A3U6UNXLAUY6ZV|0486427706|{NULL}|   CP in Texas|          I love it!|    1449705600|\n",
      "|    5.0|    true|10 26, 2015|A1SAJF5SNM6WJS|0486427706|{NULL}|   LOIS LABIER|MY HUSBAND LOVED ...|    1445817600|\n",
      "|    4.0|    true|09 15, 2015| AHJWO3SI0S0OR|0486427706|{NULL}|Saundra Hatley|                cool|    1442275200|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "json_df = spark.read.schema(schema).json(\"combined_train_data_chunked_10mb_latest.json\")\n",
    "json_df.show(5)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  11321389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of entries in the dataframe\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = json_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A1274GG1EB2JLJ|0486427706|The pictures are ...|\n",
      "|    5.0|A30X5EGBYAZQQK|0486427706|I absolutely love...|\n",
      "|    5.0|A3U6UNXLAUY6ZV|0486427706|          I love it!|\n",
      "|    5.0|A1SAJF5SNM6WJS|0486427706|MY HUSBAND LOVED ...|\n",
      "|    4.0| AHJWO3SI0S0OR|0486427706|                cool|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe after removing duplicates:  10936206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of entries in the dataframe after removing duplicates: \", unique_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=65340) is multi-threaded, use of fork() may lead to deadlocks in the child.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|\n",
      "|    3.0|A00463782V7TKAP9EMNL|B002B3YBSQ|What a sad thing ...|        0|\n",
      "|    4.0|A00463782V7TKAP9EMNL|B003SNJYOO|You may love some...|        1|\n",
      "|    3.0|A00463782V7TKAP9EMNL|B00LB8LSIS|If he had any kin...|        0|\n",
      "|    5.0|A00463782V7TKAP9EMNL|B00WLD2OQ2|K. Sterling has w...|        1|\n",
      "|    3.0|A00463782V7TKAP9EMNL|B00ZAUVLOY|Two young men are...|        0|\n",
      "|    5.0|A00463782V7TKAP9EMNL|B016XD4RH4|It's tough to try...|        1|\n",
      "|    4.0|A0096681Y127OL1H8W3U|B000HHNYHW|        good product|        1|\n",
      "|    5.0|A0096681Y127OL1H8W3U|B00U3SK7UW|good product heal...|        1|\n",
      "|    5.0|A0103047AS0C8QKUI0X2|B00HZHB0NC|Very interesting ...|        1|\n",
      "|    5.0|A01433701FZ0LF5G2RUL|B00US1SWAQ|This was a wonder...|        1|\n",
      "|    4.0|A01631062UX24GI4LJKF|B00YO22D0U|It was a good rea...|        1|\n",
      "|    5.0|A01631062UX24GI4LJKF|B017Y0YYJ6|This book was bey...|        1|\n",
      "|    5.0|A0165490GQW9X6ELEQG8|B00CNIDH7K|Totally Surprised...|        1|\n",
      "|    5.0|A0178408Z1TQAM7D75FY|B015HK93WI|I really enjoyed ...|        1|\n",
      "|    4.0|A0178408Z1TQAM7D75FY|B01GSTKE48|This was a good r...|        1|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "# Remove the rows with missing values and tokenize\n",
    "df_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:======================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|[these, collars, ...|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|[it, is, a, love,...|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|[this, book, is, ...|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|[looks, great, fi...|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|         [excellent]|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    # Use a raw string for regular expressions to avoid escape sequence warnings\n",
    "    expr = r'(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "           r'(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "    regex = re.compile(expr)\n",
    "    cleaned = [t for t in tokens if not regex.search(t) and len(t) > 0]\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4 = df_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_tokenized[\"token\"]))\n",
    "\n",
    "# remove non english characters\n",
    "df4 = df4.withColumn(\"tokens_clean\", normalizeUDF(df4[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5 = df4.drop(\"token\",\"tokens_re\")\n",
    "df5 = df5.withColumnRenamed(\"tokens_clean\", \"tokens\")\\\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6 = df5.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model = df6.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|I purchased this ...|    1|\n",
      "|I always keep one...|    1|\n",
      "|I only have one p...|    1|\n",
      "|Not a perfect fit...|    1|\n",
      "|Great sander for ...|    1|\n",
      "|have this in the ...|    1|\n",
      "|so far so good.  ...|    1|\n",
      "|Many great songs ...|    1|\n",
      "|The ship is great...|    1|\n",
      "|                good|    1|\n",
      "|Son didn't like i...|    0|\n",
      "|I bought the orig...|    1|\n",
      "|I live way up in ...|    1|\n",
      "|I was hoping that...|    0|\n",
      "|The album has som...|    1|\n",
      "|Very useful for s...|    1|\n",
      "|I love this showe...|    1|\n",
      "|This is a magneti...|    1|\n",
      "|My partner report...|    1|\n",
      "|worked fine for m...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "shuffled_df = df_for_model.orderBy(rand())\n",
    "\n",
    "# Show the shuffled DataFrame\n",
    "shuffled_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
