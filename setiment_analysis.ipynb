{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/14 15:47:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").config(\"spark.executor.memory\", \"4g\").config(\"spark.driver.memory\", \"4g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|  reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|    5.0|    true| 04 5, 2016|A1274GG1EB2JLJ|0486427706|{NULL}|   barbara ann|The pictures are ...|    1459814400|\n",
      "|    5.0|    true|02 13, 2016|A30X5EGBYAZQQK|0486427706|{NULL}|      Samantha|I absolutely love...|    1455321600|\n",
      "|    5.0|    true|12 10, 2015|A3U6UNXLAUY6ZV|0486427706|{NULL}|   CP in Texas|          I love it!|    1449705600|\n",
      "|    5.0|    true|10 26, 2015|A1SAJF5SNM6WJS|0486427706|{NULL}|   LOIS LABIER|MY HUSBAND LOVED ...|    1445817600|\n",
      "|    4.0|    true|09 15, 2015| AHJWO3SI0S0OR|0486427706|{NULL}|Saundra Hatley|                cool|    1442275200|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|    5.0|    true| 10 6, 2013|A2LSCFZM2FBZK7|0486427706|{NULL}|              Ginger|The stained glass...|    1381017600|\n",
      "|    5.0|    true| 08 9, 2013|A3IXP5VS847GE5|0486427706|{NULL}|Dragonflies &amp;...|My 11 y.o. loved ...|    1376006400|\n",
      "|    5.0|    true|10 12, 2015|A2HK5AVQW6AUQ5|0486427706|{NULL}|     scootergirl1952|             love it|    1444608000|\n",
      "|    5.0|    true|04 20, 2011|A18MVTKTTE8OS8|0486448789|{NULL}|              JudiAU|Sometimes you nee...|    1303257600|\n",
      "|    5.0|   false| 04 2, 2011|A2C2TLRMMMLAJV|0486448789|{NULL}|          D. Workman|These little book...|    1301702400|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "json_df = spark.read.schema(schema).json(\"combined_train_data_chunked_10mb_latest.json\")\n",
    "json_test_df = spark.read.schema(schema).json(\"combined_test_data_chunked_10mb_latest.json\")\n",
    "json_df.show(5)\n",
    "json_test_df.show(5)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  11321389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  2832499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of entries in the dataframe\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_df.count())\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = json_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "reduced_test_df = json_test_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A1274GG1EB2JLJ|0486427706|The pictures are ...|\n",
      "|    5.0|A30X5EGBYAZQQK|0486427706|I absolutely love...|\n",
      "|    5.0|A3U6UNXLAUY6ZV|0486427706|          I love it!|\n",
      "|    5.0|A1SAJF5SNM6WJS|0486427706|MY HUSBAND LOVED ...|\n",
      "|    4.0| AHJWO3SI0S0OR|0486427706|                cool|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A2LSCFZM2FBZK7|0486427706|The stained glass...|\n",
      "|    5.0|A3IXP5VS847GE5|0486427706|My 11 y.o. loved ...|\n",
      "|    5.0|A2HK5AVQW6AUQ5|0486427706|             love it|\n",
      "|    5.0|A18MVTKTTE8OS8|0486448789|Sometimes you nee...|\n",
      "|    5.0|A2C2TLRMMMLAJV|0486448789|These little book...|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(5)\n",
    "reduced_test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries in the dataframe after removing duplicates:  10936206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing entries in the dataframe after removing duplicates:  2806599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of training entries in the dataframe after removing duplicates: \", unique_df.count())\n",
    "\n",
    "unique_test_df = reduced_test_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of testing entries in the dataframe after removing duplicates: \", unique_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "df_test_sentiment = unique_test_df.withColumn(\"sentiment\", targetUDF(unique_test_df[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=67445) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:==============================>                          (7 + 6) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sentiment.show(5)\n",
    "df_test_sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "# Remove the rows with missing values and tokenize\n",
    "df_train_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))\n",
    "df_test_tokenized = tokenizer.transform(df_test_sentiment.filter(unique_test_df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|[these, collars, ...|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|[it, is, a, love,...|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|[this, book, is, ...|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|[looks, great, fi...|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|         [excellent]|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|[i, guess, you, c...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|[this, girl, has,...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|[mario, is, a, so...|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|[this, father, ha...|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|[this, guy, is, a...|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_tokenized.show(5)\n",
    "df_test_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    # Use a raw string for regular expressions to avoid escape sequence warnings\n",
    "    expr = r'(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "           r'(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "    regex = re.compile(expr)\n",
    "    cleaned = [t for t in tokens if not regex.search(t) and len(t) > 0]\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4_train = df_train_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized[\"token\"]))\n",
    "df4_test = df_test_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_test_tokenized[\"token\"]))\n",
    "# remove non english characters\n",
    "df4_train = df4_train.withColumn(\"tokens_clean\", normalizeUDF(df4_train[\"tokens_re\"]))\n",
    "df4_test = df4_test.withColumn(\"tokens_clean\", normalizeUDF(df4_test[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5_train = df4_train.drop(\"token\",\"tokens_re\")\n",
    "df5_test = df4_test.drop(\"token\",\"tokens_re\")\n",
    "df5_train = df5_train.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "df5_test = df5_test.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6_train = df5_train.where(F.size(F.col(\"tokens\")) > 0)\n",
    "df6_test = df5_test.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")\n",
    "df_test_for_model = df6_test.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|Positive- works w...|    1|\n",
      "|Love these.  Work...|    1|\n",
      "|Bruder's are a go...|    1|\n",
      "|       I love it!!!!|    1|\n",
      "|Good amount of po...|    1|\n",
      "|Design is the pri...|    1|\n",
      "|Great saw for a g...|    1|\n",
      "|Good book, You ca...|    1|\n",
      "|To much sugar, no...|    0|\n",
      "|Loved the crossov...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|This book was way...|    1|\n",
      "|only one time the...|    1|\n",
      "|How HEAVENLY are ...|    1|\n",
      "|Just put it on to...|    1|\n",
      "|Bright light here...|    1|\n",
      "|Works great! I ha...|    1|\n",
      "|I am adding to th...|    1|\n",
      "|Arrived as expect...|    1|\n",
      "|This is a fast pa...|    1|\n",
      "|The claws give it...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "shuffled_train_df = df_train_for_model.orderBy(rand())\n",
    "shuffled_test_df = df_test_for_model.orderBy(rand())\n",
    "\n",
    "# Show the shuffled DataFrame\n",
    "shuffled_train_df.show(10)\n",
    "shuffled_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:53:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:54:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|9223524|\n",
      "|    0|1703907|\n",
      "+-----+-------+\n",
      "\n",
      "Testing data label distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 15:55:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/14 15:55:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 48:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|2368278|\n",
      "|    0| 435983|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Printing label distribution for training and testing data\n",
    "print(\"Training data label distribution\")\n",
    "shuffled_train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"Testing data label distribution\")\n",
    "shuffled_test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"model1.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
