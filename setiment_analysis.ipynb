{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/22 19:50:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "allocated_memory = 18 * 0.75 \n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\")\\\n",
    ".config(\"spark.executor.memory\", \"6g\") \\\n",
    ".master(\"local[*]\")  \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.network.timeout\", \"800s\")\\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"120s\")\\\n",
    ".config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.memory.fraction\", \"0.8\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism : 12\n"
     ]
    }
   ],
   "source": [
    "print('Default Parallelism :', spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Size: 134217728 in bytes and 128.0 in MB\n"
     ]
    }
   ],
   "source": [
    "# Check the default partition size\n",
    "partition_size = spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\")\n",
    "print(f\"Partition Size: {partition_size} in bytes and {int(partition_size) / 1024 / 1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|  reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|    5.0|    true| 04 5, 2016|A1274GG1EB2JLJ|0486427706|{NULL}|   barbara ann|The pictures are ...|    1459814400|\n",
      "|    5.0|    true|02 13, 2016|A30X5EGBYAZQQK|0486427706|{NULL}|      Samantha|I absolutely love...|    1455321600|\n",
      "|    5.0|    true|12 10, 2015|A3U6UNXLAUY6ZV|0486427706|{NULL}|   CP in Texas|          I love it!|    1449705600|\n",
      "|    5.0|    true|10 26, 2015|A1SAJF5SNM6WJS|0486427706|{NULL}|   LOIS LABIER|MY HUSBAND LOVED ...|    1445817600|\n",
      "|    4.0|    true|09 15, 2015| AHJWO3SI0S0OR|0486427706|{NULL}|Saundra Hatley|                cool|    1442275200|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|    5.0|    true| 10 6, 2013|A2LSCFZM2FBZK7|0486427706|{NULL}|              Ginger|The stained glass...|    1381017600|\n",
      "|    5.0|    true| 08 9, 2013|A3IXP5VS847GE5|0486427706|{NULL}|Dragonflies &amp;...|My 11 y.o. loved ...|    1376006400|\n",
      "|    5.0|    true|10 12, 2015|A2HK5AVQW6AUQ5|0486427706|{NULL}|     scootergirl1952|             love it|    1444608000|\n",
      "|    5.0|    true|04 20, 2011|A18MVTKTTE8OS8|0486448789|{NULL}|              JudiAU|Sometimes you nee...|    1303257600|\n",
      "|    5.0|   false| 04 2, 2011|A2C2TLRMMMLAJV|0486448789|{NULL}|          D. Workman|These little book...|    1301702400|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "json_df = spark.read.schema(schema).json(\"combined_train_data_chunked_10mb_latest.json\")\n",
    "json_test_df = spark.read.schema(schema).json(\"combined_test_data_chunked_10mb_latest.json\")\n",
    "json_df.show(5)\n",
    "json_test_df.show(5)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  11321389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===========================================>             (10 + 3) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  2832499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of entries in the dataframe\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_df.count())\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = json_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "reduced_test_df = json_test_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A1274GG1EB2JLJ|0486427706|The pictures are ...|\n",
      "|    5.0|A30X5EGBYAZQQK|0486427706|I absolutely love...|\n",
      "|    5.0|A3U6UNXLAUY6ZV|0486427706|          I love it!|\n",
      "|    5.0|A1SAJF5SNM6WJS|0486427706|MY HUSBAND LOVED ...|\n",
      "|    4.0| AHJWO3SI0S0OR|0486427706|                cool|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A2LSCFZM2FBZK7|0486427706|The stained glass...|\n",
      "|    5.0|A3IXP5VS847GE5|0486427706|My 11 y.o. loved ...|\n",
      "|    5.0|A2HK5AVQW6AUQ5|0486427706|             love it|\n",
      "|    5.0|A18MVTKTTE8OS8|0486448789|Sometimes you nee...|\n",
      "|    5.0|A2C2TLRMMMLAJV|0486448789|These little book...|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(5)\n",
    "reduced_test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries in the dataframe after removing duplicates:  10936206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing entries in the dataframe after removing duplicates:  2806599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of training entries in the dataframe after removing duplicates: \", unique_df.count())\n",
    "\n",
    "unique_test_df = reduced_test_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of testing entries in the dataframe after removing duplicates: \", unique_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "df_test_sentiment = unique_test_df.withColumn(\"sentiment\", targetUDF(unique_test_df[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sentiment.show(5)\n",
    "df_test_sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "# Remove the rows with missing values and tokenize\n",
    "df_train_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))\n",
    "df_test_tokenized = tokenizer.transform(df_test_sentiment.filter(unique_test_df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|[these, collars, ...|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|[it, is, a, love,...|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|[this, book, is, ...|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|[looks, great, fi...|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|         [excellent]|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|[i, guess, you, c...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|[this, girl, has,...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|[mario, is, a, so...|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|[this, father, ha...|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|[this, guy, is, a...|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_tokenized.show(5)\n",
    "df_test_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    # Use a raw string for regular expressions to avoid escape sequence warnings\n",
    "    expr = r'(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "           r'(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "    regex = re.compile(expr)\n",
    "    cleaned = [t for t in tokens if not regex.search(t) and len(t) > 0]\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4_train = df_train_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized[\"token\"]))\n",
    "df4_test = df_test_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_test_tokenized[\"token\"]))\n",
    "# remove non english characters\n",
    "df4_train = df4_train.withColumn(\"tokens_clean\", normalizeUDF(df4_train[\"tokens_re\"]))\n",
    "df4_test = df4_test.withColumn(\"tokens_clean\", normalizeUDF(df4_test[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5_train = df4_train.drop(\"token\",\"tokens_re\")\n",
    "df5_test = df4_test.drop(\"token\",\"tokens_re\")\n",
    "df5_train = df5_train.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "df5_test = df5_test.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6_train = df5_train.where(F.size(F.col(\"tokens\")) > 0)\n",
    "df6_test = df5_test.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")\n",
    "df_test_for_model = df6_test.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|         As expected|    1|\n",
      "|A nice, cute hear...|    1|\n",
      "|          excellent!|    1|\n",
      "|Good quality, was...|    1|\n",
      "|We bought them fo...|    1|\n",
      "|Cats come running...|    1|\n",
      "|I don't usually t...|    1|\n",
      "|i was astonished ...|    0|\n",
      "|  they are very nice|    1|\n",
      "|Junk!!! I purchas...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|These treats are ...|    1|\n",
      "|I don't write man...|    1|\n",
      "|If the writer was...|    0|\n",
      "|works really nice...|    1|\n",
      "|Got this for my d...|    1|\n",
      "|This has a very h...|    1|\n",
      "|It is pretty hand...|    1|\n",
      "|Two of our cats g...|    1|\n",
      "|It is inexplicabl...|    1|\n",
      "|Cute and all, I w...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "shuffled_train_df = df_train_for_model.orderBy(rand())\n",
    "shuffled_test_df = df_test_for_model.orderBy(rand())\n",
    "\n",
    "# Show the shuffled DataFrame\n",
    "shuffled_train_df.show(10)\n",
    "shuffled_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing label distribution for training and testing data\n",
    "# print(\"Training data label distribution\")\n",
    "# shuffled_train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# print(\"Testing data label distribution\")\n",
    "# shuffled_test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"model1.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_train_df_partition = shuffled_train_df.repartition(5)\n",
    "# shuffled_test_df_partition = shuffled_test_df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shuffled_train_df_partition.rdd.getNumPartitions())\n",
    "# print(shuffled_test_df_partition.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_shuffle_small = spark.sparkContext.parallelize(shuffled_train_df.take(50000)).toDF()\n",
    "# test_shuffle_small = spark.sparkContext.parallelize(shuffled_test_df.take(50000)).toDF()\n",
    "\n",
    "#print(type(test_shuffle_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# # w = Window().orderBy(F.lit('A'))\n",
    "# # train_shuffle_small = train_shuffle_small.withColumn('row_num', F.floor(F.row_number().over(w) / 10) )\n",
    "# # test_shuffle_small = test_shuffle_small.withColumn('row_num', F.floor(F.row_number().over(w) / 10) )\n",
    "\n",
    "# # train_shuffle_small.show(2)\n",
    "# train_shuffle_small = train_shuffle_small.withColumn('original_order', monotonically_increasing_id())\n",
    "# train_shuffle_small = train_shuffle_small.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "# train_shuffle_small = train_shuffle_small.drop('original_order')\n",
    "\n",
    "# train_shuffle_small.show(2)\n",
    "# test_shuffle_small = test_shuffle_small.withColumn('original_order', monotonically_increasing_id())\n",
    "# test_shuffle_small = test_shuffle_small.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "# test_shuffle_small = test_shuffle_small.drop('original_order')\n",
    "# test_shuffle_small.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 1) & (col('row_num') <=10000))\n",
    "# #test_shuffle_small = test_shuffle_small.filter(test_shuffle_small.row_num(1, 10000))\n",
    "# train_shuffle_small.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "# # test_stem  = stem_pipeline.transform(shuffled_test_df_partition)\\\n",
    "# #                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# test_stem  = stem_pipeline.transform(test_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# # cache them to avoid running stemming \n",
    "# # each iteration in the grid search\n",
    "# train_stem.cache()\n",
    "# test_stem.cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_small = train_stem.take(10)\n",
    "# rdd = spark.sparkContext.parallelize(train_small)\n",
    "# train_small_df = rdd.toDF().show(2)\n",
    "# train_small_df.typedf()\n",
    "# print(type(train_small_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_small = test_stem.take(10)\n",
    "# rdd2 = spark.sparkContext.parallelize(test_small)\n",
    "# test_small_df = rdd2.toDF().show(2)\n",
    "# print(type(test_small_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import NGram\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "# tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "# idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# lr      = LogisticRegression(maxIter=10)\n",
    "\n",
    "# stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#                         .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "#                         .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "#                         .build()\n",
    "# crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "#                           estimatorParamMaps = paramGrid,\n",
    "#                           evaluator          = BinaryClassificationEvaluator(),\n",
    "#                           numFolds           = 2,\n",
    "#                           parallelism= 2\n",
    "#                           )\n",
    "\n",
    "\n",
    "# model    = crossval.fit(train_stem)\n",
    "# predictions   = model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"AUC SCORE for cross validation: {}\".format(score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"cross_val_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "# model_path = \"cross_val_model_final\"\n",
    "# model = CrossValidatorModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 10001) & (col('row_num') <=20000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.bestModel.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"cross_val_model_final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transform(test_stem).show(5)\n",
    "\n",
    "# predictions   = model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE for cross validation: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# bestModel = model.bestModel\n",
    "# predictedAndLabels = predictions.select([\"prediction\",\"label\"])\\\n",
    "#                                 .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "# metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "# print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"cross_val_model_final2\"\n",
    "# model = CrossValidatorModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 20001) & (col('row_num') <=50000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.bestModel.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 20:09:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:09:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:09:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:09:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:09:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+\n",
      "|          reviewText|label|row_num|\n",
      "+--------------------+-----+-------+\n",
      "|         As expected|    1|      1|\n",
      "|A nice, cute hear...|    1|      2|\n",
      "+--------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 20:13:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:13:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:14:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:14:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:14:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 58:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+\n",
      "|          reviewText|label|row_num|\n",
      "+--------------------+-----+-------+\n",
      "|These treats are ...|    1|      1|\n",
      "|I don't write man...|    1|      2|\n",
      "+--------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "shuffled_train_df = shuffled_train_df.withColumn('original_order', monotonically_increasing_id())\n",
    "shuffled_train_df = shuffled_train_df.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "shuffled_train_df = shuffled_train_df.drop('original_order')\n",
    "\n",
    "shuffled_train_df.show(2)\n",
    "shuffled_test_df = shuffled_test_df.withColumn('original_order', monotonically_increasing_id())\n",
    "shuffled_test_df = shuffled_test_df.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "shuffled_test_df = shuffled_test_df.drop('original_order')\n",
    "shuffled_test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:18:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int, row_num: int, tokens: array<string>, stemmed: array<string>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "train_shuffle_small = shuffled_train_df.filter( (col('row_num') >= 1) & (col('row_num') <=50000))\n",
    "#stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "#train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "                          #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "# test_stem  = stem_pipeline.transform(shuffled_test_df_partition)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "test_stem  = stem_pipeline.transform(shuffled_test_df)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# cache them to avoid running stemming \n",
    "# each iteration in the grid search\n",
    "train_stem.cache()\n",
    "test_stem.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 20:19:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:19:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:19:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:19:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:22:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:22:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:23:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:23:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:23:55 WARN BlockManager: Block rdd_239_0 already exists on this machine; not re-adding it\n",
      "24/04/22 20:24:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/22 20:24:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:11 WARN BlockManager: Block rdd_355_0 already exists on this machine; not re-adding it\n",
      "24/04/22 20:24:12 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 20:24:12 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 20:24:20 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:20 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:26 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/04/22 20:24:26 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/04/22 20:24:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:24:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:24:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:24:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:24:31 WARN BlockManager: Block rdd_551_0 already exists on this machine; not re-adding it\n",
      "24/04/22 20:24:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:40 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 20:24:40 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/04/22 20:24:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/04/22 20:25:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/22 20:25:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:25:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:26:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:26:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/22 20:30:27 WARN MemoryStore: Not enough space to cache rdd_943_0 in memory! (computed 1674.3 MiB so far)\n",
      "24/04/22 20:30:27 WARN BlockManager: Persisting block rdd_943_0 to disk instead.\n",
      "24/04/22 20:41:36 WARN MemoryStore: Not enough space to cache rdd_943_0 in memory! (computed 2.5 GiB so far)\n",
      "24/04/22 20:41:36 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 20:41:41 WARN MemoryStore: Not enough space to cache rdd_943_0 in memory! (computed 2.5 GiB so far)\n",
      "[Stage 528:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.8242746099293129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr      = LogisticRegression(maxIter=10)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "                        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "                        .build()\n",
    "crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator          = BinaryClassificationEvaluator(),\n",
    "                          numFolds           = 2,\n",
    "                          parallelism= 2\n",
    "                          )\n",
    "\n",
    "\n",
    "model    = crossval.fit(train_stem)\n",
    "predictions   = model.transform(test_stem)\n",
    "score         = evaluator.evaluate(predictions)\n",
    "print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 20:46:49 WARN TaskSetManager: Stage 540 contains a task of very large size (3196 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/22 20:46:50 WARN TaskSetManager: Stage 544 contains a task of very large size (1602 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cross_val_model_firstRun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "# model_path = \"cross_val_model_first50K\"\n",
    "\n",
    "# model = CrossValidatorModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 50001) & (col('row_num') <=100000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.bestModel.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_model.save(\"cross_val_model_100K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# model_path = \"cross_val_model_100K\"\n",
    "\n",
    "# model = PipelineModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 100001) & (col('row_num') <=150000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_model.save(\"cross_val_model_200K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# model_path = \"cross_val_model_200K\"\n",
    "\n",
    "# model = PipelineModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 150001) & (col('row_num') <=200000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_model.save(\"cross_val_model_300K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# model_path = \"cross_val_model_300K\"\n",
    "\n",
    "# model = PipelineModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 200000) & (col('row_num') <=270000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n",
    "# updated_model.save(\"cross_val_model_400K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# model_path = \"cross_val_model_400K\"\n",
    "\n",
    "# model = PipelineModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 270001) & (col('row_num') <=340000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n",
    "# updated_model.save(\"cross_val_model_5Runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 667:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10927431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 340001) & (col('row_num') <=400000))\n",
    "rows = shuffled_train_df.count()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 21:21:46 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 21:21:50 WARN MemoryStore: Not enough space to cache rdd_943_0 in memory! (computed 2.5 GiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE in this run : 0.8242746099293129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 21:26:03 WARN TaskSetManager: Stage 852 contains a task of very large size (3196 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/22 21:26:04 WARN TaskSetManager: Stage 856 contains a task of very large size (1602 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with name :  cross_val_model_2_Runs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 21:26:05 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/22 21:26:09 WARN MemoryStore: Not enough space to cache rdd_943_0 in memory! (computed 2.5 GiB so far)\n",
      "[Stage 890:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE in this run : 0.8242746099293129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 21:30:18 ERROR Instrumentation: java.io.IOException: Path cross_val_model_2_Runs already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8925.save.\n: java.io.IOException: Path cross_val_model_2_Runs already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC SCORE in this run : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(score))\n\u001b[1;32m     26\u001b[0m file_name_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_val_model_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Runs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mupdated_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved with name : \u001b[39m\u001b[38;5;124m\"\u001b[39m, file_name_string)\n\u001b[1;32m     29\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70000\u001b[39m\n",
      "File \u001b[0;32m~/trial/lib/python3.12/site-packages/pyspark/ml/util.py:262\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/trial/lib/python3.12/site-packages/pyspark/ml/util.py:213\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/trial/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/trial/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/trial/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8925.save.\n: java.io.IOException: Path cross_val_model_2_Runs already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "i = 50001\n",
    "# 10927431\n",
    "file_name_first = \"cross_val_model_firstRun\"\n",
    "file_name_string = \"\"\n",
    "file_name = 1\n",
    "\n",
    "for i in range(50001, 10927431, 70000):\n",
    "    # Add your indented block of code here\n",
    "    if i == 50001:\n",
    "        model_path = \"cross_val_model_firstRun\"\n",
    "        model = CrossValidatorModel.load(model_path)\n",
    "        stages_steps = model.bestModel.stages\n",
    "    else:\n",
    "        model_path = file_name_string\n",
    "        model = PipelineModel.load(model_path)\n",
    "        stages_steps = model.stages\n",
    "    train_shuffle_small = shuffled_train_df.filter( (col('row_num') >= i) & (col('row_num') <=i+70000))\n",
    "    stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "    stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "    updated_model = Pipeline(stages = stages_steps).fit(train_stem)\n",
    "    predictions = updated_model.transform(test_stem)\n",
    "    score         = evaluator.evaluate(predictions)\n",
    "    print(\"AUC SCORE in this run : {}\".format(score))\n",
    "    file_name_string = \"cross_val_model_\" + str(file_name + 1) + \"_Runs\"\n",
    "    updated_model.save(file_name_string)\n",
    "    print(\"Model saved with name : \", file_name_string)\n",
    "    file_name += 1\n",
    "    i += 70000\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
