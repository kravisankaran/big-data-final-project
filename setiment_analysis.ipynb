{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 12:42:18 WARN Utils: Your hostname, albertastein-2.local resolves to a loopback address: 127.0.0.1; using 10.2.12.43 instead (on interface en0)\n",
      "24/04/22 12:42:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/22 12:42:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "allocated_memory = 18 * 0.75 \n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\")\\\n",
    ".config(\"spark.executor.memory\", \"6g\") \\\n",
    ".master(\"local[*]\")  \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.network.timeout\", \"800s\")\\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"120s\")\\\n",
    ".config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    ".config(\"spark.memory.fraction\", \"0.8\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism : 12\n"
     ]
    }
   ],
   "source": [
    "print('Default Parallelism :', spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Size: 134217728 in bytes and 128.0 in MB\n"
     ]
    }
   ],
   "source": [
    "# Check the default partition size\n",
    "partition_size = spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\")\n",
    "print(f\"Partition Size: {partition_size} in bytes and {int(partition_size) / 1024 / 1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|  reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "|    5.0|    true| 04 5, 2016|A1274GG1EB2JLJ|0486427706|{NULL}|   barbara ann|The pictures are ...|    1459814400|\n",
      "|    5.0|    true|02 13, 2016|A30X5EGBYAZQQK|0486427706|{NULL}|      Samantha|I absolutely love...|    1455321600|\n",
      "|    5.0|    true|12 10, 2015|A3U6UNXLAUY6ZV|0486427706|{NULL}|   CP in Texas|          I love it!|    1449705600|\n",
      "|    5.0|    true|10 26, 2015|A1SAJF5SNM6WJS|0486427706|{NULL}|   LOIS LABIER|MY HUSBAND LOVED ...|    1445817600|\n",
      "|    4.0|    true|09 15, 2015| AHJWO3SI0S0OR|0486427706|{NULL}|Saundra Hatley|                cool|    1442275200|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin| style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "|    5.0|    true| 10 6, 2013|A2LSCFZM2FBZK7|0486427706|{NULL}|              Ginger|The stained glass...|    1381017600|\n",
      "|    5.0|    true| 08 9, 2013|A3IXP5VS847GE5|0486427706|{NULL}|Dragonflies &amp;...|My 11 y.o. loved ...|    1376006400|\n",
      "|    5.0|    true|10 12, 2015|A2HK5AVQW6AUQ5|0486427706|{NULL}|     scootergirl1952|             love it|    1444608000|\n",
      "|    5.0|    true|04 20, 2011|A18MVTKTTE8OS8|0486448789|{NULL}|              JudiAU|Sometimes you nee...|    1303257600|\n",
      "|    5.0|   false| 04 2, 2011|A2C2TLRMMMLAJV|0486448789|{NULL}|          D. Workman|These little book...|    1301702400|\n",
      "+-------+--------+-----------+--------------+----------+------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "json_df = spark.read.schema(schema).json(\"combined_train_data_chunked_10mb_latest.json\")\n",
    "json_test_df = spark.read.schema(schema).json(\"combined_test_data_chunked_10mb_latest.json\")\n",
    "json_df.show(5)\n",
    "json_test_df.show(5)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  11321389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==========================>                               (6 + 7) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe before pre processing:  2832499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of entries in the dataframe\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_df.count())\n",
    "print(\"Number of entries in the dataframe before pre processing: \", json_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = json_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "reduced_test_df = json_test_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A1274GG1EB2JLJ|0486427706|The pictures are ...|\n",
      "|    5.0|A30X5EGBYAZQQK|0486427706|I absolutely love...|\n",
      "|    5.0|A3U6UNXLAUY6ZV|0486427706|          I love it!|\n",
      "|    5.0|A1SAJF5SNM6WJS|0486427706|MY HUSBAND LOVED ...|\n",
      "|    4.0| AHJWO3SI0S0OR|0486427706|                cool|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------+----------+--------------------+\n",
      "|overall|    reviewerID|      asin|          reviewText|\n",
      "+-------+--------------+----------+--------------------+\n",
      "|    5.0|A2LSCFZM2FBZK7|0486427706|The stained glass...|\n",
      "|    5.0|A3IXP5VS847GE5|0486427706|My 11 y.o. loved ...|\n",
      "|    5.0|A2HK5AVQW6AUQ5|0486427706|             love it|\n",
      "|    5.0|A18MVTKTTE8OS8|0486448789|Sometimes you nee...|\n",
      "|    5.0|A2C2TLRMMMLAJV|0486448789|These little book...|\n",
      "+-------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(5)\n",
    "reduced_test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries in the dataframe after removing duplicates:  10936206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing entries in the dataframe after removing duplicates:  2806599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_df = reduced_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of training entries in the dataframe after removing duplicates: \", unique_df.count())\n",
    "\n",
    "unique_test_df = reduced_test_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "print(\"Number of testing entries in the dataframe after removing duplicates: \", unique_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = unique_df.withColumn(\"sentiment\", targetUDF(unique_df[\"overall\"]))\n",
    "df_test_sentiment = unique_test_df.withColumn(\"sentiment\", targetUDF(unique_test_df[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|\n",
      "+-------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|\n",
      "+-------+-------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sentiment.show(5)\n",
    "df_test_sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "# Remove the rows with missing values and tokenize\n",
    "df_train_tokenized = tokenizer.transform(df_sentiment.filter(unique_df.reviewText.isNotNull()))\n",
    "df_test_tokenized = tokenizer.transform(df_test_sentiment.filter(unique_test_df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|          reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "|    5.0|A0015332H21AK8WZ0ZCS|B005G030TC|These collars are...|        1|[these, collars, ...|\n",
      "|    4.0| A0020356UF96ZV361ST|B00ZI5OVFM|It is a love stor...|        1|[it, is, a, love,...|\n",
      "|    2.0| A0020356UF96ZV361ST|B015X7KEDM|This book is not ...|        0|[this, book, is, ...|\n",
      "|    5.0|A0024936S1WI02OHH9DP|B016AG5DR2|Looks great fits ...|        1|[looks, great, fi...|\n",
      "|    5.0|A0034986DWR7WEDQN0GV|B001VJZO2S|           excellent|        1|         [excellent]|\n",
      "+-------+--------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|overall|         reviewerID|      asin|          reviewText|sentiment|               token|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "|    3.0|A0020356UF96ZV361ST|B00FDXFFW2|I guess you can s...|        0|[i, guess, you, c...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00H6VZ0SS|This girl has bee...|        1|[this, girl, has,...|\n",
      "|    4.0|A0020356UF96ZV361ST|B00XQOGWV8|Mario is a sorry ...|        1|[mario, is, a, so...|\n",
      "|    4.0|A0020356UF96ZV361ST|B014HD23EQ|This father had h...|        1|[this, father, ha...|\n",
      "|    4.0|A0020356UF96ZV361ST|B018RSH2FW|This guy is and h...|        1|[this, guy, is, a...|\n",
      "+-------+-------------------+----------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_tokenized.show(5)\n",
    "df_test_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    # Use a raw string for regular expressions to avoid escape sequence warnings\n",
    "    expr = r'(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "           r'(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "    regex = re.compile(expr)\n",
    "    cleaned = [t for t in tokens if not regex.search(t) and len(t) > 0]\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4_train = df_train_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized[\"token\"]))\n",
    "df4_test = df_test_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_test_tokenized[\"token\"]))\n",
    "# remove non english characters\n",
    "df4_train = df4_train.withColumn(\"tokens_clean\", normalizeUDF(df4_train[\"tokens_re\"]))\n",
    "df4_test = df4_test.withColumn(\"tokens_clean\", normalizeUDF(df4_test[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5_train = df4_train.drop(\"token\",\"tokens_re\")\n",
    "df5_test = df4_test.drop(\"token\",\"tokens_re\")\n",
    "df5_train = df5_train.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "df5_test = df5_test.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6_train = df5_train.where(F.size(F.col(\"tokens\")) > 0)\n",
    "df6_test = df5_test.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_for_model = df6_train.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")\n",
    "df_test_for_model = df6_test.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                       (0 + 12) / 50]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "shuffled_train_df = df_train_for_model.orderBy(rand())\n",
    "shuffled_test_df = df_test_for_model.orderBy(rand())\n",
    "\n",
    "# Show the shuffled DataFrame\n",
    "shuffled_train_df.show(10)\n",
    "shuffled_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing label distribution for training and testing data\n",
    "# print(\"Training data label distribution\")\n",
    "# shuffled_train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# print(\"Testing data label distribution\")\n",
    "# shuffled_test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"model1.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_train_df_partition = shuffled_train_df.repartition(5)\n",
    "# shuffled_test_df_partition = shuffled_test_df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shuffled_train_df_partition.rdd.getNumPartitions())\n",
    "# print(shuffled_test_df_partition.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_shuffle_small = spark.sparkContext.parallelize(shuffled_train_df.take(50000)).toDF()\n",
    "# test_shuffle_small = spark.sparkContext.parallelize(shuffled_test_df.take(50000)).toDF()\n",
    "\n",
    "#print(type(test_shuffle_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# # w = Window().orderBy(F.lit('A'))\n",
    "# # train_shuffle_small = train_shuffle_small.withColumn('row_num', F.floor(F.row_number().over(w) / 10) )\n",
    "# # test_shuffle_small = test_shuffle_small.withColumn('row_num', F.floor(F.row_number().over(w) / 10) )\n",
    "\n",
    "# # train_shuffle_small.show(2)\n",
    "# train_shuffle_small = train_shuffle_small.withColumn('original_order', monotonically_increasing_id())\n",
    "# train_shuffle_small = train_shuffle_small.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "# train_shuffle_small = train_shuffle_small.drop('original_order')\n",
    "\n",
    "# train_shuffle_small.show(2)\n",
    "# test_shuffle_small = test_shuffle_small.withColumn('original_order', monotonically_increasing_id())\n",
    "# test_shuffle_small = test_shuffle_small.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "# test_shuffle_small = test_shuffle_small.drop('original_order')\n",
    "# test_shuffle_small.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 1) & (col('row_num') <=10000))\n",
    "# #test_shuffle_small = test_shuffle_small.filter(test_shuffle_small.row_num(1, 10000))\n",
    "# train_shuffle_small.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "# # test_stem  = stem_pipeline.transform(shuffled_test_df_partition)\\\n",
    "# #                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# test_stem  = stem_pipeline.transform(test_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# # cache them to avoid running stemming \n",
    "# # each iteration in the grid search\n",
    "# train_stem.cache()\n",
    "# test_stem.cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_small = train_stem.take(10)\n",
    "# rdd = spark.sparkContext.parallelize(train_small)\n",
    "# train_small_df = rdd.toDF().show(2)\n",
    "# train_small_df.typedf()\n",
    "# print(type(train_small_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_small = test_stem.take(10)\n",
    "# rdd2 = spark.sparkContext.parallelize(test_small)\n",
    "# test_small_df = rdd2.toDF().show(2)\n",
    "# print(type(test_small_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import NGram\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "# tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "# idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# lr      = LogisticRegression(maxIter=10)\n",
    "\n",
    "# stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#                         .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "#                         .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "#                         .build()\n",
    "# crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "#                           estimatorParamMaps = paramGrid,\n",
    "#                           evaluator          = BinaryClassificationEvaluator(),\n",
    "#                           numFolds           = 2,\n",
    "#                           parallelism= 2\n",
    "#                           )\n",
    "\n",
    "\n",
    "# model    = crossval.fit(train_stem)\n",
    "# predictions   = model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"AUC SCORE for cross validation: {}\".format(score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"cross_val_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "# model_path = \"cross_val_model_final\"\n",
    "# model = CrossValidatorModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 10001) & (col('row_num') <=20000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.bestModel.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"cross_val_model_final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transform(test_stem).show(5)\n",
    "\n",
    "# predictions   = model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE for cross validation: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# bestModel = model.bestModel\n",
    "# predictedAndLabels = predictions.select([\"prediction\",\"label\"])\\\n",
    "#                                 .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "# metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "# print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"cross_val_model_final2\"\n",
    "# model = CrossValidatorModel.load(model_path)\n",
    "\n",
    "# train_shuffle_small = train_shuffle_small.filter( (col('row_num') >= 20001) & (col('row_num') <=50000))\n",
    "# stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "# #stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "# stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "# #train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "#                           #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "# train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# updated_model = Pipeline(stages = model.bestModel.stages).fit(train_stem)\n",
    "# predictions = updated_model.transform(test_stem)\n",
    "# score         = evaluator.evaluate(predictions)\n",
    "# print(\"AUC SCORE: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = shuffled_train_df.withColumn('original_order', monotonically_increasing_id())\n",
    "shuffled_train_df = shuffled_train_df.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "shuffled_train_df = shuffled_train_df.drop('original_order')\n",
    "\n",
    "shuffled_train_df.show(2)\n",
    "shuffled_test_df = shuffled_test_df.withColumn('original_order', monotonically_increasing_id())\n",
    "shuffled_test_df = shuffled_test_df.withColumn('row_num', row_number().over(Window.orderBy('original_order')))\n",
    "shuffled_test_df = shuffled_test_df.drop('original_order')\n",
    "shuffled_test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n",
    "train_shuffle_small = shuffled_train_df.filter( (col('row_num') >= 1) & (col('row_num') <=50000))\n",
    "#stem_pipeline = Pipeline(stages= [tk, stem2]).fit(shuffled_train_df_partition)\n",
    "stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train_shuffle_small)\n",
    "\n",
    "#train_stem = stem_pipeline.transform(shuffled_train_df_partition)\\\n",
    "                          #.where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "train_stem = stem_pipeline.transform(train_shuffle_small)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "# test_stem  = stem_pipeline.transform(shuffled_test_df_partition)\\\n",
    "#                           .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "test_stem  = stem_pipeline.transform(shuffled_test_df)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# cache them to avoid running stemming \n",
    "# each iteration in the grid search\n",
    "train_stem.cache()\n",
    "test_stem.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr      = LogisticRegression(maxIter=10)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "                        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "                        .build()\n",
    "crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator          = BinaryClassificationEvaluator(),\n",
    "                          numFolds           = 2,\n",
    "                          parallelism= 2\n",
    "                          )\n",
    "\n",
    "\n",
    "model    = crossval.fit(train_stem)\n",
    "predictions   = model.transform(test_stem)\n",
    "score         = evaluator.evaluate(predictions)\n",
    "print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cross_val_model_first50K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
