{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/20 12:24:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bigrams\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\",\n",
    "# \"asin\": \"0209688726\",\n",
    "# \"style\": {\"Color:\": \" AC\"},\n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "\n",
    "])\n",
    "\n",
    "# Load JSON file into DataFrame\n",
    "json_df = spark.read.schema(schema).json(\"../combined_train_data_chunked_10mb_latest.json\")\n",
    "\n",
    "json_test_df = spark.read.schema(schema).json(\"../combined_test_data_chunked_10mb_latest.json\")\n",
    "\n",
    "# Sample 10% of the data\n",
    "train_df, test_df = json_df.randomSplit([0.8, 0.2], seed=42)\n",
    "json_df_first_half = train_df.sample(withReplacement=False, fraction=0.02, seed=42)\n",
    "json_df_second_half = test_df.sample(withReplacement=False, fraction=0.02, seed=42)\n",
    "\n",
    "json_test_df = json_test_df.sample(withReplacement=False, fraction=0.02, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df_1 = json_df_first_half.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "reduced_df_2 = json_df_second_half.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "reduced_test_df = json_test_df.select(\"overall\", \"reviewerID\", \"asin\", \"reviewText\")\n",
    "unique_df_1 = reduced_df_1.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "unique_df_2 = reduced_df_2.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "# print(\"Number of training entries in the dataframe after removing duplicates: \", unique_df.count())\n",
    "\n",
    "unique_test_df = reduced_test_df.dropDuplicates([\"reviewerID\", \"asin\"])\n",
    "# print(\"Number of testing entries in the dataframe after removing duplicates: \", unique_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_1 = unique_df_1.withColumn(\"sentiment\", targetUDF(unique_df_1[\"overall\"]))\n",
    "df_sentiment_2 = unique_df_2.withColumn(\"sentiment\", targetUDF(unique_df_2[\"overall\"]))\n",
    "df_test_sentiment = unique_test_df.withColumn(\"sentiment\", targetUDF(unique_test_df[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "# Remove the rows with missing values and tokenize\n",
    "df_train_tokenized1 = tokenizer.transform(df_sentiment_1.filter(unique_df_1.reviewText.isNotNull()))\n",
    "df_train_tokenized2 = tokenizer.transform(df_sentiment_2.filter(unique_df_2.reviewText.isNotNull()))\n",
    "df_test_tokenized = tokenizer.transform(df_test_sentiment.filter(unique_test_df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    # Use a raw string for regular expressions to avoid escape sequence warnings\n",
    "    expr = r'(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "           r'(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "    regex = re.compile(expr)\n",
    "    cleaned = [t for t in tokens if not regex.search(t) and len(t) > 0]\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "\n",
    "    filtered = filter(None, subbed)\n",
    "\n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4_train1 = df_train_tokenized1.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized1[\"token\"]))\n",
    "df4_train2 = df_train_tokenized2.withColumn(\"tokens_re\", removeWEBUDF(df_train_tokenized2[\"token\"]))\n",
    "df4_test = df_test_tokenized.withColumn(\"tokens_re\", removeWEBUDF(df_test_tokenized[\"token\"]))\n",
    "# remove non english characters\n",
    "df4_train1 = df4_train1.withColumn(\"tokens_clean\", normalizeUDF(df4_train1[\"tokens_re\"]))\n",
    "df4_train2 = df4_train2.withColumn(\"tokens_clean\", normalizeUDF(df4_train2[\"tokens_re\"]))\n",
    "df4_test = df4_test.withColumn(\"tokens_clean\", normalizeUDF(df4_test[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5_train1 = df4_train1.drop(\"token\",\"tokens_re\")\n",
    "df5_train2 = df4_train2.drop(\"token\",\"tokens_re\")\n",
    "df5_test = df4_test.drop(\"token\",\"tokens_re\")\n",
    "df5_train1 = df5_train1.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "df5_train2 = df5_train2.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "df5_test = df5_test.withColumnRenamed(\"tokens_clean\", \"tokens\")\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6_train1 = df5_train1.where(F.size(F.col(\"tokens\")) > 0)\n",
    "df6_train2 = df5_train2.where(F.size(F.col(\"tokens\")) > 0)\n",
    "df6_test = df5_test.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_for_model1 = df6_train1.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")\n",
    "df_train_for_model2 = df6_train2.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")\n",
    "df_test_for_model = df6_test.select(\"reviewText\",\"sentiment\").withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "shuffled_train_df1 = df_train_for_model1.orderBy(rand())\n",
    "shuffled_train_df2 = df_train_for_model2.orderBy(rand())\n",
    "shuffled_test_df = df_test_for_model.orderBy(rand())\n",
    "\n",
    "# Show the shuffled DataFrame\n",
    "# shuffled_train_df.show(10)\n",
    "# shuffled_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import NGram, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "bigram = NGram(inputCol=\"tokens\", outputCol=\"bigrams\", n=2)\n",
    "tf5   = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "\n",
    "bigram_pipeline  = Pipeline(stages= [tk, bigram, tf5, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/20 12:29:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:23 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/20 12:29:23 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/04/20 12:29:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:29:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:55 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:57 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:34:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/20 12:37:14 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.7843378188731388\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def make_pipeline():\n",
    "    return bigram_pipeline\n",
    "# train model with first dataset\n",
    "make_pipeline().fit(shuffled_train_df1)\n",
    "\n",
    "\n",
    "with open('bigram_model.pkl', 'wb') as infile:\n",
    "    pickle.dump(make_pipeline, infile)\n",
    "\n",
    "with open('bigram_model.pkl', 'rb') as outfile:\n",
    "    loaded_pipeline_func = pickle.load(outfile)\n",
    "\n",
    "model1 = loaded_pipeline_func().fit(shuffled_train_df2)\n",
    "\n",
    "# test dataset\n",
    "predictions = model1.transform(shuffled_test_df)\n",
    "score = evaluator.evaluate(predictions)\n",
    "print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
