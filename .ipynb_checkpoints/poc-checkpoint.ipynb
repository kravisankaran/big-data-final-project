{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abc2c2f-4a55-4bfc-b5d8-b9b93740a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").config(\"spark.executor.memory\", \"500mb\").config(\"spark.driver.memory\", \"1g\").getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f154c3-09fd-4be2-a3b8-013790fa7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType\n",
    "#\"reviewerID\": \"A8WEXFRWX1ZHH\", \n",
    "# \"asin\": \"0209688726\", \n",
    "# \"style\": {\"Color:\": \" AC\"}, \n",
    "# \"reviewerName\": \"Goldengate\",\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"style\", StructType([StructField(\"Color:\", StringType(), True)]), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", IntegerType(), True)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bddb45a-c047-4263-b866-670bae0f2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).json(r\"C:\\Users\\Emma\\Downloads\\school\\Big_Data\\project\\amazon_review_data\\AMAZON_FASHION_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bb8558-a926-4b5e-978d-029089c13817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|    4.0|   false| 05 1, 2015| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|\n",
      "|    1.0|    true|04 19, 2018| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|\n",
      "|    1.0|    true|04 16, 2018|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|\n",
      "|    3.0|    true|04 13, 2018|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|\n",
      "|    5.0|    true| 04 8, 2018|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|\n",
      "|    5.0|    true|03 24, 2018| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|\n",
      "|    3.0|    true| 03 4, 2018|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|\n",
      "|    2.0|    true| 03 1, 2018|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|\n",
      "|    4.0|    true|02 22, 2018|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|\n",
      "|    5.0|    true|01 29, 2018|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|\n",
      "|    2.0|    true|01 21, 2018|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|\n",
      "|    3.0|    true|01 14, 2018|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|\n",
      "|    5.0|    true| 01 3, 2018|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|\n",
      "|    1.0|    true|12 16, 2017|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|\n",
      "|    5.0|    true|12 15, 2017|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|\n",
      "|    1.0|    true|12 10, 2017|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|\n",
      "|    5.0|    true| 11 9, 2017|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|\n",
      "|    5.0|    true|10 29, 2017|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|\n",
      "|    3.0|    true|10 20, 2017|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|\n",
      "|    5.0|   false|09 23, 2017|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|\n",
      "+-------+--------+-----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cba66da-f8a4-4850-b5b5-3a4eeba998e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "df_modified = df.withColumn(\"style\", struct(col(\"style.Color:\").alias(\"Color\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a432e56-d412-4a35-b395-13e91a7e4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_with_date = df.withColumn(\"reviewTime\", to_date(df.reviewTime, \"MM d, yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86819dc-aa07-46f2-81cc-fdf5ed72dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "|    4.0|   false|2015-05-01| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|\n",
      "|    1.0|    true|2018-04-19| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|\n",
      "|    1.0|    true|2018-04-16|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|\n",
      "|    3.0|    true|2018-04-13|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|\n",
      "|    5.0|    true|2018-04-08|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|\n",
      "|    5.0|    true|2018-03-24| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|\n",
      "|    3.0|    true|2018-03-04|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|\n",
      "|    2.0|    true|2018-03-01|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|\n",
      "|    4.0|    true|2018-02-22|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|\n",
      "|    5.0|    true|2018-01-29|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|\n",
      "|    2.0|    true|2018-01-21|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|\n",
      "|    3.0|    true|2018-01-14|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|\n",
      "|    5.0|    true|2018-01-03|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|\n",
      "|    1.0|    true|2017-12-16|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|\n",
      "|    5.0|    true|2017-12-15|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|\n",
      "|    1.0|    true|2017-12-10|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|\n",
      "|    5.0|    true|2017-11-09|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|\n",
      "|    5.0|    true|2017-10-29|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|\n",
      "|    3.0|    true|2017-10-20|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|\n",
      "|    5.0|   false|2017-09-23|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8ee360-015e-430b-a593-17843883e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType\n",
    "# targetUDF = F.udf(lambda x: 1 if x >= 4.0 else (0 if x == 3.0 else -1), IntegerType())\n",
    "targetUDF = F.udf(lambda x: 1 if x >= 4.0 else 0, IntegerType())\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99a1669-2aaa-4b08-88c7-9a8885f760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db0b018-f69b-4700-8705-9dad8e559771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = df_with_date.withColumn(\"sentiment\", targetUDF(df_with_date[\"overall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f325a032-9a6f-4379-9ce1-d12d6dac766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|sentiment|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+\n",
      "|    4.0|   false|2015-05-01| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|        1|\n",
      "|    1.0|    true|2018-04-19| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|        0|\n",
      "|    1.0|    true|2018-04-16|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|        0|\n",
      "|    3.0|    true|2018-04-13|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|        0|\n",
      "|    5.0|    true|2018-04-08|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|        1|\n",
      "|    5.0|    true|2018-03-24| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|        1|\n",
      "|    3.0|    true|2018-03-04|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|        0|\n",
      "|    2.0|    true|2018-03-01|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|        0|\n",
      "|    4.0|    true|2018-02-22|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|        1|\n",
      "|    5.0|    true|2018-01-29|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|        1|\n",
      "|    2.0|    true|2018-01-21|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|        0|\n",
      "|    3.0|    true|2018-01-14|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|        0|\n",
      "|    5.0|    true|2018-01-03|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|        1|\n",
      "|    1.0|    true|2017-12-16|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|        0|\n",
      "|    5.0|    true|2017-12-15|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|        1|\n",
      "|    1.0|    true|2017-12-10|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|        0|\n",
      "|    5.0|    true|2017-11-09|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|        1|\n",
      "|    5.0|    true|2017-10-29|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|        1|\n",
      "|    3.0|    true|2017-10-20|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|        0|\n",
      "|    5.0|   false|2017-09-23|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|        1|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2941e4ba-fb43-4ace-b68f-c557b110c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"reviewText\",\n",
    "                      outputCol = \"token\")\n",
    "df4 = tokenizer.transform(df_sentiment.filter(df.reviewText.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ec8635-eb42-40e2-81a0-405024c94bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+--------------------+\n",
      "|overall|verified|reviewTime|    reviewerID|      asin|   style|        reviewerName|          reviewText|unixReviewTime|sentiment|               token|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+--------------------+\n",
      "|    4.0|   false|2015-05-01| A8WEXFRWX1ZHH|0209688726|   { AC}|          Goldengate|After I wrote the...|    1430438400|        1|[after, i, wrote,...|\n",
      "|    1.0|    true|2018-04-19| ABCA1A8E4DGV1|0209688726| { Blue}|                 noe|It sucks barely p...|    1524096000|        0|[it, sucks, barel...|\n",
      "|    1.0|    true|2018-04-16|A1NX8HM89FRQ32|0209688726|{ Black}|              Eduard|Well to write a s...|    1523836800|        0|[well, to, write,...|\n",
      "|    3.0|    true|2018-04-13|A1X77G023NY0KY|0209688726|   { CA}|              Lauren|I have absolutely...|    1523577600|        0|[i, have, absolut...|\n",
      "|    5.0|    true|2018-04-08|A3GK37JO2MGW6Q|0209688726|{ Black}|               danny|it ok it does it job|    1523145600|        1|[it, ok, it, does...|\n",
      "|    5.0|    true|2018-03-24| AIY18YON1TWJJ|0209688726|{ Black}|            Karen H.|Have 3 big dogs. ...|    1521849600|        1|[have, 3, big, do...|\n",
      "|    3.0|    true|2018-03-04|A2MPTQ85HBBNG2|0209688726|{ Black}|                 Giv|Pros: Good attach...|    1520121600|        0|[pros:, good, att...|\n",
      "|    2.0|    true|2018-03-01|A1SPIM9Y6HUUSH|0209688726|{ Black}|     Frank W.Brodeur|I have a 2017 out...|    1519862400|        0|[i, have, a, 2017...|\n",
      "|    4.0|    true|2018-02-22|A1Q6FHU6DA643L|0209688726|{ Black}|             nutter1|very good suction...|    1519257600|        1|[very, good, suct...|\n",
      "|    5.0|    true|2018-01-29|A3MA15RJJ59OKG|0209688726|{ Black}|            Daryl S.|love it,works gre...|    1517184000|        1|[love, it,works, ...|\n",
      "|    2.0|    true|2018-01-21|A3MMUEDWT4ISC6|0209688726|{ Black}|       Michael Jones|Its just not as e...|    1516492800|        0|[its, just, not, ...|\n",
      "|    3.0|    true|2018-01-14|A17NVKCLLV38X8|0209688726|{ Black}|     Amazon Customer|        poor suction|    1515888000|        0|     [poor, suction]|\n",
      "|    5.0|    true|2018-01-03|A2BXIIRN07VR2K|0209688726|{ Black}|          S. Francis|Powerful vacuum. ...|    1514937600|        1|[powerful, vacuum...|\n",
      "|    1.0|    true|2017-12-16|A3GVZ4AEHBLBU4|0209688726|{ Black}|          spirithing|Junk the lighter ...|    1513382400|        0|[junk, the, light...|\n",
      "|    5.0|    true|2017-12-15|A1BCDK0T74640B|0209688726|{ Black}|                R.C.|Great vacuum, it ...|    1513296000|        1|[great, vacuum,, ...|\n",
      "|    1.0|    true|2017-12-10|A285DW9TKBH5OE|0209688726|{ Black}|           AMZ Buyer|Excellent suction...|    1512864000|        0|[excellent, sucti...|\n",
      "|    5.0|    true|2017-11-09|A2BLFCOPSMBOZ9|0209688726|{ Black}|       Dave Edmiston|I have come to di...|    1510185600|        1|[i, have, come, t...|\n",
      "|    5.0|    true|2017-10-29|A2GZ47ZYJB9OUE|0209688726|{ Black}|                 Rob|So handy to have ...|    1509235200|        1|[so, handy, to, h...|\n",
      "|    3.0|    true|2017-10-20|A1CAKIX5MFTMGY|0209688726|{ Black}|Tornike Gigolashvili|Not strong enough...|    1508457600|        0|[not, strong, eno...|\n",
      "|    5.0|   false|2017-09-23|A3FFS05RMJS2X9|0209688726|{ Black}|           Mike5Coat|PROS:\\n- Easy cle...|    1506124800|        1|[pros:, -, easy, ...|\n",
      "+-------+--------+----------+--------------+----------+--------+--------------------+--------------------+--------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d41c690-a5cf-4a3a-adb5-f0978ed1a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeRegex(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes hashtags, call outs and web addresses from tokens.\n",
    "    \"\"\"\n",
    "    expr    = '(@[A-Za-z0-a9_]+)|(#[A-Za-z0-9_]+)|'+\\\n",
    "              '(https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+)'\n",
    "        \n",
    "    regex   = re.compile(expr)\n",
    "\n",
    "    cleaned = [t for t in tokens if not(regex.search(t)) if len(t) > 0]\n",
    "\n",
    "    return list(filter(None, cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08bea8d-230f-4170-8b18-5e248173a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWEBUDF = F.udf(removeRegex, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf340c2-989a-415f-bcd4-2609878074ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens : list) -> list:\n",
    "    \"\"\"\n",
    "    Removes non-english characters and returns lower case versions of words.\n",
    "    \"\"\"\n",
    "    subbed   = [re.sub(\"[^a-zA-Z]+\", \"\", s).lower() for s in tokens]\n",
    "    \n",
    "    filtered = filter(None, subbed)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "normalizeUDF = F.udf(normalize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40d5b51-ca87-4bcd-b531-5af45d5b9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtags, call outs and web addresses\n",
    "df4 = df4.withColumn(\"tokens_re\", removeWEBUDF(df4[\"token\"]))\n",
    "\n",
    "# remove non english characters\n",
    "df4 = df4.withColumn(\"tokens_clean\", normalizeUDF(df4[\"tokens_re\"]))\n",
    "\n",
    "# rename columns\n",
    "df5 = df4.drop(\"token\",\"tokens_re\")\n",
    "df5 = df5.withColumnRenamed(\"tokens_clean\", \"tokens\")\\\n",
    "\n",
    "# remove reviews where the tokens array is empty, i.e. where it was just\n",
    "# a hashtag, callout, numbers, web adress etc.\n",
    "df6 = df5.where(F.size(F.col(\"tokens\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a3d686-fda4-40dc-9007-aa438ff629a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>A8WEXFRWX1ZHH</td>\n",
       "      <td>0209688726</td>\n",
       "      <td>( AC,)</td>\n",
       "      <td>Goldengate</td>\n",
       "      <td>After I wrote the below review, the manufactur...</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>1</td>\n",
       "      <td>[after, i, wrote, the, below, review, the, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>ABCA1A8E4DGV1</td>\n",
       "      <td>0209688726</td>\n",
       "      <td>( Blue,)</td>\n",
       "      <td>noe</td>\n",
       "      <td>It sucks barely picks up anything definitely n...</td>\n",
       "      <td>1524096000</td>\n",
       "      <td>0</td>\n",
       "      <td>[it, sucks, barely, picks, up, anything, defin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified  reviewTime     reviewerID        asin     style  \\\n",
       "0      4.0     False  2015-05-01  A8WEXFRWX1ZHH  0209688726    ( AC,)   \n",
       "1      1.0      True  2018-04-19  ABCA1A8E4DGV1  0209688726  ( Blue,)   \n",
       "\n",
       "  reviewerName                                         reviewText  \\\n",
       "0   Goldengate  After I wrote the below review, the manufactur...   \n",
       "1          noe  It sucks barely picks up anything definitely n...   \n",
       "\n",
       "   unixReviewTime  sentiment  \\\n",
       "0      1430438400          1   \n",
       "1      1524096000          0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [after, i, wrote, the, below, review, the, man...  \n",
       "1  [it, sucks, barely, picks, up, anything, defin...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df6.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63401c31-418d-4517-aa49-733c8e87a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_name = \"poc\"\n",
    "# collection_name = \"reviews\"\n",
    "# df_sentiment.write.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/poc.reviews\").mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pymongo\n",
    "\n",
    "\n",
    "\n",
    "# conn = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "# db = conn.poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dde8f44-377e-44f2-bd39-1958e9e0ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = db.reviews\n",
    "# query = {\"asin\": \"0209688726\"}\n",
    "# # for res in results:\n",
    "# #     print(\"Document = {}\\n\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e51b17ac-8201-4099-b202-02ab27ee11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_sentiment = {\"$group\": \n",
    "#                      {\"_id\" : {\"sentiment\":\"$sentiment\"},  # note use a $ on the field\n",
    "#                       \"ct\"  : {\"$sum\":1}\n",
    "#                      }\n",
    "#                   }\n",
    "# results = reviews.aggregate([count_sentiment], allowDiskUse=True)\n",
    "\n",
    "# for res in results:\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74097e4c-d28a-4812-9f04-c6273da70c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.select(\"reviewText\",\"sentiment\")\\\n",
    "        .withColumnRenamed(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ced9341-57b7-44b6-8643-7cecc6d54c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|label|\n",
      "+--------------------+-----+\n",
      "|After I wrote the...|    1|\n",
      "|It sucks barely p...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a88421-0a3a-4ec4-b0c9-fd25df90a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df7.randomSplit([0.80, 0.20], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdc0451-5bdb-4549-bbd1-2262e80ae654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf0fc799-6d4b-4354-9faf-e1d9792cdab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o144.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 1 times, most recent failure: Lost task 15.0 in stage 0.0 (TID 15) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m      2\u001b[0m      \u001b[38;5;241m.\u001b[39mcount()\\\n\u001b[1;32m----> 3\u001b[0m      \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o144.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 1 times, most recent failure: Lost task 15.0 in stage 0.0 (TID 15) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "train.groupby(\"label\")\\\n",
    "     .count()\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36091cb-32c8-48db-b235-f9a4b9be8358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e1646b-6a70-4c07-a0f1-eedaf3c010e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o162.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 7.0 failed 1 times, most recent failure: Lost task 12.0 in stage 7.0 (TID 34) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mcount()\\\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o162.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 7.0 failed 1 times, most recent failure: Lost task 12.0 in stage 7.0 (TID 34) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "test.groupby(\"label\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7261098-eea5-436d-b37b-f1926d37792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c9c8f41-f513-4f56-8f99-c76f9e102dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# get the name of the metric used\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eac5783-7af1-495e-98ba-b661497226b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens from reviews\n",
    "tk = Tokenizer(inputCol= \"reviewText\", outputCol = \"tokens\")\n",
    "\n",
    "# create term frequencies for each of the tokens\n",
    "tf1 = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=1e5)\n",
    "\n",
    "# create tf-idf for each of the tokens\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "# create basic logistic regression model\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "\n",
    "# create entire pipeline\n",
    "basic_pipeline = Pipeline(stages=[tk, tf1, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b393d84f-b46f-4884-96df-e830d5953558",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o150.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1.0 (TID 21) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model1         \u001b[38;5;241m=\u001b[39m basic_pipeline\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o150.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1.0 (TID 21) (DESKTOP-B43SP6V executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.implWrite(Socket.java:1223)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1210)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:118)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "model1         = basic_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1a34354-1062-4284-875a-eae73d4a0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:14:10 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.8742515660545358\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "predictions1   = model1.transform(test)\n",
    "\n",
    "# get the performance on the test set\n",
    "score1         = evaluator.evaluate(predictions1)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c9b7f03-47f5-44c5-98b7-315010952c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kravisankaran/trial/lib/python3.12/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/04/02 20:14:15 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/04/02 20:14:16 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "[Stage 73:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9003961749034488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictedAndLabels = predictions1.select([\"prediction\",\"label\"])\\\n",
    "                                 .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ef59e70-7988-4ae7-af59-2f1e84e99a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0d4741f-173e-4276-994e-3379b88a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tf2 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "214d357c-838f-46bb-aef1-2821673993c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:14:26 WARN DAGScheduler: Broadcasting large task binary with size 1678.6 KiB\n",
      "24/04/02 20:14:32 WARN DAGScheduler: Broadcasting large task binary with size 1679.8 KiB\n",
      "24/04/02 20:14:32 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1679.2 KiB\n",
      "24/04/02 20:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "24/04/02 20:14:43 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.8537567469653522\n"
     ]
    }
   ],
   "source": [
    "sw_pipleline  = Pipeline(stages=[tk, sw, tf2, idf, lr])\n",
    "\n",
    "model2        = sw_pipleline.fit(train)\n",
    "predictions2  = model2.transform(test)\n",
    "score2        = evaluator.evaluate(predictions2)\n",
    "\n",
    "print(\"AUC SCORE: {}\".format(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb341530-320c-4ba0-9110-4283af14cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f77da5d4-2264-4c48-aaf4-c995281dadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw tokens: ['my', 'feelings', 'having', 'studied', 'all', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens  = \"my feelings having studied all day\".split(\" \")\n",
    "print(\"raw tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123f38e9-ffbb-4428-8290-1e68e81741d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean tokens: ['my', 'feel', 'have', 'studi', 'all', 'day']\n"
     ]
    }
   ],
   "source": [
    "tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(\"clean tokens: {}\".format(tokens_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a0d2e5d-4111-4382-b4e3-7a6141789e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74b453bc-30b9-466f-b338-ea7ec40f929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2 = PorterStemming(inputCol=\"tokens\", outputCol=\"stemmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3300aaf-3fce-4fa1-bd04-ccab1d906dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_pipeline = Pipeline(stages= [tk, stem2]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "526e1aa8-ebd9-4273-9b32-961148c4bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, label: int, tokens: array<string>, stemmed: array<string>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stem = stem_pipeline.transform(train)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "\n",
    "test_stem  = stem_pipeline.transform(test)\\\n",
    "                          .where(F.size(F.col(\"stemmed\")) >= 1)\n",
    "\n",
    "# cache them to avoid running stemming \n",
    "# each iteration in the grid search\n",
    "train_stem.cache()\n",
    "test_stem.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13d1015b-a383-4be1-a2ce-f0286ffe36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|          reviewText|label|              tokens|             stemmed|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| I liked it, good...|    1|[, i, liked, it,,...|[i, like, it,, go...|\n",
      "|!!MESSS!! I hope ...|    0|[!!messs!!, i, ho...|[!!messs!!, i, ho...|\n",
      "|\"Quick and fast p...|    1|[\"quick, and, fas...|[\"quick, and, fas...|\n",
      "|\"The reason I pur...|    0|[\"the, reason, i,...|[\"the, reason, i,...|\n",
      "|\"This is the wax ...|    1|[\"this, is, the, ...|[\"thi, is, the, w...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_stem.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47f5c5d6-c9dd-4880-b61b-4f1bc61f6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(inputCol=\"tokens\", outputCol=\"bigrams\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7b91cc9-2f2e-4426-a864-f131fb903285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:15:21 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:44 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:44 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:44 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:45 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:15:52 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tf5   = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "bigram_pipeline  = Pipeline(stages= [tk, bigram, tf5, idf, lr])\n",
    "\n",
    "model5           = bigram_pipeline.fit(train)\n",
    "predictions5     = model5.transform(test)\n",
    "\n",
    "score5           = evaluator.evaluate(predictions5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "825f2a88-2806-494c-a39d-dd4335a6bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.8547981766629861\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC SCORE: {}\".format(score5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a635b32-bb8e-4ff4-9d11-c36672a12837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "# Create a UDF\n",
    "stem_text_udf = udf(stem_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a9b599c-5447-4159-b071-59bb2f5e8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_0 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_8 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_10 in memory! (computed 131.7 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_5 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_2 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_1 in memory! (computed 131.8 MiB so far)\n",
      "24/04/02 20:17:07 WARN MemoryStore: Not enough space to cache rdd_596_3 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_5 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_3 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_0 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_1 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_10 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_8 to disk instead.\n",
      "24/04/02 20:17:08 WARN BlockManager: Persisting block rdd_596_2 to disk instead.\n",
      "24/04/02 20:17:29 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:43 WARN MemoryStore: Not enough space to cache rdd_629_3 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:17:43 WARN MemoryStore: Not enough space to cache rdd_629_9 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:17:43 WARN MemoryStore: Not enough space to cache rdd_629_10 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:17:43 WARN MemoryStore: Not enough space to cache rdd_629_5 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:17:43 WARN BlockManager: Persisting block rdd_629_5 to disk instead.\n",
      "24/04/02 20:17:43 WARN BlockManager: Persisting block rdd_629_10 to disk instead.\n",
      "24/04/02 20:17:43 WARN BlockManager: Persisting block rdd_629_3 to disk instead.\n",
      "24/04/02 20:17:43 WARN BlockManager: Persisting block rdd_629_9 to disk instead.\n",
      "24/04/02 20:17:45 WARN MemoryStore: Not enough space to cache rdd_629_7 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:17:45 WARN BlockManager: Persisting block rdd_629_7 to disk instead.\n",
      "24/04/02 20:17:46 WARN MemoryStore: Not enough space to cache rdd_629_11 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:17:46 WARN MemoryStore: Not enough space to cache rdd_629_8 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:17:46 WARN BlockManager: Persisting block rdd_629_11 to disk instead.\n",
      "24/04/02 20:17:46 WARN BlockManager: Persisting block rdd_629_8 to disk instead.\n",
      "24/04/02 20:17:46 WARN MemoryStore: Not enough space to cache rdd_629_6 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:17:46 WARN BlockManager: Persisting block rdd_629_6 to disk instead.\n",
      "24/04/02 20:17:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:17:56 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2.0)\n",
    "\n",
    "lr      = LogisticRegression(maxIter=20)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "model6                = stem_bigram_pipeline.fit(train_stem)\n",
    "predictions6          = model6.transform(test_stem)\n",
    "\n",
    "score6                = evaluator.evaluate(predictions6)\n",
    "print(\"AUC SCORE: {}\".format(score6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c792163d-1cd4-4317-91e2-5f3e8c7b2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_5 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_4 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_6 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_2 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_9 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_1 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_3 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_11 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_8 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_10 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN MemoryStore: Not enough space to cache rdd_761_7 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_1 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_2 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_4 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_9 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_10 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_5 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_11 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_7 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_3 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_6 to disk instead.\n",
      "24/04/02 20:18:04 WARN BlockManager: Persisting block rdd_761_8 to disk instead.\n",
      "24/04/02 20:18:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:29 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:29 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:29 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:29 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:36 WARN MemoryStore: Not enough space to cache rdd_596_7 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:18:36 WARN MemoryStore: Not enough space to cache rdd_596_8 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:18:36 WARN MemoryStore: Not enough space to cache rdd_596_10 in memory! (computed 131.7 MiB so far)\n",
      "24/04/02 20:18:36 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:18:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:55 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:18:55 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:06 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:19:16 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:35 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:19:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:19:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:02 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:20:08 WARN MemoryStore: Not enough space to cache rdd_596_7 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_4 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_0 in memory! (computed 68.0 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_5 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_7 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_3 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_10 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_6 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_11 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_8 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN MemoryStore: Not enough space to cache rdd_1409_9 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_3 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_9 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_8 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_6 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_11 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_7 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_4 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_5 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_10 to disk instead.\n",
      "24/04/02 20:20:09 WARN BlockManager: Persisting block rdd_1409_0 to disk instead.\n",
      "24/04/02 20:20:18 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:37 WARN MemoryStore: Not enough space to cache rdd_596_8 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:20:37 WARN MemoryStore: Not enough space to cache rdd_596_4 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:20:37 WARN MemoryStore: Not enough space to cache rdd_596_10 in memory! (computed 131.7 MiB so far)\n",
      "24/04/02 20:20:37 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:20:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:20:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:08 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:21:18 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:25 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:33 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:36 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:21:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:21:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:02 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:06 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:22:13 WARN MemoryStore: Not enough space to cache rdd_596_7 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:22:13 WARN MemoryStore: Not enough space to cache rdd_2057_8 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:13 WARN MemoryStore: Not enough space to cache rdd_2057_4 in memory! (computed 68.0 MiB so far)\n",
      "24/04/02 20:22:13 WARN MemoryStore: Not enough space to cache rdd_2057_0 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:13 WARN MemoryStore: Not enough space to cache rdd_2057_2 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_1 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_8 to disk instead.\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_1 to disk instead.\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_4 to disk instead.\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_0 to disk instead.\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_2 to disk instead.\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_10 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_10 to disk instead.\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_7 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_7 to disk instead.\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_11 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_11 to disk instead.\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_5 in memory! (computed 68.0 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_5 to disk instead.\n",
      "24/04/02 20:22:14 WARN MemoryStore: Not enough space to cache rdd_2057_6 in memory! (computed 67.9 MiB so far)\n",
      "24/04/02 20:22:14 WARN BlockManager: Persisting block rdd_2057_6 to disk instead.\n",
      "24/04/02 20:22:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:22:43 WARN MemoryStore: Not enough space to cache rdd_596_6 in memory! (computed 132.0 MiB so far)\n",
      "24/04/02 20:22:43 WARN MemoryStore: Not enough space to cache rdd_596_10 in memory! (computed 131.7 MiB so far)\n",
      "24/04/02 20:22:43 WARN MemoryStore: Not enough space to cache rdd_596_7 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:22:44 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:22:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:15 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:16 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:23:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:32 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:42 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:23:43 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:23:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:07 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:11 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:11 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:24:18 WARN MemoryStore: Not enough space to cache rdd_596_2 in memory! (computed 131.9 MiB so far)\n",
      "24/04/02 20:24:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:35 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:39 WARN MemoryStore: Not enough space to cache rdd_2732_0 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:24:39 WARN BlockManager: Persisting block rdd_2732_0 to disk instead.\n",
      "24/04/02 20:24:39 WARN MemoryStore: Not enough space to cache rdd_2732_1 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:24:39 WARN BlockManager: Persisting block rdd_2732_1 to disk instead.\n",
      "24/04/02 20:24:39 WARN MemoryStore: Not enough space to cache rdd_2732_10 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:24:39 WARN BlockManager: Persisting block rdd_2732_10 to disk instead.\n",
      "24/04/02 20:24:39 WARN MemoryStore: Not enough space to cache rdd_2732_7 in memory! (computed 17.0 MiB so far)\n",
      "24/04/02 20:24:39 WARN BlockManager: Persisting block rdd_2732_7 to disk instead.\n",
      "24/04/02 20:24:42 WARN MemoryStore: Not enough space to cache rdd_2732_5 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:24:42 WARN BlockManager: Persisting block rdd_2732_5 to disk instead.\n",
      "24/04/02 20:24:42 WARN MemoryStore: Not enough space to cache rdd_2732_8 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:24:42 WARN BlockManager: Persisting block rdd_2732_8 to disk instead.\n",
      "24/04/02 20:24:42 WARN MemoryStore: Not enough space to cache rdd_2732_4 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:24:42 WARN BlockManager: Persisting block rdd_2732_4 to disk instead.\n",
      "24/04/02 20:24:42 WARN MemoryStore: Not enough space to cache rdd_2732_9 in memory! (computed 33.0 MiB so far)\n",
      "24/04/02 20:24:42 WARN BlockManager: Persisting block rdd_2732_9 to disk instead.\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/04/02 20:24:51 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.9151592948776508\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "bigram2 = NGram(inputCol=\"stemmed\", outputCol=\"bigrams\", n=2)\n",
    "\n",
    "tf6     = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=2e5)\n",
    "\n",
    "idf     = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr      = LogisticRegression(maxIter=20)\n",
    "\n",
    "stem_bigram_pipeline  = Pipeline(stages= [bigram2, tf6, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(idf.minDocFreq, [2, 5]) \\\n",
    "                        .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "                        .build()\n",
    "crossval = CrossValidator(estimator          = stem_bigram_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator          = BinaryClassificationEvaluator(),\n",
    "                          numFolds           = 3)\n",
    "\n",
    "model    = crossval.fit(train_stem)\n",
    "predictions   = model.transform(test_stem)\n",
    "score         = evaluator.evaluate(predictions)\n",
    "print(\"AUC SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c85b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC SCORE: 0.856599147244647\n"
     ]
    }
   ],
   "source": [
    "# Printing bigram model without tuning\n",
    "print(\"AUC SCORE: {}\".format(score6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a45e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:30:35 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/04/02 20:30:36 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "[Stage 1031:======================>                                (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9014958461750501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bestModel = model.bestModel\n",
    "predictedAndLabels = predictions.select([\"prediction\",\"label\"])\\\n",
    "                                .rdd.map(lambda r : (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(predictedAndLabels)\n",
    "\n",
    "print(\"Test Set Accuracy: {}\".format(metrics.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ba290cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NGram_527379bedca1,\n",
       " HashingTF_f105387cfc11,\n",
       " IDFModel: uid=IDF_cd5ffa7e826a, numDocs=1368873, numFeatures=200000,\n",
       " LogisticRegressionModel: uid=LogisticRegression_7085760cc18a, numClasses=2, numFeatures=200000]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f54bd828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0, current: 2)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.stages[2].explainParam('minDocFreq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92b92880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regParam: regularization parameter (>= 0). (default: 0.0, current: 0.1)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.stages[-1].explainParam('regParam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7f574a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.50.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in ./trial/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./trial/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./trial/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./trial/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp312-cp312-macosx_11_0_arm64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp312-cp312-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.50.0 kiwisolver-1.4.5 matplotlib-3.8.3 pillow-10.3.0 pyparsing-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 20:32:56 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAIjCAYAAABBOWJ+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABok0lEQVR4nO3deVwU9R8G8Gd3YVluUORSDO8j7zM1JRVFLdMspbQkMs3U9CdZanmkllrelWVlimdeqZmapnhfaSqZF+Z9guLBDQu78/tjYBEBZXF2Z4/n/Xrti51hZvbDluzDd76HQhAEAUREREQSUspdABEREdkeBgwiIiKSHAMGERERSY4Bg4iIiCTHgEFERESSY8AgIiIiyTFgEBERkeQYMIiIiEhyDBhEREQkOQYMIiIikhwDBpEdiI6OhkKhMDwcHBxQvnx5vP3227hx40aR5wiCgCVLlqBNmzbw8vKCi4sL6tati4kTJyItLa3Y11q3bh06d+4MHx8fqNVqBAYGolevXtixY0eJas3MzMSsWbPQvHlzeHp6QqPRoHr16hgyZAjOnTtXqp+fiMxPwbVIiGxfdHQ0IiMjMXHiRFSqVAmZmZk4dOgQoqOjERwcjJMnT0Kj0RiO1+l06N27N1atWoXWrVujR48ecHFxwd69e7F8+XLUrl0b27dvh5+fn+EcQRDwzjvvIDo6Gg0bNsRrr70Gf39/3Lp1C+vWrcPRo0exf/9+tGzZstg6ExMT0alTJxw9ehQvvfQSQkND4ebmhri4OKxYsQLx8fHQarUmfa+ISCICEdm8hQsXCgCEI0eOFNg/cuRIAYCwcuXKAvsnT54sABBGjBhR6FobNmwQlEql0KlTpwL7p02bJgAQ/ve//wl6vb7QeYsXLxb++uuvx9b54osvCkqlUlizZk2h72VmZgoffvjhY88vqezsbCErK0uSaxFR0RgwiOxAcQFj48aNAgBh8uTJhn3p6emCt7e3UL16dSE7O7vI60VGRgoAhIMHDxrOKVOmjFCzZk0hJyenVDUeOnRIACD079+/RMeHhIQIISEhhfZHREQIzzzzjGH70qVLAgBh2rRpwqxZs4TKlSsLSqVSOHTokKBSqYTPPvus0DXOnj0rABC++eYbw7779+8Lw4YNEypUqCCo1WqhSpUqwtSpUwWdTmf0z0pkD9gHg8iOXb58GQDg7e1t2Ldv3z7cv38fvXv3hoODQ5Hn9e3bFwCwceNGwzn37t1D7969oVKpSlXLhg0bAABvvfVWqc5/koULF+Kbb77BgAEDMGPGDAQEBCAkJASrVq0qdOzKlSuhUqnQs2dPAEB6ejpCQkKwdOlS9O3bF19//TVatWqF0aNHIyoqyiT1Elm7on97EJFNSkpKQmJiIjIzM/HXX39hwoQJcHJywksvvWQ45vTp0wCA+vXrF3udvO+dOXOmwNe6deuWujYprvE4169fx/nz51GuXDnDvvDwcLz33ns4efIk6tSpY9i/cuVKhISEGPqYzJw5ExcuXMDx48dRrVo1AMB7772HwMBATJs2DR9++CGCgoJMUjeRtWILBpEdCQ0NRbly5RAUFITXXnsNrq6u2LBhAypUqGA4JiUlBQDg7u5e7HXyvpecnFzg6+POeRIprvE4r776aoFwAQA9evSAg4MDVq5cadh38uRJnD59GuHh4YZ9q1evRuvWreHt7Y3ExETDIzQ0FDqdDnv27DFJzUTWjC0YRHZk7ty5qF69OpKSkrBgwQLs2bMHTk5OBY7J+4DPCxpFeTSEeHh4PPGcJ3n4Gl5eXqW+TnEqVapUaJ+Pjw/at2+PVatWYdKkSQDE1gsHBwf06NHDcNx///2HEydOFAooeW7fvi15vUTWjgGDyI40a9YMTZo0AQB0794dzz//PHr37o24uDi4ubkBAGrVqgUAOHHiBLp3717kdU6cOAEAqF27NgCgZs2aAIB///232HOe5OFrtG7d+onHKxQKCEWMstfpdEUe7+zsXOT+119/HZGRkYiNjUWDBg2watUqtG/fHj4+PoZj9Ho9OnTogI8//rjIa1SvXv2J9RLZG94iIbJTKpUKU6ZMwc2bN/Htt98a9j///PPw8vLC8uXLi/2wXrx4MQAY+m48//zz8Pb2xi+//FLsOU/StWtXAMDSpUtLdLy3tzcePHhQaP+VK1eMet3u3btDrVZj5cqViI2Nxblz5/D6668XOKZKlSpITU1FaGhokY+KFSsa9ZpE9oABg8iOvfDCC2jWrBlmz56NzMxMAICLiwtGjBiBuLg4fPrpp4XO2bRpE6KjoxEWFobnnnvOcM7IkSNx5swZjBw5ssiWhaVLl+Lw4cPF1tKiRQt06tQJ8+fPx/r16wt9X6vVYsSIEYbtKlWq4OzZs7hz545h3z///IP9+/eX+OcHAC8vL4SFhWHVqlVYsWIF1Gp1oVaYXr164eDBg9i6dWuh8x88eICcnByjXpPIHnAmTyI7kDeT55EjRwy3SPKsWbMGPXv2xPfff4+BAwcCEG8zhIeH49dff0WbNm3w6quvwtnZGfv27cPSpUtRq1YtxMTEFJjJU6/X4+2338aSJUvQqFEjw0ye8fHxWL9+PQ4fPowDBw6gRYsWxdZ5584ddOzYEf/88w+6du2K9u3bw9XVFf/99x9WrFiBW7duISsrC4A46qROnTqoX78++vXrh9u3b2PevHnw8/NDcnKyYQju5cuXUalSJUybNq1AQHnYsmXL8Oabb8Ld3R0vvPCCYchsnvT0dLRu3RonTpzA22+/jcaNGyMtLQ3//vsv1qxZg8uXLxe4pUJE4EyeRPaguIm2BEEQdDqdUKVKFaFKlSoFJsnS6XTCwoULhVatWgkeHh6CRqMRnn32WWHChAlCampqsa+1Zs0aoWPHjkKZMmUEBwcHISAgQAgPDxd27dpVolrT09OF6dOnC02bNhXc3NwEtVotVKtWTfjggw+E8+fPFzh26dKlQuXKlQW1Wi00aNBA2Lp162Mn2ipOcnKy4OzsLAAQli5dWuQxKSkpwujRo4WqVasKarVa8PHxEVq2bClMnz5d0Gq1JfrZiOwJWzCIiIhIcuyDQURERJJjwCAiIiLJMWAQERGR5BgwiIiISHIMGERERCQ5BgwiIiKSnN2tRaLX63Hz5k24u7tDoVDIXQ4REZHVEAQBKSkpCAwMhFL5+DYKuwsYN2/eRFBQkNxlEBERWa1r166hQoUKjz3G7gJG3vLS165dMywPTURERE+WnJyMoKAgw2fp49hdwMi7LeLh4cGAQUREVAol6WLATp5EREQkOQYMIiIikhwDBhEREUmOAYOIiIgkx4BBREREkmPAICIiIskxYBAREZHkGDCIiIhIcgwYREREJDkGDCIiIpIcAwYRERFJjgGDiIiIJMeAQURERJJjwCAiIiLJyRow9uzZg65duyIwMBAKhQLr169/4jm7du1Co0aN4OTkhKpVqyI6OtrkdRIREZFxZA0YaWlpqF+/PubOnVui4y9duoQXX3wRbdu2RWxsLP73v//h3XffxdatW01cKRERERnDQc4X79y5Mzp37lzi4+fNm4dKlSphxowZAIBatWph3759mDVrFsLCwkxVJhERFUEQBOiF/K96QYCQ+1UvCBAACPr8bb0ACHj4GECvF7cFPHwN8bn4GuL38p4Xt6/A93NfQ3yeX6vwyHF45Lj86wuF9yH/wEev86TXQ7E/Q/Gvl//dp9eiig88nR0lu15JyRowjHXw4EGEhoYW2BcWFob//e9/xZ6TlZWFrKwsw3ZycrKpyiMiG6PXC9Dq9MjW6aHN0SNbJ4jPDdt65OgF6PQCcnTih6i4rYdOD+j0+d/X6cXv6fVCgX2G/YJ4DZ1eD13edXQCdMKTzi34WvnXeejcArUVfOR9sD8cEvR6wfBh93A4wKPbZBU2D23NgPEk8fHx8PPzK7DPz88PycnJyMjIgLOzc6FzpkyZggkTJpirRCKSgCAIyNYJyMzRITNbh6xsPTKzdcjM1hv2ZRr26ZCZo0dW9iP7c3TQ5uQHg4dDgRgShALb2TkPBwfBEB5IWgoFoFQooFQAiryvEL8qFQrx+0oFFBC/DyD3ueEKhueKh66pyN0Sn+ftN5yUf84jx+ZfP/+1UOj6imJfCw8d82it+dd/pOaHX6uImh+6rCRc1CqJr1gyVhUwSmP06NGIiooybCcnJyMoKEjGiohshyAISNPqkJKZjZTMHKRrdUjX5iBDq0OaVocMbd4+HTJyv6Y/vC87x/C9tNzzMrQ6ZGTrLPIvZAelAmoHJRxV4kOtUsBBpYRKqYBKqYCDUgGlQgEHlbitUuTuV+XuVyqgUiqhUgIOyvzzDMeqHrqGUtxWPXKeSqkUj1Hm7VcUeP0C1yvw2o+8Xu4xhg98Zf4HPx76wM/70Fc8tF0gHDzyVfnQNYs6h0zs6FHg22+BH38EHM3favEwqwoY/v7+SEhIKLAvISEBHh4eRbZeAICTkxOcnJzMUR6R1dHm6A3hIDnva0b+dvIj2ymZ2UjOyEFKVu7XzGyzBAGNoxIaRxU0DirDcydHFTQOufuL+b6TgxJOuYEgPxgooC6wLT5Xq5RwdFDkBodHjndQwlGphFLJD0iyYEeOAB07Ag8eABUrAjK33ltVwGjRogU2b95cYN+2bdvQokULmSoisgyZ2TokpmbhXpoWd9O0uJeqxd20LNxLy0ZSRm4wyMzJDQj5gSEzWy/J6zsoFXDTOMBV7QBntQouhkfutmPutpMDXBxVucc4wEUtPn/0vLyw4OQoBgT+5Uv0BIcPi+EiKQlo1QoYMULuiuQNGKmpqTh//rxh+9KlS4iNjUWZMmVQsWJFjB49Gjdu3MDixYsBAAMHDsS3336Ljz/+GO+88w527NiBVatWYdOmTXL9CEQmodMLuJuWhdvJWbiTmoW7qVrcSxO/3k3TGoLE3dxQka7VPdXruapV8HB2hLvGAR6a3K8Fth3h4ewgftWIXz0N247QODIEEMnmr7/EcJGcDDz/PLB5M+DuLndV8gaMv//+G23btjVs5/WViIiIQHR0NG7duoWrV68avl+pUiVs2rQJw4cPx5w5c1ChQgXMnz+fQ1TJqqRkZuPmg0zcfJCB+ORMJCRn4nZKFm7nfk1IzkRiqhY6I+89qFVKlHFVo4yrGmXd1Cjrqoa3qxpezuoiAoMDPHO33Zwc4KDipL5EVunQISAsTAwXrVuL4cLNTe6qAAAKQRAssCuV6SQnJ8PT0xNJSUnw8PCQuxyyMTq9gDspWbjxIAM3HmTgZu7jxv387eTMnBJdS6EAfNycUM7NyRAYyro5iQGi0HM13Jwc2IpAZE8yMoAqVYBbt4CQEGDjRpOHC2M+Q62qDwaR3PR6AbeSM3ElMQ1X7qUXDA9JGYhPykS27smZ3cvFEYGezvD31MDPwwm+7hr4ejjBL++rhwZlXdVsWSCi4jk7A8uXA9OnAytXAq6ucldUAAMGURHupGTh/O1UXL6bhsuJabiUmIbLd9Nw5W46snIe3zFSpVTA30OD8l7OKO/tjEAvDQK9nMVtL2cEeDnDzYn/9IiolLKz84egvvCC+LBA/C1Hdi0rR4fzt1Nx9lYKzsYn40zu18RUbbHnOCgVqFjGBRXLuqCCt3OB8BDo5Qw/Dw1UHM5IRKawbx8QEQH89htQp47c1TwWAwbZBUEQcDslC6dvJRvCxNlbKbhwJ7XI2RoVCuCZMi4I9nFFcFlXVPJxzX3ugvJezrx1QUTmt3cv0LkzkJYGfPEF8Msvclf0WAwYZHP0egGX7qbh5I2k3EcyzsYn4356dpHHezo7oqa/O2oFeKBWgDtq+nugup87nGWaXpeIqJA9e4AuXcRwERoKLFggd0VPxIBBVu92SiZirz7A8WsPcPzqfZy8kYzUrMIjNVRKBSr7uKJmgAdq+rujdoAHaga4w99Dw9EXRGS5du8Ww0V6OtChg3h7pJjZqy0JAwZZFUEQcOVuOvZfSMShi/dw/Op9XL+fUeg4Jwclagd6oG55T9QJ9ETtQA9U9XWDxpGtEkRkRXbuBF56SQwXYWHAunVWES4ABgyyAreTM7H/QiL2n7+Lgxfu4saDgoFCoQCq+7qjYUUvNKzohfpBXqhazo39JIjIugkCMGWKGC46dRLDhUYjd1UlxoBBFicpIxuHLt7FgfOJ2H/hLs7fTi3wfUeVAg0reqNllbJoGlwG9Sp4wl0j76qBRESSUyiANWuAyZOBzz6zqnABMGCQBRAEARfupGLrqQRsO52AE9cfFFihU6EAng30QKsqPmhZ1QdNg73houb/ukRko65cAZ55Rnzu4QFMnSpvPaXE39Ikm9M3k7Hhn5v481Q8LiamFfhe5XKuaFmlLFpV8cFzlcvC21UtU5VERGa0bRvw8svA+PHAqFFyV/NUGDDIrO6mZuG32JtYc/Q6Tt9KNuxXq5RoWbUsOtb2R9ua5RDgaR2dmIiIJPPnn2K4yMoCDhwAdDpAZb0d0xkwyOSydXrsiruDNUevYcfZ24a1OtQqJUJr+6JL3QCEVC/HfhREZL+2bgW6dRPDRbduwKpVVh0uAAYMMqGz8clY/fd1rD9+A3fT8qferlfBEz0bV0DX+oHwcuGtDyKyc3/8AbzyihguuncXFy5TW//vRgYMklRmtg6bTtzCkkNXEHvtgWG/j5sTejQqj1cbVUANf3f5CiQisiSbN4vhQqsVv65YYRPhAmDAIIncTsnEgn2XsfLIVcOU3I4qBdrX9EPPJhXQpno5OHJeCiKigs6fF8PFq6+Ka4s42s6tYgYMeirX76fjxz0XseLINWhzlzEv7+WM3s0roleTIJRzd5K5QiIiCzZ0KFCpkjiRlg2FC4ABg0rp4p1UfLPjPDb8cxO63EkrGlX0wsCQKmhfy4/LlRMRFWfHDqBhQ8DbW9zu2lXeekyEAYOMcispAzP/PIdfj103TIbVqmpZDG5bFS0ql+WiYUREj/Pbb0DPnkD9+kBMjDiRlo1iwKASydDq8P2u8/hhz0Vk5d4KCa3li6Htq6FeBS95iyMisgbr1gG9egE5OUC1aoCLi9wVmRQDBj2WIAjYdjoBE34/bVhkrFlwGYzqUhONKnrLXB0RkZVYuxYIDxfDRe/ewKJFgINtfwTb9k9HT+VyYhom/H4KO+PuAAACPTUY81JtdK7jz1shREQl9euvwOuvi+GiTx8xXFj5JFolwYBBheTo9Phhz0XM2f4ftDo9HFUK9G9dGUPaVeUiY0RExli/Xmy50OmAt94CFi60i3ABMGDQIy4lpiFqVSyOX30AAGhdzQefvfwsqpRzk7cwIiJrVKsW4OsLdOgALFhgN+ECYMCgXIIgYNXf1zB+wylkZuvh7uSAz15+Fj0aleftECKi0qpRAzh8GAgIsKtwATBgEMTpvcesP4k1R68DEIedfvVafZT34oqmRERG++UXoGxZoGNHcbtCBXnrkQkDhp27nZKJ95YcxfGrD6BUACPCamBgmypQcqIsIiLjLVsG9O0rridy+DBQt67cFcmGAcOOXbmbhrd+Poyr99LhoXHAd30a4/lqPnKXRURknZYuBSIiAL0eePNN4Nln5a5IVgwYdur0zWT0XXAYialZqFjGBdGRTVGZHTmJiEpnyRIxXAgC0L8/MG8eoLTvBR4ZMOzQ4Uv30G/REaRk5qCmvzsWv9MMvh4aucsiIrJOixYBkZFiuBgwAPj+e7sPFwADht2JOZOAQcuOIStHj6bB3pgf0RSezra1gh8Rkdns2pUfLgYOBObOZbjIxYBhR/b+dwcDlx5Ftk5A+5q++LZ3Izir7WvYFBGRpJ5/XpxIy9tbDBcc1m/AgGEnYq89wHtLxHDRpa4/5rzeEI4qpmwioqfi4CD2v1CpGC4ewU8YO3D+dgreXngY6VodWlfzwazwBgwXRESlNX8+8O674mgRQAwZDBeFsAXDxiUkZ+Ktnw/jQXo26gd5Yd6bjeHkwNsiRESl8uOPwHvvic9DQ8VFzKhI/DPWhqVrc/Duor9xKykTlcu5IvrtpnB1YqYkIiqVH37IDxfDhol9L6hYDBg2bMz6k/j3RhLKuKqx8O2m8HZVy10SEZF1+v57cZQIAAwfDsyaxdsiT8CAYaNW/X0Na4/dgFIBfNenEZ4p6yp3SURE1mnuXGDQIPH5hx8CM2YwXJQAA4YNOn87BWPWnwQADG1fDc9VLitzRUREVurKFSAqSnw+YgQwbRrDRQnxhryN0ekFjFh9AtocPV6oUQ5D21WTuyQiIuv1zDPAqlXiwmWff85wYQQGDBvz876LiL32AO4aB0ztUY+rohIRlUZKCuDuLj7v1k18kFF4i8SGXL2bjhl/ngMAjH2pNvw9ub4IEZHRZs0C6tQBLl2SuxKrxoBhQyZuPIWsHD1aVimLno0ryF0OEZH1mTlT7HNx9Srw669yV2PVGDBsRMyZBGw/cxsOSgUmdnsWCt4nJCIyzowZ4igRABg7Nv85lQoDhg3IzNZhwu+nAQD9WldCVV93mSsiIrIy06aJo0QAYNw4YMIEduh8SgwYNmDRgcu4ei8d/h4ajhohIjLWV18BH38sPv/sM4YLiXAUiZVLSs/G3J3nAQAfdqzOqcCJiIyRmQksWyY+nzBBbL0gSfDTyMr9tPcikjNzUMPPHT0asWMnEZFRNBpg+3Zg7dr8dUZIErxFYsXup2mxcL84jGp4h+pQcc4LIqKSOX48/3m5cgwXJsCAYcXm7bmANK0OzwZ6IOxZP7nLISKyDpMmAY0aiUuvk8kwYFip1KwcLDt0FQAwPLQ6h6USEZXEw/0s7t2TtxYbxz4YVurXo9eRmpWDKuVc0b6Wr9zlEBFZvrwRIgAwdSowcqSs5dg6BgwrpNcLWHTwMgAgomUwWy+IiB5HEMRwMXGiuP3VV8BHH8lakj1gwLBCe88n4uKdNLg5OXDkCBHR4wiCeEvk88/F7enTOUOnmTBgWKFFBy4DAF5rXAFunPeCiKhkZs4Ehg+Xuwq7wU8nK3Plbhp2xt0GAPRt8YzM1RARWTiFQrw10rkz0LKl3NXYFY4isTKLD16BIAAh1cuhcjk3ucshIrI8ggD89BOQni5uKxQMFzJgwLAiaVk5WPX3NQDA2y2D5S2GiMgSCYI4OmTAAODllwGdTu6K7BZvkViRdcdvICUzB8FlXRBSvZzc5RARWRZBEBctmz5d3H7lFUClkrcmO8aAYSUEQcDi3KGpb7UIhpLTghMR5RMEcbn1mTPF7blzgUGD5K3JzjFgWImDF+7iXEIqXNQq9GzCoalERAaCAERFAbNni9vffw8MHChrScSAYTUW5g5NfbVRBXhoHOUthojIkowdmx8u5s3jwmUWgp08rcC1e+mIOZMAAIhoyaGpREQF9OgBlCkD/PADw4UFYQuGFVh66Ar0AvB8VR9U9XWXuxwiIsvSqBHw339iyCCLwRYMC5eh1WHFEXFoagSHphIR5Y8WOXQofx/DhcVhwLBwv8XeQFJGNip4O6NdTa6aSkR2Tq8HBg8Gpk0TZ+fkkusWi7dILJggCIjO7dzZt8UzUHFoKhHZM71eHHr6ww/i7JyzZ7PlwoIxYFiww5fu4Wx8CjSOSvRqEiR3OURE8tHrgfffB378UQwX0dFA375yV0WPwYBhwRblTqz1SsPy8HJRy1sMEZFc9HpxdMj8+YBSCSxaBLz5ptxV0RMwYFiomw8ysPVU3tDUYHmLISKS09y5+eFi8WKgTx+5K6ISYMCwUEsPXYFOL+C5ymVQ099D7nKIiOTTvz+wdSvQu7f4IKvAgGGBMrPzh6Zy1VQiskt6vdjXQqEANBrg99/F52Q1OEzVAv3+z03cS9Mi0FOD0Fp+cpdDRGReOh0QGQl89JE45wXAcGGFZA8Yc+fORXBwMDQaDZo3b47Dhw8/9vjZs2ejRo0acHZ2RlBQEIYPH47MzEwzVWseK3NbL/o89wwcVLL/JyIiMp+8cLF4sTgM9cQJuSuiUpL102vlypWIiorC+PHjcezYMdSvXx9hYWG4fft2kccvX74co0aNwvjx43HmzBn8/PPPWLlyJT755BMzV24652+n4O8r96FSKvBaY66aSkR2RKcDIiKAJUsAlQpYsQKoX1/uqqiUZA0YM2fORP/+/REZGYnatWtj3rx5cHFxwYIFC4o8/sCBA2jVqhV69+6N4OBgdOzYEW+88cYTWz2sya/HbgAA2tbwhZ+HRuZqiIjMJCdHnNdi2TLAwQFYuRJ47TW5q6KnIFvA0Gq1OHr0KEJDQ/OLUSoRGhqKgwcPFnlOy5YtcfToUUOguHjxIjZv3owuXboU+zpZWVlITk4u8LBUer2A346LAaNHo/IyV0NEZCZ54WL5cjFcrFoFvPqq3FXRU5JtFEliYiJ0Oh38/Ap2YvTz88PZs2eLPKd3795ITEzE888/D0EQkJOTg4EDBz72FsmUKVMwYcIESWs3lUOX7uJmUibcNQ5cd4SI7Mf+/eLtEAcHYPVqoHt3uSsiCVhVD8Jdu3Zh8uTJ+O6773Ds2DGsXbsWmzZtwqRJk4o9Z/To0UhKSjI8rl27ZsaKjbPm6HUAwEv1AqBxVMlcDRGRmYSEiFN/r1nDcGFDZGvB8PHxgUqlQkJCQoH9CQkJ8Pf3L/KcsWPH4q233sK7774LAKhbty7S0tIwYMAAfPrpp1AqC+clJycnODk5Sf8DSCxbp8e20+J78UpDdu4kIhuXnQ08eACUKyduc10RmyNbC4ZarUbjxo0RExNj2KfX6xETE4MWLVoUeU56enqhEKFSiX/pC3ljpa3UXxfvISUzBz5uajR+xlvucoiITCc7G3jjDaBNGyA+Xu5qyERknckzKioKERERaNKkCZo1a4bZs2cjLS0NkZGRAIC+ffuifPnymDJlCgCga9eumDlzJho2bIjmzZvj/PnzGDt2LLp27WoIGtZq22nxH1n7mn5clp2IbFd2NvD668DatYBaDZw8CRTTak3WTdaAER4ejjt37mDcuHGIj49HgwYNsGXLFkPHz6tXrxZosRgzZgwUCgXGjBmDGzduoFy5cujatSu++OILuX4ESQiCYLg90qE2Z+4kIhul1YrhYt06MVysWwc8NJKQbItCsPZ7C0ZKTk6Gp6cnkpKS4OFhGYuInbyRhJe+2QdnRxWOj+vADp5EZHu0WqBXL+C33wAnJ2D9eqBTJ7mrIiMZ8xnKxc4swJ+nxNsjbar7MFwQke3RaoGePYENG8Rw8dtvQFiY3FWRiVnVMFVbtfWUeHsk7FnehyQiG3TvHnDqlLgq6oYNDBd2gi0YMrtyNw1xCSlwUCrQvib7XxCRDfL3B3buBM6fB9q2lbsaMhO2YMhsx1lxYbemwWXg6eIoczVERBLJygJ27crfDgpiuLAzDBgy2xl3BwA4NTgR2Y7MTKBHD3GEyOrVcldDMmHAkFGGVodDF+8CANrWLCdzNUREEsjMBF55Bdi8WRyKWras3BWRTNgHQ0aHLt6FNkePCt7OqFLOTe5yiIieTkaGuJbIn38CLi7Apk3ACy/IXRXJhAFDRseu3gcAtKxSFgoFZ+8kIiuWkQF06wZs2yaGi82bxUXMyG4xYMjo1M1kAECd8p4yV0JE9BSysoCXXwa2bwdcXcVw0aaN3FWRzNgHQ0anbiYBAGoHWMaMokREpaJWA9WqieHijz8YLggAA4ZsElOzkJCcBYUCqMWAQUTWTKEAvv0WOHYMaN1a7mrIQjBgyCTv9kilsq5wdeKdKiKyMmlpwMSJ4uqoAKBUAtWry1sTWRR+ssnEcHskkK0XRGRl0tKAF18Edu8GLl4EoqPlrogsEFswZJLXgvFsIDt4EpEVSU0FunQRw4WHBzBwoNwVkYViC4ZMThsCBlswiMhK5IWLvXvFcPHnn0Dz5nJXRRaKLRgySM3KweW7aQAYMIjISqSkAJ07i+HC01Oc74Lhgh6DLRgyOHsrGYIA+HtoUNbNSe5yiIgeTxCAnj2BffsALy8xXDRpIndVZOHYgiGDvP4X7OBJRFZBoQBGjwbKlxcn02K4oBJgC4YM8kaQ8PYIEVmNkBDg/HlAo5G7ErISbMGQwSl28CQiS5eUBHTtCpw8mb+P4YKMwBYMM9Pm6HEuIQUAh6gSkYV68AAICwMOHxZbLU6eBFQquasiK8OAYWb/3U5Btk6Ah8YBFbyd5S6HiKigBw+Ajh2BI0eAsmWBFSsYLqhUeIvEzB7u4Mkl2onIoty/D3ToIIYLHx9gxw6gfn25qyIrxRYMMztzKzdgBPD2CBFZkHv3xHBx7Fh+uKhbV+6qyIqxBcPMLiWKE2xV9XWTuRIiood88okYLsqVA3buZLigp8aAYWaXcwNGsI+LzJUQET1k2jSge3ex5aJOHbmrIRvAWyRmlK3T4/r9DABAJR9XmashIruXkQE453Y2d3cH1q2Ttx6yKWzBMKMb9zOQoxegcVTCz53jyYlIRomJwHPPAVOmyF0J2SgGDDO6lLvAWXBZVyiVHEFCRDK5cwdo1w44cQKYM0fs4EkkMQYMMzL0vyjL2yNEJJPbt8Vw8e+/gL8/sGsXUKaM3FWRDWIfDDPK7+DJgEFEMsgLF6dOAQEB4miRGjXkropsFFswzOjS3XQAQCWOICEic0tIANq2FcNFYKDYcsFwQSbEFgwzupSYCgCo5MM5MIjIzLZuBU6fzg8X1arJXRHZOAYMM8nK0XGIKhHJp29fIDNTbMVguCAzYMAwk2v30iEIgJuTA3zc1HKXQ0T2ID4ecHICvL3F7QED5K2H7Ar7YJhJXutFBW9nLnJGRKZ36xbwwgviyqgPHshdDdkhBgwzufkgEwBQ3otLtBORid28KYaLuDixc+f9+3JXRHaIAcNMbj4QWzACGTCIyJRu3BDDxblzwDPPALt3A5UqyV0V2SH2wTATBgwiMrnr18VOnOfPi+Fi1y4gOFjuqshOsQXDTG7kBozy3gwYRGQC166JLRfnz4uhguGCZMaAYSZ5ASPQk4ucEZEJZGQA6eni7RCGC7IAvEViBtk6PW4liZ08g8pwFk8iMoHq1cWpv52dgYoV5a6GiC0Y5nDrQSZ0egFqByXKuTnJXQ4R2YorV4CYmPztGjUYLshiMGCYwbX74hokFbyduUw7EUnj8mWxz8WLLwI7dshdDVEhDBhmkDeChHNgEJEk8sLF5ctAUBAXLSOLxIBhBvG5/S8C2MGTiJ7WpUtASIh4e6RaNbFDZ/nycldFVAgDhhncShYDhr8nWzCI6ClcvCi2XFy9KnbqZLggC8ZRJGbAFgwiemp5039fuybeEtm5EwgIkLsqomKxBcMM8oao+jNgEFFp+foCrVoBNWsyXJBVYAuGGcQniZ082YJBRKXm4AAsWSKujOrjI3c1RE/EFgwTy8zW4X56NgAgwIN9MIjICOfOAR99BOj14raDA8MFWQ22YJhYXv8LZ0cVPJz5dhNRCcXFiQuX3boFuLkB48fLXRGRUdiCYWK3HurgqVBwki0iKoGzZ/PDRZ06wPvvy10RkdH4J7WJ3UvTAgB83DlFOBGVQF64iI8H6tYVpwIvV07uqoiMxhYME3uQIQYML2dHmSshIot35ow4FDU+HqhXT5wCnOGCrBQDhok9yO3g6eXCgEFEj5GZCYSFAQkJQIMGYrhgh06yYgwYJpaUkRcw1DJXQkQWTaMB5s4FmjcHtm8HypaVuyKip8KAYWL3c/tgePIWCREVRRDyn3ftChw4wHBBNuGpAkZmZqZUddisBxm8RUJExThxAmjSRFxjJI+Sf/eRbTD6/2S9Xo9JkyahfPnycHNzw8Xcfxhjx47Fzz//LHmB1i4prw+GM2+RENFD/vkHaNcOOHYMGDFC7mqIJGd0wPj8888RHR2Nr776Cmp1/odmnTp1MH/+fEmLswWGUSRswSCiPLGxQPv2wN27YgsG/zgjG2R0wFi8eDF+/PFH9OnTByqVyrC/fv36OHv2rKTF2YK8USTsg0FEAIDjx/PDRdOmwLZtgLe33FURSc7ogHHjxg1UrVq10H69Xo/s7GxJirIVgiCwDwYR5Tt2TAwX9+4BzZqJ4cLLS+6qiEzC6IBRu3Zt7N27t9D+NWvWoGHDhpIUZSsys/XQ5oiLFHlzmCqRfRME4MMPgfv3xaGof/4JeHrKXRWRyRg9Vfi4ceMQERGBGzduQK/XY+3atYiLi8PixYuxceNGU9RotfL6XziqFHBRq55wNBHZNIUCWL0aGDkSmDUL8PCQuyIikzK6BaNbt274/fffsX37dri6umLcuHE4c+YMfv/9d3To0MEUNVqt+2l5/S/UXOiMyF7dvZv/3MdH7NDJcEF2oFSLnbVu3Rrbtm2TuhabwxEkRHbuyBFx+u+pU4EBA+SuhsisjG7BqFy5Mu4+nMhzPXjwAJUrV5akKFuRPwcGAwaR3Tl8GAgNFftcLFsG6HRyV0RkVkYHjMuXL0NXxD+UrKws3LhxQ5KibAVHkBDZqb/+Ajp0AJKTgdatgU2bABX7YZF9KfEtkg0bNhieb926FZ4P9X7W6XSIiYlBcHCwpMVZu/w5MDiChMhuHDwo3hZJSQHatBHDhZub3FURmV2JA0b37t0BAAqFAhEREQW+5+joiODgYMyYMUPS4qwd+2AQ2ZkDB4BOncRw8cILwMaNgKur3FURyaLEAUOvF+dzqFSpEo4cOQIfHx+TFWUr2AeDyM7s3CmGi7Ztgd9/Z7ggu2b0KJJLly6Zog6blHeLxMuVt0iI7MInnwCBgUB4OODiInc1RLIq1brAaWlp2Lx5M+bNm4evv/66wMNYc+fORXBwMDQaDZo3b47Dhw8/9vgHDx5g8ODBCAgIgJOTE6pXr47NmzeX5scwOcMtErZgENmuY8eAtDTxuUIBREYyXBChFC0Yx48fR5cuXZCeno60tDSUKVMGiYmJcHFxga+vL4YOHVria61cuRJRUVGYN28emjdvjtmzZyMsLAxxcXHw9fUtdLxWq0WHDh3g6+uLNWvWoHz58rhy5Qq8LHQuf0MLBvtgENmmPXuALl3EdUU2bmSwIHqI0S0Yw4cPR9euXXH//n04Ozvj0KFDuHLlCho3bozp06cbda2ZM2eif//+iIyMRO3atTFv3jy4uLhgwYIFRR6/YMEC3Lt3D+vXr0erVq0QHByMkJAQ1K9f39gfwywMAYOjSIhsz+7dQOfOYuuFo6PYekFEBkYHjNjYWHz44YdQKpVQqVTIyspCUFAQvvrqK3zyySclvo5Wq8XRo0cRGhqaX4xSidDQUBw8eLDIczZs2IAWLVpg8ODB8PPzQ506dTB58uQi5+XIk5WVheTk5AIPc+EoEiIbtWuX2HKRni4OSV2/HnB2lrsqIotidMBwdHSEUime5uvri6tXrwIAPD09ce3atRJfJzExETqdDn5+fgX2+/n5IT4+vshzLl68iDVr1kCn02Hz5s0YO3YsZsyYgc8//7zY15kyZQo8PT0Nj6CgoBLX+DQys3XIzBZH3ngyYBDZjh078sNFp04MF0TFMLoPRsOGDXHkyBFUq1YNISEhGDduHBITE7FkyRLUqVPHFDUa6PV6+Pr64scff4RKpULjxo1x48YNTJs2DePHjy/ynNGjRyMqKsqwnZycbJaQkZQ7i6dKqYC7U6mWfCEiS7NjB/DSS0BGhnh7ZO1aQKORuyoii2T0J9/kyZORkpICAPjiiy/Qt29fvP/++6hWrRp+/vnnEl/Hx8cHKpUKCQkJBfYnJCTA39+/yHMCAgLg6OgI1UNT7taqVQvx8fHQarVQqwv3dXBycoKTk1OJ65JK/iyejlxJlchWeHmJgaJdO+DXXwEZfrcQWQujA0aTJk0Mz319fbFly5ZSvbBarUbjxo0RExNjmCVUr9cjJiYGQ4YMKfKcVq1aYfny5dDr9YbbNOfOnUNAQECR4UJOD9I5RJXI5jRqJM7WWakSwwXRE5RqHoyiHDt2DC+99JJR50RFReGnn37CokWLcObMGbz//vtIS0tDZGQkAKBv374YPXq04fj3338f9+7dw7Bhw3Du3Dls2rQJkydPxuDBg6X6MSSTt9AZ+18QWbk//xRDRZ6aNRkuiErAqBaMrVu3Ytu2bVCr1Xj33XdRuXJlnD17FqNGjcLvv/+OsLAwo148PDwcd+7cwbhx4xAfH48GDRpgy5Ytho6fV69eNbRUAEBQUBC2bt2K4cOHo169eihfvjyGDRuGkSNHGvW65pDXguHtYlktK0RkhC1bgO7dAbVaXMTs2WflrojIapQ4YPz888/o378/ypQpg/v372P+/PmYOXMmPvjgA4SHh+PkyZOoVauW0QUMGTKk2Fsiu3btKrSvRYsWOHTokNGvY24PuA4JkXX74w/glVeArCyxQ2e1anJXRGRVSnyLZM6cOfjyyy+RmJiIVatWITExEd999x3+/fdfzJs3r1ThwpbxFgmRFdu8WWy5yMoSQ8aqVWIrBhGVWIkDxoULF9CzZ08AQI8ePeDg4IBp06ahQoUKJivOmnEWTyIrtXGjGCq0WuDVV4GVK8WZOonIKCUOGBkZGXDJnWdfoVDAyckJAQEBJivM2iVxFk8i63PgANCjhxguXnsN+OUXhguiUjKqk+f8+fPh5uYGAMjJyUF0dDR8fHwKHGPMYme2jAudEVmhRo2A0FDAzQ1YtozhgugpKARBEEpyYHBw8BMnjFIoFLh48aIkhZlKcnIyPD09kZSUBA8PD5O9Tpc5e3H6VjKiI5vihRqFV4YlIguVmQk4OIgPIirAmM/QEv8Lunz58tPWZVfypgr34jBVIsu2bh1w6BAwdaq4Iiqn/iaSBCO6iXAmTyIrsHYtEB4O5OQADRoAb7whd0VENkOymTwpnzZHjzStuIQ8+2AQWag1a4BevcRw0acPkDtKjoikwYBhAg9yR5AoFICHhgGDyOKsXg28/jqg0wFvvQUsWsQ+F0QSY8AwgXtp+bdHlEqupEpkUVauFG+F6HRA377AwoXAQys0E5E0GDBMIC9glHXjgkhEFuXaNbHFQqcDIiKABQsYLohMpFQB48KFCxgzZgzeeOMN3L59GwDwxx9/4NSpU5IWZ61SM3MAAG5ObHIlsihBQcD8+UC/fsDPPzNcEJmQ0QFj9+7dqFu3Lv766y+sXbsWqampAIB//vkH48ePl7xAa5Se28GTAYPIQmRn5z/v21cMGQwXRCZldMAYNWoUPv/8c8Oy7XnatWtnFaucmkNqltiC4erEX2BEslu6FGjYEIiPl7sSIrtidMD4999/8corrxTa7+vri8TEREmKsnZpeQFDzRYMIlktWSL2tTh1CvjxR7mrIbIrRgcMLy8v3Lp1q9D+48ePo3z58pIUZe0MAYO3SIjks2iRGC70euC994AxY+SuiMiuGB0wXn/9dYwcORLx8fFQKBTQ6/XYv38/RowYgb59+5qiRquTN8kWAwaRTKKjgchIQBCAgQOB774DlBw0R2RORv+Lmzx5MmrWrImgoCCkpqaidu3aaNOmDVq2bIkx/AsBQH4Lhhv7YBCZ38KFwDvviOFi0CCGCyKZGP0ntlqtxk8//YSxY8fi5MmTSE1NRcOGDVGtWjVT1GeV8jp5urAPBpF5ZWYCU6aI4WLwYOCbb8QpdYnI7Iz+BNy3bx+ef/55VKxYERUrVjRFTVaPw1SJZKLRADExYv+LTz9luCCSkdHthu3atUOlSpXwySef4PTp06aoyeqlspMnkXldupT/PChI7NDJcEEkK6MDxs2bN/Hhhx9i9+7dqFOnDho0aIBp06bh+vXrpqjPKqVxHgwi8/nhB6B6dWDVKrkrIaKHGB0wfHx8MGTIEOzfvx8XLlxAz549sWjRIgQHB6Ndu3amqNHqcJgqkZl8/704SiQnBzhyRO5qiOghT9W1ulKlShg1ahSmTp2KunXrYvfu3VLVZdUMw1TZyZPIdL77ThwlAgAffgh89ZW89RBRAaUOGPv378egQYMQEBCA3r17o06dOti0aZOUtVmt/GGqDBhEJvHtt+IoEQD46CNg2jT2uSCyMEZ/Ao4ePRorVqzAzZs30aFDB8yZMwfdunWDi4uLKeqzOnq9YBhFwj4YRCbwzTfA0KHi848/BqZOZbggskBGB4w9e/bgo48+Qq9eveDj42OKmqxaerbO8Jx9MIhMIC5O/DpqFDB5MsMFkYUy+hNw//79pqjDZuTdHlEpFXBy4OyBRJL75hugY0ega1eGCyILVqKAsWHDBnTu3BmOjo7YsGHDY499+eWXJSnMWhnmwFCroOAvPyJp/PYb0LkzoFaLocLOf88QWYMSBYzu3bsjPj4evr6+6N69e7HHKRQK6HS6Yr9vD9jBk0hiM2YAI0YA3bsDa9YAKvZtIrIGJfoU1Ov1RT6nwlIzcwOGhgGD6KlNmyZ25ASAevW4aBmRFTH6X+vixYuRlZVVaL9Wq8XixYslKcqapbAFg0gaX36ZHy7GjwcmTGCfCyIrYnTAiIyMRFJSUqH9KSkpiIyMlKQoa5bfguEocyVEVmzqVHGUCAB89pn4ICKrYvSf2YIgFNl58fr16/D09JSkKGuWamjB4H1iolKZNg0YPVp8PnEiMHasvPUQUamUOGA0bNgQCoUCCoUC7du3h4ND/qk6nQ6XLl1Cp06dTFKkNUnnNOFET6dZM8DFRQwZY8bIXQ0RlVKJPwXzRo/ExsYiLCwMbm5uhu+p1WoEBwfj1VdflbxAa5OuFVswXNRswSAqlZAQ4MwZoGJFuSshoqdQ4oAxfvx4AEBwcDDCw8Oh0WhMVpQ1y2vBcGEnT6KSmz4d6NQJqFNH3Ga4ILJ6RnfyjIiIYLh4DEMLhiNbMIhK5LPPxAXL2rUD7t6VuxoikkiJ/swuU6YMzp07Bx8fH3h7ez92hsp79+5JVpw1YgsGUQkJghguJk4Utz/+GChbVtaSiEg6JfoUnDVrFtzd3Q3POQV28QwBg30wiIonCMC4ccDnn4vb06cDH34ob01EJKkSBYyIiAjD87fffttUtdiEDAYMoscTBHHo6RdfiNszZwLDh8tbExFJzug+GMeOHcO///5r2P7tt9/QvXt3fPLJJ9BqtZIWZ40ycpdr17APBlHR5s/PDxezZjFcENkoowPGe++9h3PnzgEALl68iPDwcLi4uGD16tX4OG9aXzuW14LhzIBBVLTXXwdatQJmzwb+9z+5qyEiEzE6YJw7dw4NGjQAAKxevRohISFYvnw5oqOj8euvv0pdn9XJa8Fw5i0SonyCkP/c3R3YtQsYNky2cojI9IwOGIIgGFZU3b59O7p06QIACAoKQmJiorTVWSG2YBA9QhDEYahTpuTvc+AoKyJbZ/S/8iZNmuDzzz9HaGgodu/eje+//x4AcOnSJfj5+UleoLVhCwbRQwQBGDFC7MgJiJNpNWwob01EZBZGt2DMnj0bx44dw5AhQ/Dpp5+iatWqAIA1a9agZcuWkhdobQwBgy0YZO8EAYiKyg8X33/PcEFkR4xuwahXr16BUSR5pk2bBpXKvj9UdXoB2hzx9hEDBtk1QRBHh8yZI27/8AMwYIC8NRGRWZX6RujRo0dx5swZAEDt2rXRqFEjyYqyVnmtFwBvkZAdEwSxA+c334jbP/4I9O8vb01EZHZGB4zbt28jPDwcu3fvhpeXFwDgwYMHaNu2LVasWIFy5cpJXaPVyOvgqVAATg5G330isg27d4vhQqEAfvoJ6NdP7oqISAZGfwp+8MEHSE1NxalTp3Dv3j3cu3cPJ0+eRHJyMoYOHWqKGq3GwyNIOJ062a0XXhDnuJg/n+GCyI4Z3YKxZcsWbN++HbVq1TLsq127NubOnYuOHTtKWpy1YQdPslt6PZCWJs5xAXCOCyIyvgVDr9fD0dGx0H5HR0fD/Bj2itOEk13S64FBg4C2bYEHD+SuhogshNEBo127dhg2bBhu3rxp2Hfjxg0MHz4c7du3l7Q4a8OFzsju6PXAwIHiKJFjx4A9e+SuiIgshNEB49tvv0VycjKCg4NRpUoVVKlSBZUqVUJycjK+yes1bqcysnMAcAQJ2Qm9HnjvPbEjp1IJLF4MvPyy3FURkYUwug9GUFAQjh07hpiYGMMw1Vq1aiE0NFTy4qxNhla8RcRbJGTz9Hpx6OmCBfnhok8fuasiIgtiVMBYuXIlNmzYAK1Wi/bt2+ODDz4wVV1WiZ08yS7o9cC77wILF4rhYskSoHdvuasiIgtT4oDx/fffY/DgwahWrRqcnZ2xdu1aXLhwAdOmTTNlfVYlQyveImEfDLJpt24BW7aI4WLZMnH5dSKiR5S4D8a3336L8ePHIy4uDrGxsVi0aBG+++47U9ZmddiCQXahfHlg505g9WqGCyIqVokDxsWLFxEREWHY7t27N3JycnDr1i2TFGaNMrPFPhhOjpzFk2yMTgfExuZv16gB9OghWzlEZPlK/EmYlZUFV1fX/BOVSqjVamRkZJikMGuUt9CZkwNbMMiG6HTA228Dzz0HbN0qdzVEZCWM6uQ5duxYuLi4GLa1Wi2++OILeHp6GvbNzFua2Q5pdWLAUHMdErIVOTlARASwfDng4ACkpspdERFZiRIHjDZt2iAuLq7AvpYtW+LixYuGbXtffyMrtw8GFzojm5CTA/TtC/zyixguVq7kbREiKrESB4xdu3aZsAzbYGjBUDFgkJXLyQHefFMMFQ4OwKpVwCuvyF0VEVkRoyfaouJl5fAWCdmAnBxx0qxVqwBHR3G0SLducldFRFaGAUNCWYZOngwYZOVUKjFcrFnD6b+JqFT4SSghraEFg6NIyIo5OIhTf+/fz3BBRKXGgCEhLW+RkLXKzga++04ckgqIIaNpU3lrIiKrxk9CCWXlcBQJWSGtFggPBwYPFh9ERBIo1Sfh3r178eabb6JFixa4ceMGAGDJkiXYt2+fpMVZG7ZgkNXJCxfr1gFOTuzMSUSSMfqT8Ndff0VYWBicnZ1x/PhxZGVlAQCSkpIwefJkyQu0Jpxoi6yKVgv07AmsXy+Gi/Xrgc6d5a6KiGyE0Z+En3/+OebNm4effvoJjo6Ohv2tWrXCsWPHJC3O2mRlcxQJWYmsLOC114ANGwCNRvzaqZPcVRGRDTF6mGpcXBzatGlTaL+npycePHggRU1WK68FgwGDLF6fPsDvv+eHiw4d5K6IiGyM0Z+E/v7+OH/+fKH9+/btQ+XKlUtVxNy5cxEcHAyNRoPmzZvj8OHDJTpvxYoVUCgU6N69e6leV2qGPhgqDlMlCxcRAXh6iiGD4YKITMDogNG/f38MGzYMf/31FxQKBW7evIlly5ZhxIgReP/9940uYOXKlYiKisL48eNx7Ngx1K9fH2FhYbh9+/Zjz7t8+TJGjBiB1q1bG/2apmKYaIvLtZOl69oVuHwZCA2VuxIislFGfxKOGjUKvXv3Rvv27ZGamoo2bdrg3XffxXvvvYcPPvjA6AJmzpyJ/v37IzIyErVr18a8efPg4uKCBQsWFHuOTqdDnz59MGHChFK3mphCfgsGAwZZmMxMoF8/4KHFCeHlJVs5RGT7jP4kVCgU+PTTT3Hv3j2cPHkShw4dwp07dzBp0iSjX1yr1eLo0aMIfeivKKVSidDQUBw8eLDY8yZOnAhfX1/069fvia+RlZWF5OTkAg9T4TBVskgZGeLw0wULgJdeyp9Mi4jIhEq9FolarUbt2rWf6sUTExOh0+ng5+dXYL+fnx/Onj1b5Dn79u3Dzz//jNjY2BK9xpQpUzBhwoSnqrMk9HqBnTzJ8uSFi23bAFdXYN48cZ0RIiITMzpgtG3bFgqFotjv79ix46kKepyUlBS89dZb+Omnn+Dj41Oic0aPHo2oqCjDdnJyMoKCgiSvLS9cAGzBIAuRni6Gi+3bxXDxxx+ABfVZIiLbZnTAaNCgQYHt7OxsxMbG4uTJk4iIiDDqWj4+PlCpVEhISCiwPyEhAf7+/oWOv3DhAi5fvoyuXbsa9un14ge7g4MD4uLiUKVKlQLnODk5wcnJyai6SoMBgyxKerq4UFlMDODmJoaL55+XuyoisiNGB4xZs2YVuf+zzz5DamqqUddSq9Vo3LgxYmJiDENN9Xo9YmJiMGTIkELH16xZE//++2+BfWPGjEFKSgrmzJljkpaJksqbZAtgJ0+yAB9/nB8utmwBWrWSuyIisjOl7oPxqDfffBPNmjXD9OnTjTovKioKERERaNKkCZo1a4bZs2cjLS0NkZGRAIC+ffuifPnymDJlCjQaDerUqVPgfK/cnvCP7je3h6cJf9wtJCKz+Owz4J9/gC+/BFq2lLsaIrJDkgWMgwcPQqPRGH1eeHg47ty5g3HjxiE+Ph4NGjTAli1bDB0/r169CqXS8lsE8kaQOLH1guSi0+V34PTxAfbsARh2iUgmRgeMHj16FNgWBAG3bt3C33//jbFjx5aqiCFDhhR5SwQAdu3a9dhzo6OjS/WaUjMs1c5JtkgOqaniENQ33gDee0/cx3BBRDIyOmB4enoW2FYqlahRowYmTpyIjh07SlaYteEkWySblBSgSxdg3z7xtsirr4otGEREMjIqYOh0OkRGRqJu3brw9vY2VU1WiZNskSxSUsQl1vfvF9cW2bqV4YKILIJRn4YqlQodO3a0+1VTi2JYh8SBkxiRmSQni0us54WLbduAZs3kroqICEAppgqvU6cOLj68ngEBYAsGmVleuDhwQFxTZPt2oGlTuasiIjIw+tPw888/x4gRI7Bx40bcunXLbOt8WLosBgwyp1WrgIMHAW9vMVw0aSJ3RUREBZS4D8bEiRPx4YcfokuXLgCAl19+ucB8D4IgQKFQQGenCykZRpEwYJA59OsH3LkDhIUBjRrJXQ0RUSElDhgTJkzAwIEDsXPnTlPWY7V4i4RMLikJcHAQ1xVRKIDRo+WuiIioWCUOGIIgAABCQkJMVow1M8zkyWGqZAoPHgAdO4pTf2/cCLi4yF0REdFjGfVpyCmwi5e3FomTI0eRkMTu3wc6dACOHAFOnACuXpW7IiKiJzJqHozq1as/MWTcu3fvqQqyVmzBIJO4d08MF8eOifNbxMQANWvKXRUR0RMZFTAmTJhQaCZPErEPBknu3j0gNBQ4flwMFzt2AHXryl0VEVGJGBUwXn/9dfj6+pqqFqtmWOyMAYOkcPeuGC5iY4Fy5cRwIfOKwURExijxpyH7Xzxetl4MGA5Kvk8kgZs3gStXAF9fYOdOhgsisjpGjyKhouXoxPfHgX0wSAp164oTaGk0QO3acldDRGS0EgcMfe5f6FQ0nT43YLAFg0orMRG4dCl/ym9OoEVEVox/bkskO3cUiYOKAYNK4c4doF07oH174NAhuashInpqDBgSYQsGldrt22K4+PdfcSItb2+5KyIiempGjSKh4mXn9sFQKZnZyAh54eLUKSAwUOzQWb263FURET01fhpKRJfbR8WRt0iopBISgLZtxXBRvjywaxfDBRHZDLZgSCRHn9eCwYBBJXDnjhguzpzJDxdVq8pdFRGRZBgwJKJjwCBjuLsDwcFASop4W4ThgohsDAOGRHIMnTx514lKQKMB1q4V+2BUrCh3NUREkuOnoUQ4ioSe6OZN4MsvgbxJ6zQahgsisllswZAI+2DQY924Ifa5+O8/QK8HRo+WuyIiIpNiC4ZE8kaRcKItKuT6deCFF8Rw8cwzwBtvyF0REZHJMWBIJEfHFgwqwrVrYrg4f17s1Ll7t/iViMjGMWBIhH0wqJC8cHHhAlCpkjgU9Zln5K6KiMgsGDAkkt8Hg28pAcjKEtcVuXgRqFyZ4YKI7A4/DSXCFgwqwMkJGDdOnJlz1y6OFiEiu8OAIRFOtEWFvPkmcOIEEBQkdyVERGbHgCERtmAQLl0COnUCbt3K3+fkJF89REQyYsCQSE7uMFW2YNipixfFDp1btwIDB8pdDRGR7BgwJMJbJHbswgUxXFy9Kva5+P57uSsiIpIdZ/KUCGfytFN54eL6daBGDXHhsoAAuasiIpIdWzAkouNiZ/bn/HkgJEQMFzVriqNFGC6IiAAwYEiGLRh26N13xTVGatUSWy78/eWuiIjIYjBgSMTQgsG1SOzHkiVA164MF0RERWAfDInk6DiKxC5kZADOzuLzoCBgwwZ56yEislBswZBIbgMG58GwZXFxYkfOVavkroSIyOIxYEiE82DYuLNnxdEi164BU6cCOTlyV0REZNEYMCTCUSQ27MwZMVzExwP16gF//gk48O4iEdHj8NNQIhxFYqNOnxbDRUICUL8+EBMD+PjIXRURkcVjwJCAXi9AYB8M23PqFNC2LXD7NtCgAcMFEZERGDAkkNd6AQBKBgzbsXy5GC4aNhTDRdmycldERGQ1eCNZAnohP2DwFokN+fxzwMsL6NcPKFNG7mqIiKwKWzAk8HDAYL6wcufPA1qt+FyhAD76iOGCiKgUGDAk8NAdEigVTBhW659/gOeeA3r1yg8ZRERUKgwYEtA93AeDAcM6xcYC7doBd+8CN2+KM3YSEVGpMWBIQGAfDOt2/DjQvj1w7x7QvDmwbRvg6Sl3VUREVo0BQwIFWzBkLISMd+xYfrh47jlg61aGCyIiCTBgSECX24KhUAAK3iKxHkePiuHi/n2gRQuGCyIiCTFgSCDvDomK4cK6pKWJnTlbtgS2bAE8POSuiIjIZnAeDAnk3SJhB08r06YNsHMnUKsW4O4udzVERDaFAUMCefNgcJ0zK3D4MKDRiIuWAUCzZvLWQ0Rko/iRKIHcldrZgmHpDh0COnQQ+12cPSt3NURENo0BQwJ5LRjsg2HBDh4EOnYEkpOB2rWBChXkroiIyKYxYEjg4VEkZIEOHADCwoCUFCAkBNi8GXBzk7sqIiKbxoAhAX1uJ09OsmWB9u/PDxcvvABs2gS4uspdFRGRzWPAkEDePFvsg2Fhjh4FOnUCUlPFacAZLoiIzIajSCRgGKbKFgzLUr06UL++OGpkwwbAxUXuioiI7AYDhgTYydNCubsDf/wBqFQMF0REZsZbJBIwzIPBfCG/3buBadPyt93dGS6IiGTAFgwJGPpgMGHIa+dO4KWXgPR0oGJFIDxc7oqIiOwWWzAkwKnCLcCOHcCLL4rholMnoFs3uSsiIrJrDBgSEAQOU5VVTIzYcpGRAXTpAqxbJ3bsJCIi2fAWiQTyWjDYgCGD7duBrl2BzEwxXKxdCzg5yV0VEZHdYwuGBDgPhkxu3ABeflkMFy++yHBBRGRB2IIhAYGjSORRvjwwdarYirF6NcMFEZEFYQuGBNiCYWa5gQ4AMHQosH49wwURkYVhwJCA3rDYGQOGyf3xB9C6NXD/fv4+Jf83JiKyNPzNLAFOtGUmmzcD3buLC5g9PJkWERFZHAYMCeQ12PMWiQlt3Ai88gqg1QKvvgpMmCB3RURE9BgMGBJgJ08T+/13oEcPMVy89hrwyy+Ao6PcVRER0WNYRMCYO3cugoODodFo0Lx5cxw+fLjYY3/66Se0bt0a3t7e8Pb2Rmho6GOPNwe9XvzKPhgmsGGD2GKRnQ307AksX85wQURkBWQPGCtXrkRUVBTGjx+PY8eOoX79+ggLC8Pt27eLPH7Xrl144403sHPnThw8eBBBQUHo2LEjbty4YebK87EPholkZQHDhonhIjyc4YKIyIrIHjBmzpyJ/v37IzIyErVr18a8efPg4uKCBQsWFHn8smXLMGjQIDRo0AA1a9bE/PnzodfrERMTY+bK8+UNU2ULhsScnICtW4EPPgCWLgUcOG0LEZG1kDVgaLVaHD16FKGhoYZ9SqUSoaGhOHjwYImukZ6ejuzsbJQpU6bI72dlZSE5ObnAQ2rsgyGxxMT859WrA19/zXBBRGRlZA0YiYmJ0Ol08PPzK7Dfz88P8fHxJbrGyJEjERgYWCCkPGzKlCnw9PQ0PIKCgp667kexBUNCa9YAlSqJLRdERGS1ZL9F8jSmTp2KFStWYN26ddAUs3rm6NGjkZSUZHhcu3ZN8jp0bMGQxurVwOuvA6mpYtAgIiKrJWu7s4+PD1QqFRISEgrsT0hIgL+//2PPnT59OqZOnYrt27ejXr16xR7n5OQEJxNPI63Xc7n2p7ZqFdC7N6DTAX37AvPmyV0RERE9BVlbMNRqNRo3blygg2Zeh80WLVoUe95XX32FSZMmYcuWLWjSpIk5Sn2s/FEkDBilsmJFfriIiAAWLABUKrmrIiKipyB7z7moqChERESgSZMmaNasGWbPno20tDRERkYCAPr27Yvy5ctjypQpAIAvv/wS48aNw/LlyxEcHGzoq+Hm5gY3NzdZfgYdWzBK75dfgDffFCcTiYwEfvqJ4YKIyAbIHjDCw8Nx584djBs3DvHx8WjQoAG2bNli6Ph59epVKB9azOr777+HVqvFa6+9VuA648ePx2effWbO0g3YgvEU/vhDDBfvvCOGCy5cRkRkE2QPGAAwZMgQDBkypMjv7dq1q8D25cuXTV+QkXS5M3kyYJTCggVASIjYesFwQURkM/gbXQJ5LRgqvpsls2+f2N8CEOe36NeP4YKIyMbwt7oE8gMGWzCeaNEioE0bMVTkhQwiIrI5DBgSyOvkyVskTxAdLd4KEQTA2Rng+0VEZLMYMCSQN5MnWzAeY8ECsSOnIADvvw/MncvbIkRENoy/4SWQtxYJ40Uxfv4ZePddMVwMGsRwQURkB/hbXgKGYapswSjs4XAxZAjw7be8NUJEZAcsYpiqtTMsdsY2jMJ8fQFHR/G2yOzZDBdERHaCAUMCuQ0YXOysKF27AkePAnXqMFwQEdkR3iKRAGfyfMSiRcCFC/nbdesyXBAR2RkGDAkYOnnyMxT47jvg7beBtm2BxES5qyEiIpkwYEgg7xaJwt4Txty5wODB4vPwcKBsWXnrISIi2TBgSEDPPhjAN9+Io0QA4OOPga++YpMOEZEdY8CQgN7eb5F8/TUwdKj4fORIYOpUO34ziIgIYMCQRG4Dhn128ly6FBg2THw+ejQwZQrDBRERcZiqFAR7HkXSqRNQr544HHXSJIYLIiICwIAhibxbJHbJxwc4cABwcWG4ICIiA94ikZDdfL5OmwbMm5e/7epqRz88ERGVBFswJGBXDRhffgmMGiU+b9oUaNxY3nqIiMgisQVDQja/FsnUqfnhYsIEhgsiIioWA4YE7KIBY/JkcZQIIHbmHDdO3nqIiMii8RaJhGy2G8IXXwBjxuQ//+QTeeshIiKLx4AhAZvug7FnT364eLgVg4iI6DEYMCRkkw0YbdqIt0NcXMRZOomIiEqAAYMKEwQgOxtQq8XtCRPkrYeIiKwOO3lKQLClbp6CAIwfD4SFAenpcldDRERWigFDQlbfyVMQxNshkyYBu3YBGzfKXREREVkp3iKRgi00YAiC2Jlz8mRxe+ZMoFcveWsiIiKrxYAhIYW1NmEIgjj0dOpUcXvWLOB//5O1JCIism4MGBKw6gYMQRCHnn75pbg9Zw4wdKi8NRERkdVjwJCQVbZf3LwJ/Pij+Pybb4AhQ+Sth4iIbAIDhgQEa55pq3x5ICYG+PtvoH9/uashIiIbwYAhJWtpwhAE4PJloFIlcbthQ/FBREQkEQ5TlYBVNWAIAvDhh0D9+sDBg3JXQ0RENooBQ0IWv1y7IADDh4ujRFJSgFOn5K6IiIhsFG+RSMAqGjAEARg2TOzICYgdO999V96aiIjIZjFgSMhip8EQBOCDD4C5c8Xtn35iuCAiIpNiwJCARffBEARx6Ol334kJaP584J135K6KiIhsHAOGhCyyASM7WxwxolAAP/8MREbKXREREdkBBgxbp1YDv/4K7N4trpBKRERkBhxFIgGLW65drwdWr86/d6PRMFwQEZFZMWBIyCI6eer1wMCB4kqoH38sdzVERGSneItEAhbTyVOvBwYMEPtaKJVAgwZyV0RERHaKAUNCsk60pdeLa4ksWCCGiyVLgN695auHiIjsGgOGLdDpxHktoqPFcLFsGfD663JXRUREdox9MCQkWx+MAQPEcKFSAcuXM1wQEZHsGDAkIPty7W3bisNRly8HwsPlrYWIiAi8RSIp2XpgvPkmEBICBAXJVQEREVEBbMGQgNnbL3JygFGjgFu38vcxXBARkQVhwJCSOTph5OQAffsCX34pTp6Vk2P61yQiIjISb5FIwGxdMHJygLfeAlasABwcgIkTxa9EREQWhp9OEjJp+0VODtCnD7BqFeDoKE4F3q2bKV+RiIio1BgwJGDytUiys8VwsXq1GC5+/RXo2tW0r0lERPQU2AfDGowcKYYLtRpYu5bhgoiILB4DhoRM1sczKgp49lkxXLz0kolehIiISDq8RSIBk3TyFIT8xFKhAhAbyw6dRERkNdiCISHJFjvTaoGePYGVK/P3MVwQEZEVYcCQgKQNGFlZwGuviR05+/UD7tyR8upERERmwT+LJfTUfTDywsXGjYBGI/a5KFdOktqIiIjMiQFDApL0wcjKAl59Fdi0SQwXGzYAHTpIcGEiIiLzY8CQUKkbMDIzxXCxebMYLn7/HQgNlbI0IiIis2IfDEk8ZRPGokViuHB2Fm+PMFwQEZGVYwuGhErdB2PAAODcOeDFF4F27SStiYiISA4MGBIoVR+MjAxApRJn51QogBkzJK+LiIhILrxFIiFFSZswMjLEhcp69RLnvCAiIrIxbMGQgFEtGOnpYrjYvh1wdQXOngXq1TNZbURERHJgwDCn9HRxobIdO8Rw8ccfDBdERGSTeIvEXNLSxIXKduwA3NyALVuA1q3lroqIiMgk2IIhAeFJw1TzwsWuXYC7uxguWrY0S21ERERyYMCQULF9PM+eBY4cEcPF1q1AixZmrYuIiMjcGDAk8MROno0bi1OAq9UMF0REZBcYMCRUYLn21FTg+nWgZk1xOyREnqKIiIhkwE6eEijUgJGSAnTuLHbi/PdfOUoiIiKSFQOGhBQKAMnJQKdOwL59QHa2uJAZERGRnbGIgDF37lwEBwdDo9GgefPmOHz48GOPX716NWrWrAmNRoO6deti8+bNZqq0aIY+GJmZYrg4cADw8hIn02raVM7SiIiIZCF7wFi5ciWioqIwfvx4HDt2DPXr10dYWBhu375d5PEHDhzAG2+8gX79+uH48ePo3r07unfvjpMnT5q58sIU8+cDBw8C3t5iuGjSRO6SiIiIZCF7wJg5cyb69++PyMhI1K5dG/PmzYOLiwsWLFhQ5PFz5sxBp06d8NFHH6FWrVqYNGkSGjVqhG+//dbMlecT8tYTuXo1P1w0bixbPURERHKTNWBotVocPXoUoaGhhn1KpRKhoaE4ePBgkeccPHiwwPEAEBYWVuzxWVlZSE5OLvCQXO7gEYWzMxATAzRqJP1rEBERWRFZA0ZiYiJ0Oh38/PwK7Pfz80N8fHyR58THxxt1/JQpU+Dp6Wl4BAUFSVP8QyoHeKFpkAf8PhoKNGwo+fWJiIisjc3PgzF69GhERUUZtpOTkyUPGUPaVcOQdtUkvSYREZE1kzVg+Pj4QKVSISEhocD+hIQE+Pv7F3mOv7+/Ucc7OTnByclJmoKJiIioRGS9RaJWq9G4cWPExMQY9un1esTExKBFMVNqt2jRosDxALBt27ZijyciIiLzk/0WSVRUFCIiItCkSRM0a9YMs2fPRlpaGiIjIwEAffv2Rfny5TFlyhQAwLBhwxASEoIZM2bgxRdfxIoVK/D333/jxx9/lPPHICIioofIHjDCw8Nx584djBs3DvHx8WjQoAG2bNli6Mh59epVKJX5DS0tW7bE8uXLMWbMGHzyySeoVq0a1q9fjzp16sj1IxAREdEjFILwxLVAbUpycjI8PT2RlJQEDw8PucshIiKyGsZ8hso+0RYRERHZHgYMIiIikhwDBhEREUmOAYOIiIgkx4BBREREkmPAICIiIskxYBAREZHkGDCIiIhIcgwYREREJDkGDCIiIpIcAwYRERFJjgGDiIiIJMeAQURERJKTfbl2c8tbPDY5OVnmSoiIiKxL3mdnSRZit7uAkZKSAgAICgqSuRIiIiLrlJKSAk9Pz8ceoxBKEkNsiF6vx82bN+Hu7g6FQiHJNZOTkxEUFIRr167Bw8NDkmvaO76n0uN7Ki2+n9LjeyotU7yfgiAgJSUFgYGBUCof38vC7lowlEolKlSoYJJre3h48B+FxPieSo/vqbT4fkqP76m0pH4/n9RykYedPImIiEhyDBhEREQkOQYMCTg5OWH8+PFwcnKSuxSbwfdUenxPpcX3U3p8T6Ul9/tpd508iYiIyPTYgkFERESSY8AgIiIiyTFgEBERkeQYMIiIiEhyDBglNHfuXAQHB0Oj0aB58+Y4fPjwY49fvXo1atasCY1Gg7p162Lz5s1mqtR6GPOe/vTTT2jdujW8vb3h7e2N0NDQJ/43sDfG/j+aZ8WKFVAoFOjevbtpC7RCxr6nDx48wODBgxEQEAAnJydUr16d//YfYuz7OXv2bNSoUQPOzs4ICgrC8OHDkZmZaaZqLd+ePXvQtWtXBAYGQqFQYP369U88Z9euXWjUqBGcnJxQtWpVREdHm65AgZ5oxYoVglqtFhYsWCCcOnVK6N+/v+Dl5SUkJCQUefz+/fsFlUolfPXVV8Lp06eFMWPGCI6OjsK///5r5sotl7Hvae/evYW5c+cKx48fF86cOSO8/fbbgqenp3D9+nUzV26ZjH0/81y6dEkoX7680Lp1a6Fbt27mKdZKGPueZmVlCU2aNBG6dOki7Nu3T7h06ZKwa9cuITY21syVWyZj389ly5YJTk5OwrJly4RLly4JW7duFQICAoThw4ebuXLLtXnzZuHTTz8V1q5dKwAQ1q1b99jjL168KLi4uAhRUVHC6dOnhW+++UZQqVTCli1bTFIfA0YJNGvWTBg8eLBhW6fTCYGBgcKUKVOKPL5Xr17Ciy++WGBf8+bNhffee8+kdVoTY9/TR+Xk5Aju7u7CokWLTFWiVSnN+5mTkyO0bNlSmD9/vhAREcGA8Qhj39Pvv/9eqFy5sqDVas1VolUx9v0cPHiw0K5duwL7oqKihFatWpm0TmtVkoDx8ccfC88++2yBfeHh4UJYWJhJauItkifQarU4evQoQkNDDfuUSiVCQ0Nx8ODBIs85ePBggeMBICwsrNjj7U1p3tNHpaenIzs7G2XKlDFVmVajtO/nxIkT4evri379+pmjTKtSmvd0w4YNaNGiBQYPHgw/Pz/UqVMHkydPhk6nM1fZFqs072fLli1x9OhRw22UixcvYvPmzejSpYtZarZF5v5ssrvFzoyVmJgInU4HPz+/Avv9/Pxw9uzZIs+Jj48v8vj4+HiT1WlNSvOePmrkyJEIDAws9I/FHpXm/dy3bx9+/vlnxMbGmqFC61Oa9/TixYvYsWMH+vTpg82bN+P8+fMYNGgQsrOzMX78eHOUbbFK83727t0biYmJeP755yEIAnJycjBw4EB88skn5ijZJhX32ZScnIyMjAw4OztL+npswSCrM3XqVKxYsQLr1q2DRqORuxyrk5KSgrfeegs//fQTfHx85C7HZuj1evj6+uLHH39E48aNER4ejk8//RTz5s2TuzSrtGvXLkyePBnfffcdjh07hrVr12LTpk2YNGmS3KVRCbEF4wl8fHygUqmQkJBQYH9CQgL8/f2LPMff39+o4+1Nad7TPNOnT8fUqVOxfft21KtXz5RlWg1j388LFy7g8uXL6Nq1q2GfXq8HADg4OCAuLg5VqlQxbdEWrjT/jwYEBMDR0REqlcqwr1atWoiPj4dWq4VarTZpzZasNO/n2LFj8dZbb+Hdd98FANStWxdpaWkYMGAAPv30UyiV/PvYWMV9Nnl4eEjeegGwBeOJ1Go1GjdujJiYGMM+vV6PmJgYtGjRoshzWrRoUeB4ANi2bVuxx9ub0rynAPDVV19h0qRJ2LJlC5o0aWKOUq2Cse9nzZo18e+//yI2NtbwePnll9G2bVvExsYiKCjInOVbpNL8P9qqVSucP3/eENYA4Ny5cwgICLDrcAGU7v1MT08vFCLywpvAJbRKxeyfTSbpOmpjVqxYITg5OQnR0dHC6dOnhQEDBgheXl5CfHy8IAiC8NZbbwmjRo0yHL9//37BwcFBmD59unDmzBlh/PjxHKb6CGPf06lTpwpqtVpYs2aNcOvWLcMjJSVFrh/Bohj7fj6Ko0gKM/Y9vXr1quDu7i4MGTJEiIuLEzZu3Cj4+voKn3/+uVw/gkUx9v0cP3684O7uLvzyyy/CxYsXhT///FOoUqWK0KtXL7l+BIuTkpIiHD9+XDh+/LgAQJg5c6Zw/Phx4cqVK4IgCMKoUaOEt956y3B83jDVjz76SDhz5owwd+5cDlO1BN98841QsWJFQa1WC82aNRMOHTpk+F5ISIgQERFR4PhVq1YJ1atXF9RqtfDss88KmzZtMnPFls+Y9/SZZ54RABR6jB8/3vyFWyhj/x99GANG0Yx9Tw8cOCA0b95ccHJyEipXrix88cUXQk5OjpmrtlzGvJ/Z2dnCZ599JlSpUkXQaDRCUFCQMGjQIOH+/fvmL9xC7dy5s8jfi3nvY0REhBASElLonAYNGghqtVqoXLmysHDhQpPVx+XaiYiISHLsg0FERESSY8AgIiIiyTFgEBERkeQYMIiIiEhyDBhEREQkOQYMIiIikhwDBhEREUmOAYOIiIgkx4BBZGOio6Ph5eUldxmlplAosH79+sce8/bbb6N79+5mqYeISocBg8gCvf3221AoFIUe58+fl7s0REdHG+pRKpWoUKECIiMjcfv2bUmuf+vWLXTu3BkAcPnyZSgUCsTGxhY4Zs6cOYiOjpbk9Yrz2WefGX5OlUqFoKAgDBgwAPfu3TPp6xLZCi7XTmShOnXqhIULFxbYV65cOZmqKcjDwwNxcXHQ6/X4559/EBkZiZs3b2Lr1q1Pfe3ilu9+mKen51O/Tkk8++yz2L59O3Q6Hc6cOYN33nkHSUlJWLlyZamvae9Lt5P9YAsGkYVycnKCv79/gYdKpcLMmTNRt25duLq6IigoCIMGDUJqamqx1/nnn3/Qtm1buLu7w8PDA40bN8bff/9t+P6+ffvQunVrODs7IygoCEOHDkVaWtpja1MoFPD390dgYCA6d+6MoUOHYvv27cjIyIBer8fEiRNRoUIFODk5oUGDBtiyZYvhXK1WiyFDhiAgIAAajQbPPPMMpkyZUuDaebdIKlWqBABo2LAhFAoFXnjhBQAFb5H8+OOPCAwMLLBMOgB069YN77zzjmH7t99+Q6NGjaDRaFC5cmVMmDABOTk5j/05HRwc4O/vj/LlyyM0NBQ9e/bEtm3bDN/X6XTo168fKlWqBGdnZ9SoUQNz5swpcI28Wr/44gsEBgaiRo0aAIBr166hV69e8PLyQpkyZdCtWzdcvnz5sfUQWRMGDCIro1Qq8fXXX+PUqVNYtGgRduzYgY8//rjY4/v06YMKFSrgyJEjOHr0KEaNGgVHR0cAwIULF9CpUye8+uqrOHHiBFauXIl9+/ZhyJAhRtXk7OwMvV6PnJwczJkzBzNmzMD06dNx4sQJhIWF4eWXX8Z///0HAPj666+xYcMGrFq1CnFxcVi2bBmCg4OLvO7hw4cBANu3b8etW7ewdu3aQsf07NkTd+/exc6dOw377t27hy1btqBPnz4AgL1796Jv374YNmwYTp8+jR9++AHR0dH44osvSvwzXr58GVu3bi3Q+qDX61GhQgWsXr0ap0+fxrhx4/DJJ59g1apVBc6NiYlBXFwctm3bho0bNyI7OxthYWFwd3fH3r17sX//fri5uaFTp07QarUlronIoplsnVYiKrWIiAhBpVIJrq6uhsdrr71W5LGrV68WypYta9heuHCh4Onpadh2d3cXoqOjizy3X79+woABAwrs27t3r6BUKoWMjIwiz3n0+ufOnROqV68uNGnSRBAEQQgMDBS++OKLAuc0bdpUGDRokCAIgvDBBx8I7dq1E/R6fZHXByCsW7dOEARBuHTpkgBAOH78eIFjHl1evlu3bsI777xj2P7hhx+EwMBAQafTCYIgCO3btxcmT55c4BpLliwRAgICiqxBEARh/PjxglKpFFxdXQWNRmNYCnvmzJnFniMIgjB48GDh1VdfLVCrn5+fkJWVVeC1a9SoUeA9yMrKEpydnYWtW7c+9vpE1oJ9MIgsVNu2bfH9998btl1dXQGIf81PmTIFZ8+eRXJyMnJycpCZmYn09HS4uLgUuk5UVBTeffddLFmyxNDMX6VKFQDi7ZMTJ05g2bJlhuMFQYBer8elS5dQq1atImtLSkqCm5sb9Ho9MjMz8fzzz2P+/PlITk7GzZs30apVqwLHt2rVCv/88w8A8ZZBhw4dUKNGDXTq1AkvvfQSOnbs+FTvVZ8+fdC/f3989913cHJywrJly/D6669DqVQafs79+/cXaLHQ6XSPfd8AoEaNGtiwYQMyMzOxdOlSxMbG4oMPPihwzNy5c7FgwQJcvXoVGRkZ0Gq1aNCgQYFj6tatW6Dl459//sH58+fh7u5e4LjMzExcuHDhad4KIovBgEFkoVxdXVG1atUC+y5fvoyXXnoJ77//Pr744guUKVMG+/btQ79+/aDVaov8oPzss8/Qu3dvbNq0CX/88QfGjx+PFStW4JVXXkFqairee+89DB06tNB5FStWLLY2d3d3HDt2DEqlEgEBAXB2dgYAJCcnP/HnatSoES5duoQ//vgD27dvR69evRAaGoo1a9Y88dzidO3aFYIgYNOmTWjatCn27t2LWbNmGb6fmpqKCRMmoEePHoXO1Wg0xV5XrVYb/htMnToVL774IiZMmIBJkyYBAFasWIERI0ZgxowZaNGiBdzd3TFt2jT89ddfBa6TFw4frqdx48YFgl0eS+nIS/S0GDCIrMjRo0eh1+sxY8YMw1/nj97vL0r16tVRvXp1DB8+HG+88QYWLlyIV155BY0aNcLp06cLBZknUSqVRZ7j4eGBwMBA7N+/HyEhIYb9+/fvR7NmzQocFx4ejvDwcLz22mvo1KkT7t27hzJlyhS4Xt5f/Tqd7rH1aDQa9OjRA8uWLcP58+dRo0YNNGrUyPD9Ro0aIS4uzuif81FjxoxBu3bt8P777xt+zpYtW2LQoEGGY0rSAtGoUSOsXLkSvr6+8PDweKqaiCwVO3kSWZGqVasiOzsb33zzDS5evIglS5Zg3rx5xR6fkZGBIUOGYNeuXbhy5Qr279+PI0eOGG59jBw5EgcOHMCQIUMQGxuL//77D7/99pvRnTwf9tFHH+HLL7/EypUrERcXh1GjRiE2NhbDhg0DAMycORO//PILzp49i3PnzmH16tXw9/cvcnIwX19fODs7Y8uWLUhISEBSUlKxr9unTx9s2rQJCxYsMHTuzDNu3DgsXrwYEyZMwKlTp3DmzBmsWLECY8aMMepna9GiBerVq4fJkycDAKpVq4a///4bW7duxblz5zB27FgcOXLkidfp06cPfHx80K1bN+zduxeXLl3Crl27MHToUFy/ft2omogsFQMGkRWpX78+Zs6ciS+//BJ16tTBsmXLCgzxfJRKpcLdu3fRt29fVK9eHb169ULnzp0xYcIEAEC9evWwe/dunDt3Dq1bt0bDhg0xbtw4BAYGlrrGoUOHIioqCh9++CHq1q2LLVu2YMOGDahWrRoA8fbKV199hSZNmqBp06a4fPkyNm/ebGiReZiDgwO+/vpr/PDDDwgMDES3bt2Kfd127dqhTJkyiIuLQ+/evQt8LywsDBs3bsSff/6Jpk2b4rnnnsOsWbPwzDPPGP3zDR8+HPPnz8e1a9fw3nvvoUePHggPD0fz5s1x9+7dAq0ZxXFxccGePXtQsWJF9OjRA7Vq1UK/fv2QmZnJFg2yGQpBEAS5iyAiIiLbwhYMIiIikhwDBhEREUmOAYOIiIgkx4BBREREkmPAICIiIskxYBAREZHkGDCIiIhIcgwYREREJDkGDCIiIpIcAwYRERFJjgGDiIiIJPd/YPruY7jbL8IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = bestModel.stages[-1].summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(summary.roc.select('FPR').collect(),\n",
    "         summary.roc.select('TPR').collect())\n",
    "plt.xlabel('False Positive Rare')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
